[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Political Science",
    "section": "",
    "text": "Course Notes\nThis document will include important links and course notes for 01:790:391:01: Data Science for Political Science for the fall 2023 semester.\n\nThis site will be updated throughout the semester with new content.\nThe Canvas modules will provide links to the relevant sections to review for a given week of the course.\nThe primary text for the course is Quantitative Social Science: An Introduction by Kosuke Imai. We will refer to this as QSS in the notes.\nThis is a living document. If you spot errors or have questions or suggestions, please email me at k.mccabe@rutgers.edu or post to the course Canvas site.\nOccasionally the notes are updated with embedded video explainers of portions of the code in different sections. These will be located in the playlist linked here.\n\nIn addition, the chat bot below can be used to ask questions about material included on lecture slides posted on Canvas as well as the pdf version of this website, which can be accessed using the pdf icon in the top-left sidebar of this page."
  },
  {
    "objectID": "intro.html#what-have-i-signed-up-for",
    "href": "intro.html#what-have-i-signed-up-for",
    "title": "1  Introduction",
    "section": "1.1 What have I signed up for?",
    "text": "1.1 What have I signed up for?\nFirst: What is Data Science?\n\nData Science involves a combination of math/statistics and programming/coding skills, which, for our purposes, we will combine with social science knowledge.\n\nDrew Conway has a nice venn diagram of how these different skill sets intersect.\nNote: This course will not assume prior familiarity with data science in general or coding, specifically. For those brand new to data science, the idea of learning to code may seem intimidating, but anyone can succeed with a bit of patience and an open mind.\n\n\n\nNext: What is political science?\n\nThe science of politics, of course! Politics focuses on studying governance and the distribution of power in society, broadly conceived.\n\nHow else might you define politics and political science? What do we study in political science?\n\n\n\n1.1.1 Data Science Can Help Social Scientists\nExample: Mapping poverty using mobile phone and satellite data\nResearchers used modern data sources, including mobile phone data, as a way to predict and describe poverty in different geographic regions. These tools helped social scientists come up with methods that are much more cost-effective and efficient, but still as accurate as traditional methods for this type of measurement.\n\nHow might measures of global poverty be useful to political scientists?\n\n\nSteele et al. 2017: “Poverty is one of the most important determinants of adverse health outcomes globally, a major cause of societal instability and one of the largest causes of lost human potential. Traditional approaches to measuring and targeting poverty rely heavily on census data, which in most low- and middle-income countries (LMICs) are unavailable or out-of-date. Alternat emeasures are needed to complement and update estimates between censuses. This study demonstrates how public and private data sources that are commonly available for LMICs can be used to provide novel insight into the spatial distribution of poverty.We evaluate the relative value of modelling three traditional poverty measures using aggregate data from mobile operators and widely available geospatial data.”\n\n\n1.1.2 Course Goals\nSocial Science Goals\nWe have several goals in social science. Here are four that data science can help us pursue:\n\nDescribe and measure\n\nHas the U.S. population increased?\n\nExplain, evaluate, and recommend (study of causation)\n\nDoes expanding Medicaid improve health outcomes?\n\nPredict\n\nWho will win the next election?\n\nDiscover\n\nHow do policies diffuse across states?\n\n\nWhat are other examples of these goals?\nNote: In this course, we are exploiting the benefits of quantitative data to help achieve goals of social science. However, quantitative data have their shortcomings, too. We will also discuss the limitations of various applications of social science data, and we encourage you to always think critically about how we are using data.\nThis course will provide you with a taste of each of these social science goals, and how the use of data can help achieve these goals. By the end of the course, you should be able to\n\nProvide examples of how quantitative data may be used to help answer social science research questions.\nCompare and contrast the goals of description, causation, prediction, and discovery in social science research.\nUse the programming language R to import and explore social science data and conduct basic statistical analyses.\nInterpret and describe visual displays of social science data, such as graphs and maps.\nDevelop your own analyses and visualizations to understand social science phenomena.\n\nIf you are someone that loves data, we hope you will find this course engaging. If you are someone who loathes or finds the idea of working with data and statistics alarming, we hope you keep an open mind. We will meet you where you are. This course will not assume knowledge of statistical software, and there will be plenty of opportunities to ask questions and seek help from classmates and the instructor throughout the semester.\nThe first section of course will walk people through how to use the statistical program– R– that we will employ this semester.\nWill this course help me in the future?\nEven if you do not plan on becoming a social scientist or a data scientist, an introduction to these skills may prove helpful throughout your academic and professional careers.\n\nTo become an informed consumer of news articles and research involving quantitative analyses.\nTo practice analytical thinking to make informed arguments and decisions.\nTo expand your toolkit for getting a job that may involve consuming or performing some data analysis, even if that is not the traditional role.\n\nExample: Journalism- How 5 Data Dynamos Do Their Jobs"
  },
  {
    "objectID": "intro.html#rsetup",
    "href": "intro.html#rsetup",
    "title": "1  Introduction",
    "section": "1.2 Setup in R",
    "text": "1.2 Setup in R\nGoal\nBy the end of the first week of the course, you will want to have R and RStudio installed on your computer (both free), feel comfortable using R as a calculator, and making documents using the R Markdown file type within RStudio.\nR is an application that processes the R programming language. RStudio is also an application, which serves as a user interface that makes working in R easier. We will primarily open and use RStudio to work with R.\nIn other classes, you may come across Stata, SPSS, Excel, or SAS, which are programs that also conduct data analysis. R has the advantage of being free and open-source. Even after you leave the university setting, you will be able to use R/RStudio for free. As an open-source program, it is very flexible, and a community of active R/RStudio users is constantly adding to and improving the program. You might also encounter the Python language at some point. R and Python have similarities, and learning R can also make learning Python easier down the road.\nR and RStudio Installation\nThis content follows and reinforces section QSS 1.3 in our book. Additional resources are also linked below.\n\nThis video from Professor Christopher Bail explains why many social scientists use R and describes the R and RStudio installation process. This involves\n\nGoing to cran, select the link that matches your operating system, and then follow the installation instructions, and\nVisiting RStudio and follow the download and installation instructions. R is the statistical software and programming language used for analysis. RStudio provides a convenient user interface for running R code."
  },
  {
    "objectID": "intro.html#open-r-script-in-rstudio",
    "href": "intro.html#open-r-script-in-rstudio",
    "title": "1  Introduction",
    "section": "1.3 Open R Script in RStudio",
    "text": "1.3 Open R Script in RStudio\nThis next section provides a few notes on using R and RStudio now that you have installed it. In this section, we cover the following materials:\n\nUsing R as a calculator and assigning objects using &lt;-\nSetting your working directory and the setwd() function.\nCreating and saving an R script (.R file)\nCreating, saving, and compiling an R Markdown document (.Rmd) into an html document (.html)\n\nThis section highlights important concepts from QSS chapter 1.\nRStudio is an open-source and free program that greatly facilitates the use of R, especially for users new to programming. Once you have downloaded and installed R and RStudio, to work in R, all you need to do now is open RStudio (it will open R). It should look like this, though your version numbers will be different:\n\nNote: The first time you open RStudio, you likely only have the three windows above. We will want to create a fourth window by opening an R script to create the fourth window.\n\nTo do this, in RStudio, click on File -&gt; New -&gt; R script in your computer’s toolbar. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment.\n\nYou can alternatively click on the green + sign indicator in the top-left corner of the RStudio window, which should give you the option to create a new R script document.\n\n\nNow you should have something that looks like this, similar to Figure 1.1. in QSS:\n\n\nThe upper-left window has our .R script document that will contain code.\nThe lower-left window is the console. This will show the output of the code we run. We will also be able to type directly in the console.\nThe upper-right window shows the environment (and other tabs, such as the history of commands). When we load and store data in RStudio, we will see a summary of that in the environment.\nThe lower-right window will enable us to view plots and search help files, among other things.\n\n\n1.3.1 Using R as a Calculator\nThe bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard:\n\n5 + 3\n\n[1] 8\n\n5 - 3\n\n[1] 2\n\n5^2\n\n[1] 25\n\n5 * 3\n\n[1] 15\n\n5/3\n\n[1] 1.666667\n\n(5 + 3) * 2\n\n[1] 16\n\n\nAgain, in the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course.\n\n\n1.3.2 Working in an R Script\nEarlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window.\nSet your working directory setwd()\nMany times you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets.\nThere are many ways to do this.\n\nAn easy way is to go to Session -&gt; Set Working Directory -&gt; Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it.\nNote: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future.\nIf you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now\n\n\n## Example of where my directory was\ngetwd()\n\n[1] \"/Users/ktmccabe/Dropbox/GitHub2/dsps23\"\n\n\nIf I want to change the working directory, I can go to the top toolbar of my computer and use Session -&gt; Set Working Directory -&gt; Choose Directory or just type my file pathway using the setwd() below:\n\n## Example of setting the working directory using setwd().\n## Your computer will have your own file path.\nsetwd(\"/Users/ktmccabe/Dropbox/Rutgers Teaching/\")\n\n\n\n1.3.3 Saving the R Script\nLet’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory.\nGive the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension.\n\n\n1.3.4 Annotating your R script\nNow that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now.\n\nJust like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments– annotations that explain what your code is doing. Below is a screenshot example of a template R script.\nYou can specify your working directory at the top, too. Add your own filepath inside setwd()\n\n\n\nThen you can start answering problems in the rest of the script.\nThink of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script.\n\n\n\n1.3.5 Running Commands in your R script\nThe last thing we will note in this section is how to execute commands in your R script.\nTo run / execute a command in your R script (the upper left window), you can\n\nHighlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows\nPlace your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or\nDo 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window.\n\nTry it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this:\n\nAfter you executed the code, you should see it pop out in your Console:\n\n5 + 3\n\n[1] 8\n\n\n\nNote: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do.\n\n## Example\nsum53 &lt;- 5 + 3 # example of assigning an addition calculation\n\n\n\n1.3.6 Objects\nSometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53:\n\nsum53 &lt;- 5 + 3\n\nAfter we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just has sum53, it will output 8. Try it:\n\nsum53\n\n[1] 8\n\n\nNow we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do:\n\nsum53 - 2\n\n[1] 6\n\n\nLet’s say we wanted to divide two stored calculations:\n\nten &lt;- 5 + 5\ntwo &lt;- 1 + 1\nten / two\n\n[1] 5\n\n\nThe information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks.\n\nmccabe &lt;- \"professor for this course\"\nmccabe\n\n[1] \"professor for this course\"\n\n\nNote: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names.\n\nGOOD CODE: practice.calc &lt;- 5 + 3\nBAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3\n\nWhile these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course.\nWe can also store an array or “vector” of information using c()\n\nsomenumbers &lt;- c(3, 6, 8, 9)\nsomenumbers\n\n[1] 3 6 8 9\n\n\nImportance of Clean Code\nIdeally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused.\nFor example, R is case sensitive. Let’s say we assigned our object like before:\n\nsum53 &lt;- 5 + 3\n\nHowever, when we went to execute sum53, we accidentally typed Sum53:\n\nSum53\n\nError in eval(expr, envir, enclos): object 'Sum53' not found\n\n\nOnly certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2:\n\nmccabe / 2\n\nError in mccabe/2: non-numeric argument to binary operator\n\n\nA big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages."
  },
  {
    "objectID": "intro.html#r-markdown",
    "href": "intro.html#r-markdown",
    "title": "1  Introduction",
    "section": "1.4 R Markdown",
    "text": "1.4 R Markdown\nAn R Markdown document, which you can also create in RStudio, allows you to weave together regular text, R code, and the output of R code in the same document. This can be very convenient when conducting data analysis because it allows you more space to explain what you are doing in each step. We will use it as an effective platform for writing up problem sets.\nR Markdown documents can be “compiled” into html, pdf, or docx documents by clicking the Knit button on top of the upper-left window. Below is an example of what a compiled html file looks like.\n\nNote that the image has both written text and a gray chunk, within which there is some R code, as well as the output of the R code (e.g., the number 8 and the image of the histogram plot. \n\nWe say this is a “compiled” RMarkdown document because it differs from the raw version of the file, which is a .Rmd file format. Below is an example of what the raw .Rmd version looks like, compared to the compiled html version.\n \n\n1.4.1 Getting started with RMarkdown\nJust like with a regular R script, to work in R Markdown, you will open up RStudio.\n\nFor additional support beyond the notes below, you can also follow the materials provided by RStudio for getting started with R Markdown https://rmarkdown.rstudio.com/lesson-1.html.\n\nThe first time you will be working in R Markdown, you will want to install two packages: rmarkdown and knitr. You can do this in the Console window in RStudio (remember the lower-left window!).\nType the following into the Console window and hit enter/return.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\nOnce you have those installed, now, each time you want to create an R Markdown document, you will open up a .Rmd R Markdown file and get to work.\n\nGo to File -&gt; New File -&gt; R Markdown in RStudio\n\nAlternatively, you can click the green + symbol at the top left of your RStudio window\n\nThis should open up a window with several options, similar to the image below\n\nCreate an informative title and change the author name to match your own\nFor now, we will keep the file type as html. In the future, you can create pdf or .doc documents. However, these require additional programs installed on your computer, which we will not cover in the course.\n\n\n\n\nAfter you hit “OK” a new .Rmd script file will open in your top-left window with some template language and code chunks, similar to the image below. Alternatively, you can start from scratch by clicking “Create Empty Document” or open a template .Rmd file of your own saved on your computer.\n\n\n\nSave as .Rmd file. Save the file by going to “File -&gt; Save as” in RStudio\n\n\nGive the file an informative name like your LastnamePractice1.Rmd\n\n\nKey Components. Now you are ready to work within the Rmd script file. We will point to four basic components of this file, and you can build your knowledge of RMarkdown from there.\n\nThe top part bracketed by --- on top and bottom is the YAML component. This tells RStudio the pertinent information about how to “compile” the Rmd file.\n\nMost of the time you can leave this alone, but you can always edit the title, author, or date as you wish.\n\nThe next component are the global options for the document. It is conveniently labeled “setup.” By default what this is saying is that the compiled version will “echo” (i.e., display all code chunks and output) unless you specifically specify otherwise. For example, note that it says include = FALSE for the setup chunk. That setting means that this code chunk will “run” but it will not appear in the nicely compiled .html file.\n\nMost of the time you will not need to edit those settings.\n\nThe third component I want to bring attention to is the body text. The # symbol in RMarkdown is used to indicate that you have a new section of the document. For example, in the compiled images at the beginning, this resulted in the text being larger and bolded when it said “Problem 2.” In addition to just using a single #, using ## or ### can indicate subsections or subsubsections. Other than that symbol, you can generally write text just as you would in any word processing program, with some exceptions, such as how to make text bold or italicized.\nThe final component I want to call attention to are the other main body code chunks. These are specific parts of the document where you want to create a mini R script. To create these, you can simply click the + C symbol toward the top of the top left window of RStudio and indicate you want an R chunk.\n\n\n\n\nWriting R Code. Within a code chunk, you can type R code just like you would in any R script, as explained in the previous section. However, in RMarkdown, you also have the option of running an entire code chunk at once by hitting the green triangle at the top-right of a given code chunk.\n\n\n\nKnitting the document. Once you have added a code chunk and/or some text, you are ready to compile or “Knit” the document. This is what generates the .html document.\n\nTo do so, click on the Knit button toward the top of the top-left window of Rstudio. After a few moments, this should open up a preview window displaying the compiled html file.\nIt will also save an actual .html file in your working directory (the same location on your computer where you have saved the .Rmd file)\nTry to locate this compiled .html file on your computer and open it. For most computers, .html files will open in your default web browser, such as Google Chrome or Safari.\nThis step is a common place where errors are detected and generated. Sometimes the compiling process fails due to errors in the R code in your code chunks or an error in the Markdown syntax. If your document fails to knit, the next step is to try to troubleshoot the error messages the compiling process generates. The best way to reduce and more easily detect errors is to “knit as you go.” Try to knit your document after each chunk of code you create."
  },
  {
    "objectID": "intro.html#assignment-1",
    "href": "intro.html#assignment-1",
    "title": "1  Introduction",
    "section": "1.5 Assignment 1",
    "text": "1.5 Assignment 1\nBelow is an exercise that will demonstrate you are able to use R as a calculator, create R scripts, and create and compile R Markdown files.\nWe will start walking through this assignment together during class, but you are welcome to try to do this ahead of time on your own.\nYou will submit three documents on Canvas:\n\nAn R script (.R) file with your code. Follow the best practices by titling your script and using # comments to explain your steps. This code should be clean. I should be able to run your code to verify that the code produces the answers you write down.\nAn .Rmd document and a compiled RMarkdown .html document that you get after “knitting” the .Rmd file. This should also have a title including your name and use text or # comments to explain your steps.\n\nYou can create these documents from scratch using the guidance in the previous sections, or you can download and open the .R and .Rmd templates, provided on Canvas, in RStudio to get started. This video provides a brief overview of opening an R script and R Markdown file in RStudio. The notes provide additional details.\n\nAssignment Exercises\n\nCreate a .R script saved as “LastnameSetup1.R” (use your last name). Within this file, make sure to title it and provide your name.\n\nSet your working directory, and include the file pathway (within setwd()) at the top of your .R script\nDo the calculation 8 + 4 - 5 in your R script. Store it as an object with an informative name. Report the answer as a comment # below the code.\nDo the calculation 6 x 3 in your R script. Store it as an object with an informative name. Report the answer as a comment # below the code.\nAdd these two calculations together. Note: do this by adding together the objects you created, not the underlying raw calculations. Report the answer as a # below the code.\n\nIn this problem, we will just re-format what we did in the first problem in an R Markdown format. Create a .Rmd R Markdown file saved as “LastnameSetup1.Rmd.” Within this file, make sure to title it and provide your name.\n\nCreate a Markdown heading # Problem 2.1. Underneath this, create an R code chunk in which you do the calculation 8 + 4 - 5. Store it as an object with an informative name. Report the answer in plain language below the code chunk.\nCreate a Markdown heading # Problem 2.2. Underneath this, create an R code chunk in which you do the calculation 6 x 3 in your R script. Store it as an object with an informative name. Report the answer in plain language below the code chunk.\nCreate a Markdown heading # Problem 2.3. Underneath this, create an R code chunk in which you add the previous two calculations together. Note: do this by adding together the objects you created, not the underlying raw calculations. Report the answer in plain language below the code chunk.\nCreate a Markdown heading # Problem 2.4. Write down how you will complete your R assignments this semester. For example, if you have a personal laptop with R and RStudio on it, you will simply write “I will use my personal laptop.” If you don’t have a personal computer or laptop, please indicate where on campus or off-campus you will have regular access to a computer with R/RStudio to do your work. It is essential that you have regular access to a computer so that you will not fall behind in this course.\n\nCreate a compiled .html file by “knitting” the .Rmd file into a .html document. Save the file as “LastnameSetup1.html.”\n\nAll done! Submit the three documents on Canvas."
  },
  {
    "objectID": "02-Description.html#process-of-describing",
    "href": "02-Description.html#process-of-describing",
    "title": "2  Description",
    "section": "2.1 Process of Describing",
    "text": "2.1 Process of Describing\nHow do we go about a descriptive quantitative analysis?\n\nSubstantive Expertise: Start with a topic, puzzle, or question (e.g., How is the economy doing?)\nFind outcome data relevant to that question (e.g., GDP)\n\nStart from a concept: what we want to describe (i.e., health of the economy)\nMove toward an “operationalization” (i.e., a way to measure it)\nEasy! except… social science is messy. Our concepts are rich, while our measures may be very narrow or concrete.\n\nFor example, GDP is one way to measure economic health, but is it the only measure?\nChoose measures based on validity, reliability, cost\n\n\nFind multiple relevant units or “data points”\n\nE.g., Multiple years of data (e.g., U.S., from 1900 to 2020)\nE.g., Multiple countries from one year (e.g., U.S. to Germany to other countries)\n\nSummarize the data to help answer the question\n\n\n2.1.1 Example Process\n\nHow is the economy doing?\nFind outcome data relevant to that question\n\nLet’s ask people\n\nFind multiple relevant units or data points\n\nWe will ask several people. Each person will be a data point.\n\nSummarize the data\n\nLet’s take the mean\n\n\n\nHow would you summarize information in explaining it to another person? You would probably want to describe how most people feel about the economy. In other words, you would describe the “central tendency” of people’s responses (the central tendency of the data)."
  },
  {
    "objectID": "02-Description.html#summarizing-univariate-data",
    "href": "02-Description.html#summarizing-univariate-data",
    "title": "2  Description",
    "section": "2.2 Summarizing univariate data",
    "text": "2.2 Summarizing univariate data\nFor a video explainer of the code in this section, see below. The video only discusses the code. Use the notes and lecture discussion for additional context. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nUnivariate data refers to data coming from one “variable,” where a variable captures the values of a changing characteristic.\nOur set of values is Outcome = {0,0,0,0,1,1,0,1,0,1}.\n\nWe will call this a vector of values, where a vector is just a collection of things.\nBecause our vector contains only numbers, we will call it a numeric vector.\nEach value can be indexed by i, denoting the position of the value in the\nFor example, Jesse is in position i=10 of the vector, and his value is 1\n\nWe can create vectors in R by using c() and assigning &lt;- it to an object we will call Outcome.\n\nOutcome &lt;- c(0,0,0,0,1,1,0,1,0,1) # Use commas to separate values\n\nWe can extract a particular value within our vector using brackets and the value’s numeric position in the vector.\n\nOutcome[10] # what value is in the 10th position?\n\n[1] 1\n\n\nWe can label our outcomes using names()\n\nnames(Outcome) &lt;-c(\"Joe\",\"Sally\", \"Trevor\", \"Emily\", \"Mark\",\n                   \"Sarah Jane\", \"Stacey\", \"Steve\", \"Phoebe\", \"Jesse\")\nOutcome[10]\n\nJesse \n    1 \n\n\nWe can overwrite whole vectors or values within a vector\n\nOutcome &lt;- c(5,0,2, 6,1,1, 7, 8, 0, 1) # oops we put the wrong numbers\nOutcome\n\n [1] 5 0 2 6 1 1 7 8 0 1\n\nOutcome &lt;- c(0,0,0,0,1,1,0,1,0,1) # no problem, just overwrite it\nOutcome\n\n [1] 0 0 0 0 1 1 0 1 0 1\n\n\nOops we accidentally type a 0 for Jesse.\n\nOutcome &lt;- c(0,0,0,0,1,1,0,1,0,0) # oops typo for Jesse\nOutcome\n\n [1] 0 0 0 0 1 1 0 1 0 0\n\nOutcome[10] &lt;- 1 # no prob bob. Assign a 1 in position 10\nOutcome\n\n [1] 0 0 0 0 1 1 0 1 0 1\n\n\nVectors do not have to be numeric. Character vectors contain a collection of words and phrases. In R, we use quotations around character values\nExample: let’s create a vector of names that we will call People.\n\nPeople &lt;- c(\"Joe\",\"Sally\", \"Trevor\", \"Emily\", \"Mark\", \"Sarah Jane\", \"Stacey\", \"Steve\", \"Phoebe\", \"Jesse\")\nPeople[10]\n\n[1] \"Jesse\"\n\n\nWe can use the R function class() to tell us the type of object we have.\n\nclass(Outcome)\n\n[1] \"numeric\"\n\nclass(People)\n\n[1] \"character\""
  },
  {
    "objectID": "02-Description.html#functions-to-summarize-univariate-data",
    "href": "02-Description.html#functions-to-summarize-univariate-data",
    "title": "2  Description",
    "section": "2.3 Functions to summarize univariate data",
    "text": "2.3 Functions to summarize univariate data\nFor univariate data, often we are interested in describing the range of the values and their central tendency.\n\nrange: the minimum (min()) and maximum (max()) values\nmean: the average value (mean())\n\nThe average is the sum of the values divided by the number of values:\n\\(\\bar{X} = \\frac{\\text{sum of values}}{\\text{number of values}} = \\frac{x_1 + x_2 + ... + x_N}{N}=\\frac{1}{N}\\sum_{i=1}^{i=N} x_i\\)\nLet’s do this in R for our set of 10 values\n\n(0 + 0 + 0 + 0 + 1 + 1 + 0 + 1 + 0 + 1)/10\n\n[1] 0.4\n\n\nThe average outcome is .4. Note: when a variable contains only 0’s and 1’s its mean is the proportion of 1’s. 40% of people think the economy is doing well.\n\n2.3.1 Using functions in R (overview)\nA function is an action(s) that you request R to perform on an object or set of objects. For example, we will use the mean() function to ask R to take the mean or “average” of a vector.\n\nInside the function you place inputs or “arguments.”\n\n\nmean(Outcome)\n\n[1] 0.4\n\n\nR also has functions that take the sum sum() of a vector of values.\n\nsumofvalues &lt;- sum(Outcome)\n\nAnd that count the total number of values or “length” length() of the vector.\n\nnumberofvalues &lt;- length(Outcome)\n\nNote that the below is also equivalent to the mean\n\nsumofvalues / numberofvalues\n\n[1] 0.4\n\n\nReturning to our example, we found that 40% of people surveyed thought the economy was doing well. Surveying people about their opinions on how the country doing is a common way that social scientists use description. We could extend this exercise in many ways going forward, even with the same question.\n\nStart with a question: How is the economy doing?\nLet’s find a measure: Ask people if the economy is doing well.\nFind data points: Multiple people (we could stop there with the average!), or add more variables:\n\nAcross time: Survey people across multiple years\nAcross type of people: Survey different partisan groups\n\n\nThese types of trends are often used by news organizations and public opinion organizations like, Gallup.\n\nThis was just a first example of description in political science. There are many other ways to describe how the economy is doing and many other topics we might want to describe in politics."
  },
  {
    "objectID": "02-Description.html#loading-data-into-r",
    "href": "02-Description.html#loading-data-into-r",
    "title": "2  Description",
    "section": "2.4 Loading data into R",
    "text": "2.4 Loading data into R\nFor this section, our motivating example will be methods to measure voter turnout in the United States.\nDescribing voter turnout\n\nWhat is a typical level of voter turnout?\nHow has turnout changed over time?\nIs turnout higher in presidential years or in midterm years?\n\nHow can we measure turnout? Think about the validity, reliability, and cost of different approaches.\nExample: Dataset on Voter Turnout in the U.S. across multiple years\n\nIn this dataset, each row is an election year. Each column contains information about the population, potential voters, or voter turnout. These will help us compute the turnout rate in a given year. To work with this dataset, we need to load it into R.\n\n2.4.1 Working with datasets in R\nFor a video explainer of the code in this section, see below. The video only discusses the code. Use the notes and lecture discussion for additional context. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nOften the variables we care about are stored inside of rectangular datasets\n\nThese have a number of rows nrow() and columns ncol()\nEach row is an “observation,” representing the information collected from an individual or entity\nEach column is a variable, representing a changing characteristic across multiple observations\n\nWhen we import a dataset into R, we have a few options.\nOption 1: Download dataset to your computer\n\nMove the dataset to your working directory\nIdentify the file type (e.g., csv, dta, RData, txt)\nPick the appropriate R function to match the type (e.g., read.csv(), read.dta(), load(), read.table())\nAssign the dataset to an object. This object will now be class() of data.frame\n\n\nturnout &lt;- read.csv(\"turnout.csv\")\n\n\n\nClick here for an alternative function for csv files.\n\nSome scholars prefer to use the function read_csv to load csv data. It is better at handling more complicated types of data. We will not need to use this function in this course, but you may encounter it elsewhere.\nTo use this function, the first time we will go about using it, we have to first install a “package” called readr. Packages in R give us additional tools beyond what the base version of R provides. It is like installing an extra app on your phone.\n\ninstall.packages(\"readr\")\n\nOnce we have that installed, now anytime we want to use the function, we will call (open) the “readr” package using library(), and then the syntax is just like using the read.csv function.\n\nlibrary(readr)\nturnout &lt;- read_csv(\"turnout.csv\")\n\n\nOption 2: Read file from a url provided\n\nNeed an active internet connection for this to work\nURL generally must be public\nInclude the url inside the function used to read the data\n\n\nturnout &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/turnout.csv\")\n\n\nclass(turnout)\n\n[1] \"data.frame\"\n\n\nYou can also open up a window to view the data:\n\nView(turnout)\n\n\n\n2.4.2 Measuring the Turnout in the US Elections\nRelevant questions with voter turnout\n\nWhat is a typical level of voter turnout?\nIs turnout higher in presidential years or in midterm years?\nIs turnout higher or lower based on voting-eligible (VEP) or voting-age (VAP) populations? We have a lot of people who are citizens 18 and older who are ineligible to vote. This makes the VEP denominator smaller than the VAP.\n\nVoter Turnout in the U.S.\n\nNumerator: total: Total votes cast (in thousands)\nDenominator:\n\nVAP: (voting-age population) from Census\nVEP (voting-eligible population) VEP = VAP + overseas voters - ineligible voters\n\nAdditional Variables and Descriptions\n\nyear: election year\nANES: ANES self-reported estimated turnout rate\nVEP: Voting Eligible Population (in thousands)\nVAP: Voting Age Population (in thousands)\ntotal: total ballots cast for highest office (in thousands)\nfelons: total ineligible felons (in thousands)\nnoncitizens: total non-citizens (in thousands)\noverseas: total eligible overseas voters (in thousands)\nosvoters: total ballots counted by overseas voters (in thousands)\n\n\n\n\n2.4.3 Getting to know your data\n\n## How many observations (the rows)?\nnrow(turnout)\n\n[1] 14\n\n## How many variables (the columns)?\nncol(turnout)\n\n[1] 9\n\n## What are the variable names?\nnames(turnout)\n\n[1] \"year\"     \"VEP\"      \"VAP\"      \"total\"    \"ANES\"     \"felons\"   \"noncit\"  \n[8] \"overseas\" \"osvoters\"\n\n## Show the first six rows\nhead(turnout)\n\n  year    VEP    VAP total ANES felons noncit overseas osvoters\n1 1980 159635 164445 86515   71    802   5756     1803       NA\n2 1982 160467 166028 67616   60    960   6641     1982       NA\n3 1984 167702 173995 92653   74   1165   7482     2361       NA\n4 1986 170396 177922 64991   53   1367   8362     2216       NA\n5 1988 173579 181955 91595   70   1594   9280     2257       NA\n6 1990 176629 186159 67859   47   1901  10239     2659       NA\n\n\nExtract a particular column (vector) from the data using the $.\n\nturnout$year\n\n [1] 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2008\n\n\nExtract the 10th year. Just like before! We use 10 to indicate the value of the year column in position (row 10) of the data.\n\nturnout$year[10]\n\n[1] 1998\n\n\nWe can take the mean() of a particular column, too. Let’s take it of the total number of voters.\n\nmean(turnout$total)\n\n[1] 89778.29\n\n\nAnd get the class() (Note: integer is just a type of numeric variable)\n\nclass(turnout$total)\n\n[1] \"integer\"\n\n\nWe can also use brackets in the full data frame, but because our data frame has BOTH rows and columns, we cannot just supply one position i. Instead, we have to tell R which row AND which column by using a comma between the positions.\n\nturnout[1,2] # value in row 1, column 2\n\n[1] 159635\n\n\nWe can use the column name instead\n\nturnout[1, \"VEP\"]\n\n[1] 159635\n\n\nIf we leave the second entry blank, it will return all columns for the specified row\n\nturnout[1,] # All variable values for row 1\n\n  year    VEP    VAP total ANES felons noncit overseas osvoters\n1 1980 159635 164445 86515   71    802   5756     1803       NA\n\n\nThe opposite is true if we leave the first entry blank.\n\nturnout[,2] # VEP for all rows\n\n [1] 159635 160467 167702 170396 173579 176629 179656 182623 186347 190420\n[11] 194331 198382 203483 213314"
  },
  {
    "objectID": "02-Description.html#comparing-vep-and-vap-turnout",
    "href": "02-Description.html#comparing-vep-and-vap-turnout",
    "title": "2  Description",
    "section": "2.5 Comparing VEP and VAP turnout",
    "text": "2.5 Comparing VEP and VAP turnout\n\n2.5.1 Creating new variables in R\nLet’s create a new variable that is VAP that adds overseas voters.\n\n# Use $ to add a new variable (i.e., column) to a dataframe\nturnout$VAPplusoverseas &lt;- turnout$VAP + turnout$overseas\n\nUnder the hood, what this is doing is taking each value of turnout$VAP and adding it to its corresponding values of turnout$overseas.\nAnd, yes, this new variable shows up as a new column in turnout. Go ahead, View() it\n\nView(turnout)\n\nThis does not change the underlying turnout.csv file, only the turnout data.frame we are working with in the current R session.\n\nThis is an advantage of using an R script.\nYou don’t have to worry about overwriting/messing up the raw data.\nYou start from the original raw data when you load turnout.csv, and then everything else is done within R.\n\nThis is our new denominator. Now we can calculate turnout based on this denominator.\n\nturnout$newVAPturnout &lt;- turnout$total / turnout$VAPplusoverseas\n\nJust like with adding two vectors, when we divide, each value in the first vector is divided by its corresponding value in the second vector.\n\nturnout$newVAPturnout\n\n [1] 0.5203972 0.4024522 0.5253748 0.3607845 0.4972260 0.3593884 0.5404097\n [8] 0.3803086 0.4753376 0.3483169 0.4934211 0.3582850 0.5454777 0.5567409\n\n\nLet’s calculate the VEP turnout rate and turn it into a percentage. This time, we do it in one step.\n\n(total votes / VEP) \\(\\times\\) 100:\n\n\nturnout$newVEPturnout &lt;- (turnout$total / turnout$VEP) * 100\nturnout$newVEPturnout\n\n [1] 54.19551 42.13701 55.24860 38.14115 52.76848 38.41895 58.11384 41.12625\n [9] 51.65793 38.09316 54.22449 39.51064 60.10084 61.55433\n\n\nLet’s change it from a proportion to a percentage. How? Multiply each value of turnout$newVAP by 100\n\nturnout$newVAPturnout &lt;- turnout$newVAPturnout * 100\n\nThis multiplies each number within the vector by 100.\n\nturnout$newVAPturnout\n\n [1] 52.03972 40.24522 52.53748 36.07845 49.72260 35.93884 54.04097 38.03086\n [9] 47.53376 34.83169 49.34211 35.82850 54.54777 55.67409\n\n\nWhat is typical turnout?\n\nmean(turnout$newVAPturnout)\n\n[1] 45.45658\n\nmean(turnout$newVEPturnout)\n\n[1] 48.94937\n\n\nWe find that turnout based on the voting age population is lower than turnout based on the voting eligible population. This is a pattern that political scientists have examined, going back several decades. For example, in a 2001 article McDonald and Popkin show that is it the ineligible population that grew from the 1970s onward and not the population of people who simply prefer not to vote. (See more here.)\n\n\n\nMcDonald and Popkin 2001"
  },
  {
    "objectID": "02-Description.html#comparing-presidential-vs.-midterm-turnout",
    "href": "02-Description.html#comparing-presidential-vs.-midterm-turnout",
    "title": "2  Description",
    "section": "2.6 Comparing Presidential vs. Midterm turnout",
    "text": "2.6 Comparing Presidential vs. Midterm turnout\nHow does turnout compare in presidential vs. midterm years? Sometimes using a single summary of turnout may obscure important underlying differences in the data. To detect these differences, we may want to summarize different parts of the data.\nOh dear. We need to extract specific years from the turnout data frame. Which rows contain the years we want?\n\nturnout$year\n\n [1] 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2008\n\n\nOk: rows 1,3,5,7,9,11,13,14 are the presidential. And rows 2,4,6,8,10,12 are midterms.\n\n## we can extract all of these at once by using c()\nturnout$year[c(1,3,5,7,9,11,13,14)] # presidential\n\n[1] 1980 1984 1988 1992 1996 2000 2004 2008\n\n\nLet’s take the mean VEP turnout for presidential years.\n\nmean(turnout$newVEPturnout[c(1,3,5,7,9,11,13,14)])\n\n[1] 55.983\n\n\nLet’s take the mean VEP turnout for midterm years.\n\nmean(turnout$newVEPturnout[c(2,4,6,8,10,12)])\n\n[1] 39.5712\n\n\nLet’s take the difference by storing each mean and then subtracting\n\nmean.VEP.pres &lt;- mean(turnout$newVEPturnout[c(1,3,5,7,9,11,13,14)])\nmean.VEP.mid &lt;- mean(turnout$newVEPturnout[c(2,4,6,8,10,12)])\nmean.VEP.pres -  mean.VEP.mid\n\n[1] 16.41181\n\n\nPresidential turnout, on average, is higher than midterm turnout.\n\n2.6.1 R shortcut for writing vectors\nSometimes we write numbers that are in a predictable sequence (e.g., 1,2,3,4,5). In R, we have functions that prevent us from having to type each number when this is the case.\n\nc(1,2,3,4,5) # is equivalent to:\n\n[1] 1 2 3 4 5\n\n1:5 # is equivalent to:\n\n[1] 1 2 3 4 5\n\nseq(from = 1, to = 5, by = 1)\n\n[1] 1 2 3 4 5\n\n\nWe can use the last one to our advantage to extract the midterm years, which go by 2\n\nmean(turnout$newVEPturnout[c(2,4,6,8,10,12)]) # is the same as\n\n[1] 39.5712\n\nmean(turnout$newVEPturnout[seq(2, 12, 2)])\n\n[1] 39.5712\n\n\nNot a big deal now, but imagine if you had to write 100 numbers or 1 MILLION NUMBERS!"
  },
  {
    "objectID": "02-Description.html#creating-dataframes-from-within-r",
    "href": "02-Description.html#creating-dataframes-from-within-r",
    "title": "2  Description",
    "section": "2.7 Creating dataframes from within R",
    "text": "2.7 Creating dataframes from within R\nWhile importing data from outside of R is the most common way to work with dataframes in R, you can also create dataframes from inside R. Ultimately, a dataframe just binds together multiple vectors / columns to create a rectangular object.\nFor example, let’s say we want to create a dataframe with columns indicating just the midterm years and their VEP turnout. These correspond to the two vectors:\n\nturnout$newVEPturnout[seq(2, 12, 2)]\nturnout$year[seq(2, 12, 2)]\n\nIn R, you can create a rectangular data.frame object with the data.frame function.\n\nWithin this function, you can make several entries that follow the syntax colname = values. We supply what we would like the name of the column to be, such as midyear, and then provide R with a set of values. We can then provide a comma and add more columns.\n\nYou just want to make sure each column has the same number of values.\n\n\n\nmidtermdata &lt;- data.frame(midyear = turnout$year[seq(2, 12, 2)], \n                          VEPturnout = turnout$newVEPturnout[seq(2, 12, 2)])\n\nYou can supply the values for each column using objects or just vectors of raw numeric values like the below:\n\nmidtermdata &lt;- data.frame(midyear = c(1982, 1986, 1990, 1994, 1998, 2002), \n                          VEPturnout = c(42.13701, 38.14115, 38.41895, 41.12625, 38.09316, 39.51064))\n\nThe result is a nice rectangular dataframe similar to what we loaded using the turnout.csv dataset from outside of R.\n\nmidtermdata\n\n  midyear VEPturnout\n1    1982   42.13701\n2    1986   38.14115\n3    1990   38.41895\n4    1994   41.12625\n5    1998   38.09316\n6    2002   39.51064\n\n\nNow, because our dataframe has a different name. If we want to access columns from this dataframe, we start with midterm$ followed by the variable name.\n\nmidtermdata$midyear\n\n[1] 1982 1986 1990 1994 1998 2002"
  },
  {
    "objectID": "02-Description.html#wrapping-up-description",
    "href": "02-Description.html#wrapping-up-description",
    "title": "2  Description",
    "section": "2.8 Wrapping Up Description",
    "text": "2.8 Wrapping Up Description\nIn this section, we have described voter turnout using multiple measures and types of elections. There are several other questions that political scientists may be interested in when it comes to voter turnout.\nFor example, during and following the 2020 elections, many states passed laws that changed election procedures: Ability to vote by mail, Ballot dropboxes, Length of early voting. What else?\n\nWhat effect (if any) do these laws have on voter turnout?\n\nIn the next section, we start to examine how to evaluate causal claims.\n\n2.8.1 Summary of R tools\nWe have touched on a number of R tools thus far. Here is a summary of some of the key items to remember going forward:\n\nsetwd(): sets the working directory in R, which tells R which folder on your computer contains the datasets or other R files where you will be working. You should get into the habit of setting your working directory each time you work in RStudio.\n\nCan set this in the toolbar Session -&gt; Set Working Directory -&gt; Choose Directory, followed by clicking the “Open” button on the folder where you want to work.\nExample: setwd(\"~/Downloads/Data Science\")\n\n##: Hashtags are used to help annotate your code. Anything behind a hashtag is treated as plain text\n+ - * /: These are some of the mathematical operators you can use in R\n\nYou can also control which operations are performed first using () just like you would do with math outside of R. For example, try to compare the answer to 6 + 4 * 3 with (6 + 4) * 3\n\n&lt;-: This is an assignment tool that allows us to store calculations, vectors, datasets, and more as objects in R.\n\nExample: sum53 &lt;- 5 + 3 creates an object called sum53 that stores the calculation on the right.\n\n[]: Brackets are used to extract specific components of objects we create. The number(s) inside the brackets tell us which entries to extract.\n\nExample: Outcome[2] will tell us to extract the second entry in the object Outcome\nNote: when we use datasets, the brackets will have two entries, one corresponding to the row entry and one corresponding to the column. Example turnout[1,2] means the entry in the first row and second column.\n\n\nFunctions We have already started using a number of functions in R, which are operations we ask R to do for us, such as creating vectors, importing data, or summarizing data by finding the mean, range, etc. Functions come in the same format, which starts with the function name followed by parentheses. Example: mean(). Each function then takes a particular input(s). When you “run” a line of code with a function, R applies the function to the input.\n\nc(): This is a function that combines a set of values into a vector in R. The values can be numbers or text items and should be separated by commas. If text, each text item should be in quotation marks.\n\nExample: Outcome &lt;- c(3, 4, 6, 2, 1)\nExample: People &lt;- c(\"Sam\", \"Julie\", \"Mark\")\n\nmean(), median(), min(), max(), range(): These functions summarize vectors that are numeric/integers in nature.\n\nExample mean(Outcome) takes the average of the values in the Outcome vector\n\nread.csv(): This function loads a rectangular .csv file into R as a data.frame\n\nExample: turnout &lt;- read.csv(\"turnout.csv\")\nNot all datasets will be .csv files. In the future, we will use other functions, such as load() or read.dta() to import datasets of different file types.\n\n\nDataframes\nWe have started working with dataframes in R. These objects are rectangular datasets that include a collection of vectors. Every column in a dataframe generally represents a different concept or “variable,” while each row represents a different unit or “observation.”\n\n$: When we are working with vectors that are inside of a dataframe (the columns inside of a dataframe), we use the $ to access them.\n\nExample: turnout$year will show us the values in the year column vector inside our turnout rectangular dataframe\n\nnrow(), ncol(), dim(), head(), names(): These functions help us explore the dataframes by telling us the number of rows and columns (the dimensions), giving us a sneak peek of the first 6 rows of the dataframe, or showing us the names of the variables (columns) in the data.\n\nExample: nrow(turnout)"
  },
  {
    "objectID": "03-CausalityI.html#what-separates-causation-from-correlation",
    "href": "03-CausalityI.html#what-separates-causation-from-correlation",
    "title": "3  Causation with Experiments",
    "section": "3.1 What separates causation from correlation?",
    "text": "3.1 What separates causation from correlation?\nHere’s an example. In 2016, researchers at the NY Times noticed that areas in the country where the television show Duck Dynasty was popular also tended to support Donald Trump at higher rates.\n\nIf we put our social scientist hat on, we might want to distinguish whether this is a causal or, more likely, just a correlational relationship:\n\nCorrelation: Areas that watch Duck Dynasty are more likely to support Trump (degree to which two variables “move together”)\nCausality: Watching Duck Dynasty (vs. not watching) increases your support of Trump.\n\nCausal Question: Does the manipulation of one factor (the treatment), (holding everything else constant), cause a change in an outcome?\n\n3.1.1 Potential Outcomes Framework\nWhen studying causal relationships, we distinguish two concepts:\n\ntreatment: variable whose change may produce a change in the outcome (e.g., watching vs. not watching Duck Dynasty)\noutcome (\\(Y\\)): what may change as a result (e.g., their support for Trump)\n\nWe imagine two states of the world or “potential outcomes.”\n\n\\(Y(1)\\): the outcome if the treatment is administered (e.g., watching the show)\n\\(Y(0)\\): the outcome if the treatment is NOT administered or maybe something else is (e.g., not watching the show)\n\nPolitical Science Example: How does voter turnout (\\(Y\\)) change as a result of varying whether someone receives a mail-in ballot (the treatment)?\n\n\\(Y(\\text{sent a mail-in ballot})\\): do you vote or not\n\\(Y(\\text{not sent a mail-in ballot})\\): do you vote or not\n\nWe compare your likelihood of turning out to vote in a world where you did receive a mail-in ballot vs. a counterfactual state of the world in which you did not receive a mail-in ballot, generally assuming that this is the only thing that is different between these two potential states of the world.\nIn many cases in social science, we might start by observing some connection in the real world. To make a causal claim, we then have to imagine what that counterfactual state of the world would be. Examples:\nCausal Question: Does the minimum wage increase the unemployment rate?\n\n(Hypothetical) Factual: An unemployment rate went up after the minimum wage increased\nImplied Counterfactual: Would the unemployment rate have gone up, had the minimum wage increase not occurred?\n\nCausal Question: Does the gender of a political messenger influence the persuasiveness of the message?\n\n(Hypothetical) Factual: Suppose a political messenger perceived as a man had a somewhat persuasive effect delivering a message on abortion.\nImplied Counterfactual: Would a political messenger perceived as a woman have a similar or different persuasive effect?\n\nWe use causal logic all of the time outside of social science.\nFor example, many viewers get angry after watching the movie Titanic because they believe Jack did not have to die. We can place their claims in our causal framework:\n\n\nOutcome: Jack Surviving the Titanic\nPotential Outcomes in two states of the world\n\nRose did not share the floating door, and Jack died.\nCounterfactual question: If Rose had shared the floating door, would Jack have lived?\n\n\nIn Bit by Bit, Matt Salganik notes that sometimes cause-and-effect questions are implicit. For example, in more general questions about maximization of some performance metric, we might want to compare several alternatives:\nThe question “What color should the donate button be on an NGO’s website?” is really lots of questions about the effect of different button colors on donations.\n\nFactual: A voter donates some amount with a black button\nCounterfactual: What would a voter donate if the button were blue?\nCounterfactual: What would a voter donate if the button were red?\n\nWhat other causal questions might social scientists or data scientists ask?\n\n\n3.1.2 Causal Effects\nWhen we are conducting a causal analysis, we will want to estimate a causal effect.\n\nCausal effects are all about ideal comparisons between treated vs. untreated\n\nA causal effect is the change in the outcome Y that is caused by a change in the treatment variable.\n\n\\(Y(1) - Y(0)\\) = causal effect or “treatment effect”\ne.g., Donation if contacted - Donation if not contacted\n\nWe often want to know the average treatment effect in some population, not just the causal effect for a single individual. Here, we might ask, on average, how much would our outcome change if our units were treated instead of untreated. To do so, we simply sum up all of the causal effects and divide them by the number of units in our population.\n\n\\(\\frac{1}{N} \\sum_{i=1}^N (Y_i (1)-Y_i (0))\\) = “average treatment effect” (ATE)\n\ne.g., Average donations if contacted - Average donations if not contacted\n\n\nNote: If the math above is helpful, you can use it. If it is difficult to read, focus on the plain language definitions that go before it. The notation here is less important than the conceptual understanding.\n\n\n3.1.3 Fundamental Problem of Causal Inference\nThe problem: Fundamental Problem of Causal Inference\nWhat makes the evaluation of causal claims difficult, is that in the real world, we suffer from the fundamental problem of causal inference:\n\nFor any individual, we only get to see (observe) the result from one state of the world\n\nThis makes that subtraction of potential outcomes impossible.\n\n\n(Unless we are in Groundhog Day"
  },
  {
    "objectID": "03-CausalityI.html#randomized-controlled-trials",
    "href": "03-CausalityI.html#randomized-controlled-trials",
    "title": "3  Causation with Experiments",
    "section": "3.2 Randomized Controlled Trials",
    "text": "3.2 Randomized Controlled Trials\nOne approach for addressing the fundamental problem of causal inference is to simulate two potential states of the world through random assignment: Randomized Controlled Trials / Experiments\nExperiments approximate an ideal factual vs. counterfactual comparison\n\nWe randomly assign one group to receive a “treatment” and another not to receive a treatment (the control)\n\nThere can be more than two groups. The key is that each group varies (is manipulated) in some way.\n\nWhen treatment assignment is randomized, the only thing that distinguishes the treatment group from the control group, besides the treatment itself, is chance.\n\n\n\n\nSalganik Bit by Bit Chapter 4.4\n\n\nThis allows us to compare the average outcomes between groups in order to estimate our causal effects (more on this below).\n\n3.2.1 Experiments: Why Randomize?\nRandomization is essential for being able to identify and isolate the causal effect of the treatment on the outcome.\nWithout randomization, there may be several reasons why two groups differ beyond the treatment of interest.\n\nFor example, if we randomly assigned half of Rutgers seniors to watch the movie Oppenheimer and half to watch Barbie we would expect the groups to have about equal proportions of female students, average age, racial composition, majors, etc.\n\n(If we didn’t randomly assign, and just let people “select” into watching a particular movie, the groups could look very different.)\n\n\nBut because we randomized assignment, on average, we’d expect the two groups to be identical except for the treatment– in this case, which movie they watched.\n\nGreat news! This means any differences in the outcomes between the two groups can be attributed to the treatment. So if we wanted to see if Top Gun Maverick leads people to take up flying lessons, we could compare the average number of flying lessons among seniors who watched Top Gun Maverick compared to those who didn’t, and instead watched a different movie.\n\n\n\n3.2.2 Experiments: How to Analyze\nDifference in Means: We compare each group’s average outcome by subtracting one from the other to estimate the average treatment effect (ATE) aka the average causal effect of the treatment.\n\n\\(\\widehat{ATE} = \\bar{Y}(treatment) - \\bar{Y}(control)\\)\n\nThis is an estimate of, on average, how much our outcome would change if units went from being untreated to treated.\n\nE.g., on average how much a person donates to a campaign if contacted by phone compared to if not contacted by phone.\n\n\n\n3.2.3 Ingredients of an Experiment\nFrom Bit by Bit\n\nFor every experiment, you should be able to\n\nState the causal question or relationship of interest\nDescribe how the experiment will be implemented (e.g., recruitment of subjects)\nIdentify and describe the randomization into treatment group(s) and control group and what happens in each group\nIdentify the outcome of interest, how it is measured\nEvaluate the relevant comparison (between two different experimental conditions)\n\nWe will turn to an example in the next section."
  },
  {
    "objectID": "03-CausalityI.html#application-is-there-racial-discrimination-in-the-labor-market",
    "href": "03-CausalityI.html#application-is-there-racial-discrimination-in-the-labor-market",
    "title": "3  Causation with Experiments",
    "section": "3.3 Application: Is there racial discrimination in the labor market?",
    "text": "3.3 Application: Is there racial discrimination in the labor market?\nMarianne Bertrand and Sendhil Mullainathan. 2004. “Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination.”\n“We perform a field experiment to measure racial discrimination in the labor market. We respond with fictitious resumes to help-wanted ads in Boston and Chicago newspapers.”\n\nRecruitment: Construct resumes to send to ads\nRandomization: To manipulate perception of race, each resume is (randomly) assigned\nTreatment: either a very African American sounding name\nControl: or a very White sounding name\nOutcome: Does the resume receive a callback?\nComparison: Callback rates for African American (sounding) names vs. White (sounding) names (the difference in means between groups)\n\nFor a video explainer of the code in this section, see below. The video only discusses the code. Use the notes and lecture discussion for additional context. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nLet’s load the data. Note: When we have variables that are text-based categories, we may want to tell R to treat these “strings” of text information as factor variables, a particular type of variable that represents data as a set of nominal (unordered) or ordinal (ordered) categories. We do this with the stringsAsFactors argument.\n\nresume &lt;- read.csv(\"resume.csv\", stringsAsFactors = T)\n\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\",\n                   stringsAsFactors = T)\n\nVariables and Description\n\nfirstname: first name of the fictitious job applicant\nsex: sex of applicant (female or male)\nrace: race of applicant (black or white)\ncall: whether a callback was made (1 = yes, 0 = no)\n\nThe data contain 4870 resumes and 4 variables.\n\nnrow(resume) # number of rows\n\n[1] 4870\n\nncol(resume) # number of columns\n\n[1] 4\n\ndim(resume) # number of rows and columns\n\n[1] 4870    4\n\n\nNote: These data look a little different from what we used last week. For example, the sex and race variables contain words, not numbers.\n\nhead(resume)\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n5    Carrie female white    0\n6       Jay   male white    0\n\n\n\n3.3.1 Variable classes\nWe can check the class of each variable: Look, we have a new type, a “factor” variable.\n\nclass(resume$firstname)\n\n[1] \"factor\"\n\nclass(resume$sex)\n\n[1] \"factor\"\n\nclass(resume$race)\n\n[1] \"factor\"\n\nclass(resume$call)\n\n[1] \"integer\"\n\n\nWe have now encountered numeric, character, and factor vectors and/or variables in R. Note: This is simply how R understands them. Sometimes R can get it wrong. For example, if we write:\n\nsomenumbers &lt;- c(\"1\", \"3\", \"4\")\nclass(somenumbers)\n\n[1] \"character\"\n\n\nBecause we put our numbers in quotation marks, R thinks the values in somenumbers are text. The number “3” might as well be the word “blue” for all R knows. Fortunately, we can easily switch between classes.\n\nsomenumbers &lt;- as.numeric(somenumbers)\nclass(somenumbers)\n\n[1] \"numeric\"\n\n\nHere, we used as.numeric() to overwrite and change the character vector into a numeric vector.\nRules of Thumb\n\nUsually, we want character variables to store text (e.g., open-ended survey responses)\nWe want numeric variables to store numbers.\nUsually, we want factor variables to store categories.\n\nWithin R, factor variables assign a number to each category, which is given a label or level in the form of text.\nCategories might be ordinal or “ordered” (e.g., Very likely, Somewhat likely, Not likely) or\nUnordered (e.g., “male”, “female”)\nR won’t know if a factor variable is ordered or unordered. Alas, we have to be smarter than R.\nR might think you have a character variable when you want it to be a factor or the reverse.\n\nThat’s when as.factor() and as.character() are useful.\n\n\nAlways check class() to find out the variable type"
  },
  {
    "objectID": "03-CausalityI.html#making-tables",
    "href": "03-CausalityI.html#making-tables",
    "title": "3  Causation with Experiments",
    "section": "3.4 Making tables",
    "text": "3.4 Making tables\nA nice thing about numeric and factor variables is we can use the table command to see how many observations in our data fall into each category or numerical value.\n\n## Example: how many black vs. white sounding resumes\ntable(resume$race)\n\n\nblack white \n 2435  2435 \n\n\nAs mentioned, factor variables have levels:\n\nlevels(resume$race)\n\n[1] \"black\" \"white\"\n\n\n\n3.4.1 Crosstabulation\nWe can also use the table command to show a crosstabulation: a table that displays the frequency of observations across two variables.\n\n## Example: how many black vs. white sounding resumes by call backs\n## We can label the two dimensions of the table with the =\ntable(calledback = resume$call, race = resume$race)\n\n          race\ncalledback black white\n         0  2278  2200\n         1   157   235"
  },
  {
    "objectID": "03-CausalityI.html#conditional-means",
    "href": "03-CausalityI.html#conditional-means",
    "title": "3  Causation with Experiments",
    "section": "3.5 Conditional Means",
    "text": "3.5 Conditional Means\nRecall how to take a mean of a variable in our data. For example, let’s take the mean of the variable call.\n\nmean(resume$call)\n\n[1] 0.08049281\n\n\nThis gives us the average callbacks (or callback rate) for everyone in our data. In experiments, we want to take the mean for a specific group within our data– the treatment group, and then the mean for the control group.\nSomehow, we have to identify, within our data, which rows were part of the treatment group and which were a part of the control group. In this study, we want to identify resumes with an assigned name perceived to be black vs. perceived to be white. This is in our race variable.\nWe will cover a couple of tools to do this, with the first being tapply.\nTo find how the average of one variable (e.g., our outcome- the callback rate) varies across different categories of our factor variable, we use tapply().\n\n## take the mean of input1 by categories of input2\n## mean of the call variable conducted separately by race\ntapply(resume$call, INDEX=resume$race, mean)\n\n     black      white \n0.06447639 0.09650924 \n\n\nThis tells us the callback rate for each group of people in our data. That’s not the only way to do this, however. We can also use the tools below."
  },
  {
    "objectID": "03-CausalityI.html#relational-operators-in-r",
    "href": "03-CausalityI.html#relational-operators-in-r",
    "title": "3  Causation with Experiments",
    "section": "3.6 Relational Operators in R",
    "text": "3.6 Relational Operators in R\nGoal: Compare callback rates for white sounding names to black sounding names, so we need to be able to filter by race.\nGood news: We have several relational operators in R that evaluate logical statements:\n\n==, &lt;, &gt;, &lt;=, &gt;=, !=\nWe have a statement and R evaluates it as TRUE or FALSE\n\n\n## for each observation, does the value of race equal \"black\"?\nresume$race == \"black\"\n\nBy putting this logical statement within [ ], we are asking R to take the mean() of the variable resume$call for the subset of observations for which this logical statement is TRUE.\n\nmean(resume$call[resume$race == \"black\"])\n\n[1] 0.06447639\n\n\nUltimately, each of these paths has led us to a place where we can estimate the average treatment effect by calculation the difference in means: the difference in callback rates for black and white applicants.\nWe said the ATE = \\(\\bar{Y}(treatment) - \\bar{Y}(control)\\)\n\nate &lt;- mean(resume$call[resume$race == \"black\"]) - \n  mean(resume$call[resume$race == \"white\"])\nate\n\n[1] -0.03203285\n\n\nHow can we interpret this? Do white applicants have an advantage?"
  },
  {
    "objectID": "03-CausalityI.html#subsetting-data-in-r",
    "href": "03-CausalityI.html#subsetting-data-in-r",
    "title": "3  Causation with Experiments",
    "section": "3.7 Subsetting data in R",
    "text": "3.7 Subsetting data in R\nSubsetting Dataframes in R\nMaybe we are interested in differences in callbacks for females. One approach for looking at the treatment effect for female applicants, only, is to subset our data to include only female names.\n\nTo do this, we will assign a new data.frame object that keeps only those rows where sex == \"female\" and retains all columns\nBelow are two approaches for this subsetting, one that uses brackets and one that uses the subset function\n\n\n## option one\nfemales &lt;- resume[resume$sex == \"female\", ]\n## option two using subset()- preferred\nfemales &lt;- subset(resume, sex == \"female\")\n\nNow that we have subset the data, this simplifies estimating the ATE for female applicants only.\nWe said the ATE = \\(\\bar{Y}(treatment) - \\bar{Y}(control)\\)\n\nate.females &lt;- mean(females$call[females$race == \"black\"]) -\n  mean(females$call[females$race == \"white\"])\nate.females\n\n[1] -0.03264689\n\n\n\n3.7.1 Getting Booooooooolean\nWe can make this slightly more complex by adding more criteria. Let’s say we wanted to know the callback rates for just female black (sounding) names.\n\nR allows use to use & (and) and | (or)\n\n\nfemaleblack &lt;- subset(resume, sex == \"female\" & race == \"black\")\n\nWe could now find the callback rate for Black females using the tools from above:\n\nmean(femaleblack$call)\n\n[1] 0.06627784"
  },
  {
    "objectID": "03-CausalityI.html#creating-new-variables-using-conditional-statements",
    "href": "03-CausalityI.html#creating-new-variables-using-conditional-statements",
    "title": "3  Causation with Experiments",
    "section": "3.8 Creating New Variables using Conditional statements",
    "text": "3.8 Creating New Variables using Conditional statements\nWe can instead create a new variable in our main dataframe. Let’s make a variable that takes the value 1 if a name is female and black sounding and 0, otherwise\n\n# Initialize a new variable called femaleblackname\nresume$femaleblackname &lt;- NA\n# Assign a 1 to our new variable where sex is female and race is black\nresume$femaleblackname[resume$sex == \"female\" & resume$race == \"black\"] &lt;- 1\n# Assign a 0 if sex is not female OR if race is not black\nresume$femaleblackname[resume$sex != \"female\" | resume$race != \"black\"] &lt;- 0\n\nWe can check our work\n\ntable(name = resume$firstname, femaleblack = resume$femaleblackname)\n\n          femaleblack\nname         0   1\n  Aisha      0 180\n  Allison  232   0\n  Anne     242   0\n  Brad      63   0\n  Brendan   65   0\n  Brett     59   0\n  Carrie   168   0\n  Darnell   42   0\n  Ebony      0 208\n  Emily    227   0\n  Geoffrey  59   0\n  Greg      51   0\n  Hakim     55   0\n  Jamal     61   0\n  Jay       67   0\n  Jermaine  52   0\n  Jill     203   0\n  Kareem    64   0\n  Keisha     0 183\n  Kenya      0 196\n  Kristen  213   0\n  Lakisha    0 200\n  Latonya    0 230\n  Latoya     0 226\n  Laurie   195   0\n  Leroy     64   0\n  Matthew   67   0\n  Meredith 187   0\n  Neil      76   0\n  Rasheed   67   0\n  Sarah    193   0\n  Tamika     0 256\n  Tanisha    0 207\n  Todd      68   0\n  Tremayne  69   0\n  Tyrone    75   0\n\n\nLet’s say we wanted to know the callback rates for just female black (sounding) names.\n\nmean(femaleblack$call)\n\n[1] 0.06627784\n\nmean(resume$call[resume$femaleblackname == 1])\n\n[1] 0.06627784\n\n\nBINGO: two ways to do the same thing.\n\n3.8.1 ifelse statements\nRemember how we created the variable femaleblack, well there is another way to do that in R using what are called conditional statements with ifelse().\n\nCan be read: If this relational statement is TRUE, I assign you A, otherwise I assign you B\n\n\nresume$femaleblackname &lt;- ifelse(resume$sex == \"female\" &\n                                   resume$race == \"black\", 1, 0)\n\nCan be read: If sex is female and race is black, give the observation in the new variable a 1, otherwise give it a 0.\nLike most things, we can also get more complicated here. Let’s say we wanted to create a variable that indicated both race and sex.\n\nCan be read: If this relational statement is TRUE, I assign you A,\nOtherwise if this second relational statement is TRUE, I assign you B,\nOtherwise if this third relational statement is TRUE, I assign you C,\nOtherwise I assign you D\n\n\nresume$racesex &lt;- ifelse(resume$sex == \"female\" &\n                                   resume$race == \"black\", \"FemaleBlack\", \n                         ifelse(resume$sex == \"female\" &\n                                   resume$race == \"white\", \"FemaleWhite\",\n                                ifelse(resume$sex == \"male\" &\n                                   resume$race == \"white\", \"MaleWhite\", \"MaleBlack\")))\n\nNote: what you assign can be numeric or text."
  },
  {
    "objectID": "03-CausalityI.html#types-of-experiments",
    "href": "03-CausalityI.html#types-of-experiments",
    "title": "3  Causation with Experiments",
    "section": "3.9 Types of Experiments",
    "text": "3.9 Types of Experiments\nExperiments can vary:\n\nSetting: Lab, Survey, Field\nMode: Analog vs. Digital\nAnd in Validity\n\nInternal: were the processes conducted in a correct, reliable way?\nExternal: can we generalize from the experiment to the real world, or would the results change?\nContext: Would people act the same way outside of the experiment?\nRecruitment: Are the people in our experiment representative of the people we care about?\nConstruct\n\nTreatment: Is the experimental treatment similar to what people see in the real world?\nOutcome: Is the outcome something we care about in the real world? Are we measuring it in a realistic, accurate way?\n\n\n\nReview Bit by Bit chapter 4 for more examples of social science experiments.\nExample: Televised Incivility, Trust and Emotions (Mutz and Reeves)\n\nParticipants sat alone in a room with electrodes attached to their hands to measure skin conductance. Subjects viewed 20 minutes of a political debate created for the experiment, which varied in civility and politeness. Results showed respondents had more of an emotional response to the uncivil condition and expressed less trust in politicians.\nExample: Online Survey Experiment\nAudience Costs (Tomz)\nA country sent its military to take over a neighboring country. The attacking country was led by a [dictator, who invaded OR democratically elected government, which invaded] [to get more power and resources OR because of a longstanding historical feud.\nThe attacking country had a [strong military, so it would OR weak military, so it would not] have taken a major effort for the United States to help push them out.\nA victory by the attacking country would [hurt OR not affect] the safety and economy of the United States.\n\nParticipants provided a different version of the vignette above, and a reaction by the president\nPresidential approval varies depending on the president’s response and the nature of the situation\n\nExample: Digital Field Experiments in Campaigns\nExample: A/B Testing in Campaigns\n\nEmails are virtually costless. Very easy to ask: Are people more likely to open them with X subject or Y subject or Z subject?"
  },
  {
    "objectID": "03-CausalityI.html#wrapping-up-causation-with-experiments",
    "href": "03-CausalityI.html#wrapping-up-causation-with-experiments",
    "title": "3  Causation with Experiments",
    "section": "3.10 Wrapping Up Causation with Experiments",
    "text": "3.10 Wrapping Up Causation with Experiments\nIn this section, we have discussed what it means to make a causal claim, why it is essentially impossible to make causal comparisons in real life due to the fundamental problem of causal inferences, and how experiments can help us make comparisons that approximate our causal ideals.\nIn the next section, we start to examine how to visualize data.\n\n3.10.1 Summary of R tools in this section\nHere are some of the R tools we used in this section:\n\ntable(): this function summarizes the frequency of observations that take a particular value. The input is one or more variables in your data.\n\nE.g., table(resume$sex) or table(resume$sex, resume$call)\n\ntapply(): this function applies a given operation like mean to whichever variable is in the first position, separately or “conditionally” by different values of the variable in the second “index” position.\n\nE.g., tapply(resume$call, INDEX=resume$race, mean) finds the average callbacks for applicants separately for different races of applicants in the data.\n\n== &gt; &lt; &gt;= &lt;= !=: Relational operators help us set up “logical statements” in R that are evaluated as TRUE or FALSE\n\nE.g., resume$race == \"black\" evaluates whether for each observation in the race column is “black” in which case the statement is TRUE or not black, in which case the statement is FALSE\nE.g., resume$call &lt; 1 evaluates whether for each observation in the call column has a value less than one in which case the statement is TRUE or not less than 1, in which case the statement is FALSE\nWe can then isolate certain parts of columns using relational operators and the brackets []. For example we can take the mean callbacks for applicants who are black using mean(resume$call[resume$race == \"black\"])\n\n& and |: These are boolean operators that allow us to combine multiple relational operators using an AND statement (&) or an OR statement |. Note the bar is a bar that is usually above your backslash key and not a capitalized i.\n\nE.g., mean(resume$call[resume$race == \"black\" & resume$sex == \"female\"])\n\nsubset(): We can subset whole rows of our data using this function. It takes two inputs– the first is the name of the original dataframe, and the second is a relational statement. Usually we store this output in R by assigning the results to a new object, a dataframe that contains only those rows for which the logical statement using the relational operators is true. E.g., females &lt;- subset(resume, sex == \"female\") subsets our data to keep only those rows where applicants were female."
  },
  {
    "objectID": "04-Visualization.html#application-social-status-and-economic-views",
    "href": "04-Visualization.html#application-social-status-and-economic-views",
    "title": "4  Visualization",
    "section": "4.1 Application: Social Status and Economic Views",
    "text": "4.1 Application: Social Status and Economic Views\nWe are going to explore different types of visualizations through different social science examples. The first application we visit is a survey experiment.\nThal, A. (2020). The desire for social status and economic conservatism among affluent Americans. American Political Science Review, 114(2), 426-442.\nIn the experiment, affluent Americans are randomly assigned to encounter Facebook posts in which others broadcast their economic success. These posts are designed in a way that encourages affluent respondents to view economic success as a means of achieving social status.\nCausal claims\n\n“I expect that exposure to these posts will cause affluent Americans to become more supportive of conservative economic policies.”\n“I also expect that exposure to these posts will cause especially large increases in economic conservatism among affluent men.”\n\nThe experiment includes a sample of 2010 affluent Americans– people who report household incomes in the top 10 percent of the U.S. income distribution.\nExperiment Ingredients:\n\nCausal Question: Does desire for social status influence economic views of affluent Americans?\nRecruitment: Ask affluent Americans to take a survey online\nRandomization: Randomly assign respondents to view different fictional Facebook posts designed to signal different motivations\nOutcome: an index based on respondents’ support for decreasing “taxes on households making $150,000 or more a year,” support for decreasing the “taxes on money people make from selling investments, also referred to as capital gains,” and support for decreasing “government regulation of business and industry.”\nComparison: Average economic views between experimental conditions.\n\nSnapshot of status conditions\n\nSnapshot of Concrete and Placebo comparison conditions\n\nCan you put this into the potential outcomes framework?"
  },
  {
    "objectID": "04-Visualization.html#boxplots",
    "href": "04-Visualization.html#boxplots",
    "title": "4  Visualization",
    "section": "4.2 Boxplots",
    "text": "4.2 Boxplots\nFor a video explainer of the code for boxplots and barplots, see below. The video only discusses the code. Use the notes and lecture discussion for additional context. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nLet’s load the data! Here, note that the data file is in a .RData format instead of .csv. This means that instead of using read.csv, we should use a function to load the data that is suitable for the .RData format. This will be load. That function works the following way:\n\nload(\"status.RData\")\n\nAfter running the above code, an object will show up in your R environment.\n\nhead(status)\n\n        condition male   econcon\n2        Concrete    1 0.7500000\n3     Self-Esteem    1 1.0000000\n4         Placebo    1 0.6666667\n5     Self-Esteem    0 0.2500000\n6     Self-Esteem    0 1.0000000\n7 Social Approval    0 0.8333333\n\n\nThe data include the following variables\n\ncondition: Placebo, Concrete, Self-Esteem, Social Approval, Conspicuous Consumption\ngender: 1= male; 0= otherwise\neconcon: Economic views. Numeric variable from 0 to 1, with higher values reflecting more conservative views\n\n\n4.2.1 Data Summary: Boxplot\nCharacterize the distributions of continuous numeric variables at once\n\nFeatures: box, whiskers, outliers\nWe will supply the function with a column in our data, and the boxplot displays the distribution of that variable.\n\n\nFigure from Will Lowe\nHere is an example of the boxplot using our econcon variable.\n\nWe have added a title and y-axis label to the plot through the main and ylab arguments. Play around with changing the words in those arguments.\n\n\nboxplot(status$econcon,\n        main=\"Economic Views in the Survey Sample\",\n        ylab=\"Economic Views\")\n\n\n\n\nAfter you execute the plot code, a preview of the plot should appear in the bottom-right window of RStudio.\nBoxplots are also useful for data summary across multiple distribution: boxplot(y ~ x, data = d)\n\nboxplot(econcon ~ condition, data=status,\n        main=\"Economic Views by Experimental Condition\",\n        ylab=\"Economic Views\",\n        names = c(\"Placebo\", \"Concrete\", \"Conspicuous\", \n                  \"Self-Esteem\", \"Social\"),\n        xlab = \"Experimental Condition\",\n        col = c(\"red3\", rep(\"dodgerblue\", 4)))\n\n\n\n\nThe additional arguments are just aesthetics. Play around with different settings.\n\nFor example, can you change the code to make the first two boxes red? Colors are supplied as a vector using the col = argument.\n\nTo explore colors in R, run this function colors() in your R console.\n\n\nHow should we interpret these results? Does status or social approval motivation, specifically, influence economic views? What about other potential motivations?"
  },
  {
    "objectID": "04-Visualization.html#barplots",
    "href": "04-Visualization.html#barplots",
    "title": "4  Visualization",
    "section": "4.3 Barplots",
    "text": "4.3 Barplots\nComparing frequencies (raw N), proportions, and/or means across categories\n\n\n\n\n\nWe will use the barplot() function.\n\nIn contrast to the boxplot, the barplot function takes a vector of values that will serve as the top of the bars in the plot– it does not summarize a variable from within the function\n\nE.g., we could supply it a set of means to plot, not a raw variable\n\nMany of the other arguments are aesthetics similar to those when working with boxplot.\nThis means that barplots are pretty easy to create in R. We can supply it a short vector of any values (e.g., valuesbar &lt;- c(20, 30, 40, 10)), and we could also supply it a vector of any names to label those values.\n\n\n## Example\nvaluesbar &lt;- c(20, 30, 40, 10)\n\nnamesbar &lt;- c(\"Livingston Dining \\n Commons\",\n              \"Neilson \\n Dining Hall\",\n              \"Busch \\n Dining Hall\",\n              \"Brower \\n Commons\")\n\nbarplot(valuesbar,\n        names=namesbar,\n        cex.names = .6,\n        main=\"Hypothetical Evaluation of RU Dining\",\n        ylab=\"Percent Prefer Dining Option\",\n        cex.lab = .7, \n        col=\"red3\")\n\n-   For real applications, this means we could supply a barplot with the output of a `tapply()` function, a `table()` summarizing a single variable, or a set of `mean()` values we have combined into a vector using `c()`.\nFor example, in experiments, we may use barplots to compare the mean from the treatment group(s) \\(\\bar{Y}(1)\\) to the control \\(\\bar{Y}(0)\\) on some outcome. Let’s do it!\n\nFirst, we need the means. Let’s find the conditional means of economic views.\n\n\ncondmeans &lt;- tapply(status$econcon, status$condition, mean)\ncondmeans # save as object to supply to the barplot function\n\n                Placebo                Concrete Conspicuous Consumption \n              0.6340948               0.6647485               0.6724065 \n            Self-Esteem         Social Approval \n              0.6564103               0.6904444 \n\n\nThe first input is the vector of means/proportions/frequency you want to plot.\n\nbarplot(condmeans,\n        ylim =  c(0,1), # y-axis dimensions\n        names = c(\"Placebo\", \"Concrete\", \"Conspicuous\", \n                  \"Self-Esteem\", \"Social\"),\n        col = \"black\", # color of bars\n        main = \"Mean Economic Views by Condition\", # plot title\n        cex.main = .8, # size of plot title\n        cex.names = .8, # size of name labels\n        ylab = \"Mean Views\", # yaxis label\n        cex.lab = .8,# size of yaxis label\n        las = 1) # controls angle of axis labels\n\n\n\n\nThe remaining arguments alter the look of the plot to make it more informative.\n\nHow could we improve this plot to make the interpretation easier?\n\n\n4.3.1 Saving Plots\nYou can save an image of your plot as a png() to your working directory. Place png() just before your plot with a name in quotations, and then specify the dimensions. Place dev.off() at the bottom.\n\npng(\"mybarplot.png\", width = 7, height = 4, res=300, units=\"in\")\nbarplot(condmeans,\n        ylim =  c(0,1), # y-axis dimensions\n        names = c(\"Placebo\", \"Concrete\", \"Conspicuous\", \n                  \"Self-Esteem\", \"Social\"),\n        col = \"black\", # color of bars\n        main = \"Mean Economic Views by Condition\", # plot title\n        cex.main = .8, # size of plot title\n        cex.names = .8, # size of name labels\n        ylab = \"Mean Views\", # yaxis label\n        cex.lab = .8,# size of yaxis label\n        las = 1) # controls angle of axis labels\ndev.off()\n\nAlternatively, you can save it as an image, by going to the plot window in your RStudio environment, and clicking on Export -&gt; Save as Image. Here, you can save it in any file format you would like, as well as change the dimensions.\n\n\n\n4.3.2 Creating New Variables\nThe author theorizes that social approval, self-esteem, and conspicuous consumption are all elements of “status motivation.” We could analyze the results by collapsing them into a single category called “status motivation” and compare it to the other experimental groups.\n\nCreate a new variable conditionnew\nCode the variable into new categories based on the values in the original condition variable\nCheck the class of the new variable and convert if necessary\nVerify new variable by exploring values\n\n\nstatus$conditionnew &lt;- NA # create new variable\n## Code new variable\nstatus$conditionnew[status$condition == \"Placebo\"] &lt;- \"Placebo\"\nstatus$conditionnew[status$condition == \"Concrete\"] &lt;- \"Concrete\"\nstatus$conditionnew[status$condition == \"Conspicuous Consumption\" |\n                     status$condition == \"Self-Esteem\" |  \n                      status$condition == \"Social Approval\"] &lt;- \"Status\"\n\n# class(status$conditionnew) check the class\nstatus$conditionnew &lt;- as.factor(status$conditionnew) # convert\n\nRecall, an alternative way to create the new variable is through an ifelse statement.\n\nCan be read: If this relational statement is TRUE, I assign you A, otherwise I assign you B\nThis often works best when we change factor variables to character\n\n\nstatus$conditionnew2 &lt;- as.character(status$condition)\nstatus$conditionnew2 &lt;- ifelse(status$condition == \"Conspicuous Consumption\" |\n                     status$condition == \"Self-Esteem\" |  \n                      status$condition == \"Social Approval\", \n                     \"Status\", status$conditionnew2)\nstatus$conditionnew2 &lt;- as.factor(status$conditionnew2)\ntable(status$conditionnew2)\n\n\nConcrete  Placebo   Status \n     391      394     1157 \n\n\nNote: Barplots don’t have to display means. We could also display frequencies. For example, let’s make a plot of the number of people in each condition using our new variable.\n\nfreqobs &lt;- table(status$conditionnew)\n\n\nbarplot(freqobs,\n        ylim = c(0, 1200),\n        col = \"black\", # color of bars\n        main = \"Number of People per Condition\", # plot title\n        cex.main = .8, # size of plot title\n        cex.names = .8, # size of name labels\n        ylab = \"N of Observations\", # yaxis label\n        cex.lab = .8,# size of yaxis label\n        las = 1) # controls angle of axis labels"
  },
  {
    "objectID": "04-Visualization.html#application-changing-minds-on-gay-marriage",
    "href": "04-Visualization.html#application-changing-minds-on-gay-marriage",
    "title": "4  Visualization",
    "section": "4.4 Application: Changing Minds on Gay Marriage",
    "text": "4.4 Application: Changing Minds on Gay Marriage\nWe now turn to a study that asks the question\n\nResearch Question Can we effectively persuade people to change their minds?\n\nContact Hypothesis: outgroup hostility diminishes through extended positive contact\n\n\nThe authors conduct two randomized control trials in Los Angeles\n\nTarget population: voters in Los Angeles\nRecruitment: select people from a registered voter list\nRandomized treatment conditions:\n\nCanvassers have a conversation about same-sex marriage vs.\nRecycling scripts (placebo)\nControl group: no canvassing\n\nOutcome measures:\n\nFeeling towards gay couples (survey responses over multiple waves)\n\nComparison\n\nCompare average change in feelings between treatment conditions\n\n\nLet’s load the data. Data available through QSS. See QSS Chapter 2 for additional discussion.\n\nstudy: Which study is the data from (1 = Study1, 2 = Study2)\ntreatment: Five possible treatment assignment options\ntherm1: Survey thermometer rating of feeling towards gay couples in waves 1 (0–100) (asked before people were canvassed)\ntherm2: Survey thermometer rating of feeling towards gay couples in waves 2 (0–100) (asked after people were canvassed)\n\n\nmarriage &lt;- read.csv(\"gayreshaped.csv\", stringsAsFactors = T)\n\n\n## How many rows and columns\ndim(marriage)\n\n[1] 11948     6\n\n## How many observations in each treatment group, in each study\ntable(marriage$treatment, marriage$study)\n\n                                                \n                                                    1    2\n  No Contact                                     5238 1203\n  Recycling Script by Gay Canvasser              1046    0\n  Recycling Script by Straight Canvasser         1039    0\n  Same-Sex Marriage Script by Gay Canvasser      1151 1238\n  Same-Sex Marriage Script by Straight Canvasser 1033    0\n\n\nFor a video explainer of the code for the barplot, scatter plot and histogram created with this application, see below. The video only discusses the code. Use the notes and lecture discussion for additional context. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nLet’s focus on study 1 only.\n\nmarriage1 &lt;- subset(marriage, study ==  1)\n\nWe have to do some work to prepare our outcome and treatment conditions.\nIn experiments, we compare the mean from the treatment group(s) \\(\\bar{Y}(1)\\) to the control \\(\\bar{Y}(0)\\) on some outcome\n\nHere are outcome is Change in Support for gay couples: Wave 2 - Wave 1 feeling thermometer scores\n\n\nmarriage1$outcome &lt;- marriage1$therm2 - marriage1$therm1\n\n\n4.4.1 Recall: Creating new variables\nLet’s create a new variable treatmentnew that collapses the two Recycling and Same-Sex marriage conditions.\n\nmarriage1$treatmentnew &lt;- NA\nmarriage1$treatmentnew[marriage1$treatment == \"No Contact\"] &lt;- \"No Contact\"\nmarriage1$treatmentnew[marriage1$treatment == \"Recycling Script by Gay Canvasser\" |\n                         marriage1$treatment == \n                         \"Recycling Script by Straight Canvasser\"] &lt;- \"Recycling\"\nmarriage1$treatmentnew[marriage1$treatment == \"Same-Sex Marriage Script by Gay Canvasser\" |\n                         marriage1$treatment ==\n                         \"Same-Sex Marriage Script by Straight Canvasser\"] &lt;- \"Marriage\"\nmarriage1$treatmentnew &lt;- as.factor(marriage1$treatmentnew)\n\ntable(marriage1$treatmentnew)\n\n\n  Marriage No Contact  Recycling \n      2184       5238       2085 \n\n\n\n\n4.4.2 Recall: Using ifelse to create new variable\nAn alternative way we could create a variable is to use ifelse\nLet’s try another way using the ifelse command.\n\nCan be read: If this relational statement is TRUE, I assign you A (in this case “No Contact”), otherwise (ifelse())\nif this alternative relational statement is TRUE, I assign you B (in this case “Recycling”), otherwise (ifelse())\nif this alternative relational statement is TRUE, I assign you C (in this case “Marriage”), otherwise\nIf all of those were FALSE I assign you D (in this case an NA)\n\n\nmarriage1$treatmentnew2 &lt;- ifelse(marriage1$treatment == \"No Contact\", \"No Contact\",\n                                  ifelse(marriage1$treatment == \n                                           \"Recycling Script by Gay Canvasser\" |\n                                           marriage1$treatment ==  \n                                           \"Recycling Script by Straight Canvasser\",  \n                                         \"Recycling\",\n                                    ifelse(marriage1$treatment ==\n                                        \"Same-Sex Marriage Script by Gay Canvasser\" |  \n                                            marriage1$treatment ==\n                                        \"Same-Sex Marriage Script by Straight Canvasser\",\n                                        \"Marriage\", \n                                        NA)))\nmarriage1$treatmentnew2 &lt;- as.factor(marriage1$treatmentnew2)\n\n\n\n4.4.3 Calculating the Average Treatment Effect\nWe now have our outcome and our treatment conditions. In an experiment, we want to look at the difference in means between conditions. Let’s calculate the means.\n\nouts &lt;- tapply(marriage1$outcome, marriage1$treatmentnew, mean, na.rm=T)\n\nNote: Sometimes data include missing cells. In R, these have an NA. To ignore these when calculating a mean, we add na.rm = T to the mean() or tapply() functions.\n\n\n4.4.4 Visualize means in a barplot\nLet’s also add a line at 0 using abline()\n\nbarplot(outs,\n        col=\"black\",\n        ylim =  c(-2, 2), # y-axis dimensions\n        border = NA, # removes bar borders\n        main = \"Change in FT W2-W1 by Type of Treatment\", # plot title\n        cex.main = .8, # size of plot title\n        ylab = \"Mean Change in FT W2-W1\", # yaxis label\n        cex.lab = .8,# size of yaxis label\n        las = 1) # controls angle of axis labels\nabline(h=0, lty=2, col = \"red\", lwd=2) # adds horizontal line at 0 with dashes \n\n\n\n\nHow should we interpret these results?\n\nIn the Marriage condition, it looks like on average, views toward gay couples became warmer (the bar is positive) after the conversations with canvassers about same-sex marriage.\nIn contrast, the views of people in the Recycling or No Contact conditions did not change much and if anything, became slightly colder.\nComparing between these bars, then, it seems like there is an “average treatment effect” given that the change in the Marriage condition was different from the Recycling an No Contact control groups."
  },
  {
    "objectID": "04-Visualization.html#scatterplots",
    "href": "04-Visualization.html#scatterplots",
    "title": "4  Visualization",
    "section": "4.5 Scatterplots",
    "text": "4.5 Scatterplots\nIt turns out that study was completely fabricated, and the article was eventually retracted.\nHow did people know? Well a team of researchers became suspicious based on exploratory analyses they conducted with the data. Let’s do a few of these to learn about scatterplots and histograms.\nScatter plots show the relationship between two numeric variables.\nA common way to describe and quantify a relationship is through correlation.\n\nCorrelation: When \\(x\\) changes, \\(y\\) also changes by a fixed proportion\n\nAsks: If you are a certain degree above the mean of \\(x\\), are you similarly that much above the mean of \\(y\\)?\nPositive correlation: data cloud slopes up;\nNegative correlation: data cloud slopes down;\nHigh positive or negative correlation: data cluster tightly around a sloped line\nNot affected by changes of scale: cm vs. inch, etc.\n\n\nRange of Correlation is between \\(-1\\) and \\(1\\)\n\nLook at the graphs below for examples of high and low positive and negative correlations.\n\n\n\n\nFrom R for Dummies\n\n\nThe plot() function in R works using x and y coordinates.\n\nWe have to tell R precisely at which x- and y- coordinates to place points (e.g., place a point at x=20 and y=40)\nIn practice, we will generally supply R with a vector of x-coordindates and a vector of corresponding y-coordinates.\n\nTo illustrate a scatterplot, we will examine the relationship between the Wave 1 and Wave 2 feeling thermometer scores in the field experiment, for just the control “No Contact” condition.\n\n## Subset data to look at control only\ncontrolonly &lt;- subset(marriage1, treatment == \"No Contact\")\n\nIn the plot(), we supply the x and y vectors.\n\nxlim and ylim specify the range of the x and y axis.\npch is the point type. You can play around with that number to view different plot types\n\n\nplot(x=controlonly$therm1, y=controlonly$therm2, \n     main = \"Relationship between W1 and  W2\",\n     xlab = \"Wave 1\", xlim = c(0, 100),\n     ylab = \"Wave 2\", ylim = c(0, 100),\n     pch = 20)\n\n\n\n\nThe correlation looks extremely high! It is positively sloped and tightly clustered.\nIn fact, if we use R’s function to quantify a correlation between two variables, we will see it is a correlation above .99, very close to the maximum value.\n\nBy default, R calculate the “pearson” correlation coefficient, a number that will be between -1 and 1. It represents the strength of the linear association between two variables.\n\n\n## use = \"pairwise\" means to use all observations where neither variable has missing NA data\ncor(marriage1$therm1, marriage1$therm2, use = \"pairwise\")\n\n[1] 0.995313\n\n\nThis high correlation was unusual for this type of data.\n\nFeeling thermometers suffer from low reliability. How a person answers the question at one point in time (perhaps 83) in Wave 1 often differs from the numbers they say when asked again at a future point in time in Wave 2. A person’s responses often aren’t that stable.\nBecause there was such a high correlation, it suggested that the data might not have been generated by real human responses"
  },
  {
    "objectID": "04-Visualization.html#histograms",
    "href": "04-Visualization.html#histograms",
    "title": "4  Visualization",
    "section": "4.6 Histograms",
    "text": "4.6 Histograms\nThe researchers later discovered the Wave 1 data was suspiciously correlated with an existing survey: 2012 CCAP.\n\nThey believe the researcher likely used CCAP for Wave 1 - used survey responses from real humans that took a real survey – but not the humans that the researcher claimed to interview in the experiment.\nThen the researcher generated the Wave 2 data by adding random noise to the Wave 1 data\nPart of why they believe this has to do with a histogram plot they generated to compare Waves 1 and 2\n\nA histogram is a useful plot for summarizing the distribution of a single variable.\n\nIt shows the frequency of observations (e.g., the number of survey respondents) who give an answer within a particular interval of numeric values\n\nBecause a histogram is a single variable summary, we just supply R with the numeric variable we want to summarize.\n\nThe new argument here breaks tells R how many of the individual rectangles we want. You can play around with that number to see how the plot changes.\n\n\nhist(x=controlonly$therm1, breaks=50,\n     main = \"W1 Histogram\", ylim = c(0,1000))\n\n\n\nhist(x=controlonly$therm2,breaks=50,\n     main = \"W2 Histogram\", ylim = c(0,1000))  \n\n\n\n\nThe researchers noticed that the heaping patterns were different between Wave 1 and Wave 2.\n\nWhen real humans answer these types of feeling thermometer questions, we often see heaping (tall spikes) at values of 0, 50, and 100. Humans tend to gravitate toward those nice round numbers to anchor their responses. In addition, often researchers might recode people with missing responses (people who skip a question), as having a score of 50, increasing the number at that point.\n\nWave 1 has a lot of this heaping– look at the higher bars around 0, 50, and 100, suggesting a lot of survey respondents gave those answers.\nHowever, Wave 2 has less heaping, particularly at 50. This suggested to the researchers that the Wave 2 data were likely generated by a computer and not real humans\n\n\n\n4.6.1 Happy research ending\nWhile the original article was retracted\n\nResearchers who found irregularities received funding to conduct similar studies with real data this time\nMultiple publications suggest the canvassing approach was effective:\n\nBroockman and Kalla. 2016. “Durably reducing transphobia: A field experiment on door-to-door canvassing” Science 352 no. 6282.\nBroockman and Kalla. 2020. “Reducing exclusionary attitudes through interpersonal conversation: evidence from three field experiments.” American Political Science Review\nKalla and Broockman. 2021. “Which narrative strategies durably reduce prejudice? Evidence from field and survey experiments supporting the efficacy of perspective-getting.” American Journal of Political Science. Forthcoming."
  },
  {
    "objectID": "04-Visualization.html#line-plots",
    "href": "04-Visualization.html#line-plots",
    "title": "4  Visualization",
    "section": "4.7 Line Plots",
    "text": "4.7 Line Plots\nIn this application, we will create a line plot in R. Line plots are built very similarly to scatterplots. We provide R an input of values for the x-axis and corresponding values on the y-axis.\nFor example, if we wanted to plot the US life expectancy for the past few years from 2018-2021, we could do the following based on data from the National Center for Health Statistics:\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nyvalues &lt;- c(78.7, 78.8, 77, 76.1)\n\n## Key to making this a line plot is type=\"l\"\nplot(x=years,\n     y=yvalues,\n     type=\"l\",\n     main=\"US Life Expectancy by year\",\n     xlab = \"Year\",\n     ylab = \"Life Expectancy at birth\",\n     las=2 ) # orientation of axis labels\n\n\n\n\n\n## Alternate approach is to customize axis\nplot(x=1:4,\n     y=yvalues,\n     type=\"l\",\n     main=\"US Life Expectancy by year\",\n     xlab = \"Year\",\n     ylab = \"Life Expectancy at birth\",\n     las=2, # orientation of axis labels\n     xaxt=\"n\") # remove original axis\naxis(1, at=1:4, labels = years) # label points only at relevant ticks\n\n\n\n\nThe two vectors of values may come from variables in your data or may come from calculations you make using those variables.\nLet’s do a more complex example and break this down step by step."
  },
  {
    "objectID": "04-Visualization.html#application-trends-during-covid",
    "href": "04-Visualization.html#application-trends-during-covid",
    "title": "4  Visualization",
    "section": "4.8 Application: Trends during COVID",
    "text": "4.8 Application: Trends during COVID\nSince the onset of the pandemic in 2020, researchers have evaluated attitudinal and behavioral responses to policy changes, political messages, and COVID case/hospitalization/death rates.\n\nSurvey data on attitudes and self-reported behavior\nHealth care provider administrative data\nMobile phone data to track locations\nSocial media data to track attitudes and mobility\n\nExample: Using Survey data from over 1.1 million responses to measure concern about the coronavirus over time.\n\nClinton, Joshua, et al. “Partisan pandemic: How partisanship and public health concerns affect individuals’ social mobility during COVID-19.” Science advances 7.2 (2021): eabd7204.\n\n\nExample: Using the geotracking data of 15 million smartphones per day to compute percentage reduction in general movement and visiting non-essential services relative to before COVID-19 (before March 9).\n\nGollwitzer, Anton, et al. “Partisan differences in physical distancing are linked to health outcomes during the COVID-19 pandemic.” Nature human behaviour 4.11 (2020): 1186-1197.\n\n\nExample: Using Twitter geolocation data to track how much movement users have by looking at the distances from all locations where a given user has tweeted.\n\nPaiheng Xu, Mark Dredze, David A Broniatowski. “The Twitter Social Mobility Index: Measuring Social Distancing Practices from Geolocated Tweets.” Journal of Medical Internet Research (JMIR), 2020.\n\n\nWe will use the Twitter social mobility index to study how the movement of geo-located Twitter users changed from 2019 into April 2022.\n\nWe will compare this movement for users located in the Northeast vs. South\n\nEach row of the dataset represents a week of the year. Each column represents a particular geography for which social mobility was calculated by the researchers.\n\nDates indicates the date\nNortheast: social mobility data for those in the northeast of the U.S.\nSouth: social mobility data for those in the south of the U.S.\n\n\n## Load the data from the author Mark Dredze's website\ncovid &lt;- read.csv(\"https://raw.githubusercontent.com/mdredze/covid19_social_mobility.github.io/master/data/longitudinal_compiled.csv\")\n\nJust like we have encountered numeric, factor, and character variables, R also has the ability to treat variables specifically as dates. We will want R to treat the date variable we read in as a date, and not as raw text or some other variable type. To do this, we will use the as.Date function.\n\n## Date variable original format and class\nhead(covid$Dates)\n\n[1] \"2019-01-01\" \"2019-01-07\" \"2019-01-14\" \"2019-01-21\" \"2019-01-28\"\n[6] \"2019-02-04\"\n\nclass(covid$Dates)\n\n[1] \"character\"\n\n## Convert to class Date\ncovid$Dates &lt;- as.Date(covid$Date)\nhead(covid$Dates)\n\n[1] \"2019-01-01\" \"2019-01-07\" \"2019-01-14\" \"2019-01-21\" \"2019-01-28\"\n[6] \"2019-02-04\"\n\nclass(covid$Dates)\n\n[1] \"Date\"\n\n\nThe researchers continue to add to these data. Let’s look at the portion of data from 2019 to April 2022.\n\nNote the use of as.Date again to make sure R knows our text should be treated as a date\nNote the use of the greater than or equal to &gt;= and less than or equal signs &lt;= to specify which rows we want to keep in the data. We want rows that are in dates after January 1, 2019 and (&) on or before April 25, 2022.\n\n\ncovidsub &lt;- subset(covid, Dates &gt;= as.Date(\"2019-01-01\") &\n                     Dates &lt;= as.Date(\"2022-04-25\"))\n\nThese data are collected by week. That is very detailed. While that may be useful, let us create another variable that contains just the month and year, which will allow us to calculate the average per month. With a date variable, we can use the format function to change the format to just year and month.\n\ncovidsub$monthyear &lt;- format(covidsub$Dates, \"%Y-%m\")\nrange(covidsub$monthyear)\n\n[1] \"2019-01\" \"2022-04\"\n\n\nWhere we are going …\n\n\n\n\n\nStarting from the bottom …\n\nLet’s first create a scatterplot by providing R with our two variables\nIn a trend/line plot, we want each month on the x-axis\nWe want our outcome on the y-axis, in this case, average social mobility by month\nUltimately we will want to compare the Northeast with the South. We will plot one line at a time, starting with the Northeast\n\nWe first need to find the average by month. Recall our tapply() function.\n\nmobilitybymonthNE &lt;- tapply(covidsub$Northeast, covidsub$monthyear, mean,\n                          na.rm=T)\n\nmobilitybymonthSO &lt;- tapply(covidsub$South, covidsub$monthyear, mean,\n                          na.rm=T)\n\nLet’s look at the output for the Northeast. Each value is what we ultimately want on the y-axis– the average social mobility in a given month.\n\nmobilitybymonthNE\n\n 2019-01  2019-02  2019-03  2019-04  2019-05  2019-06  2019-07  2019-08 \n51.22066 52.26420 63.20130 61.38417 61.27622 60.49753 58.91779 61.20730 \n 2019-09  2019-10  2019-11  2019-12  2020-01  2020-02  2020-03  2020-04 \n54.44546 54.93814 56.59830 55.44538 51.12414 45.80660 34.55917 18.15076 \n 2020-05  2020-06  2020-07  2020-08  2020-09  2020-10  2020-11  2020-12 \n23.29190 28.71901 27.02149 32.73828 29.07536 32.07877 29.83641 30.56208 \n 2021-01  2021-02  2021-03  2021-04  2021-05  2021-06  2021-07  2021-08 \n28.75507 28.76227 35.35340 45.02537 48.19897 50.18401 52.96105 51.19241 \n 2021-09  2021-10  2021-11  2021-12  2022-01  2022-02  2022-03  2022-04 \n45.81695 49.15654 48.69051 51.24941 46.96813 53.55241 59.70933 54.04312 \n\n\nWe want to plot them each at their own point on the x-axis, from the first month to the last month. We can start by creating a vector of the same length as we have months:\n\n1:length(mobilitybymonthNE)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n\n\nThese become our two inputs in the plot.\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE, pch=16) # pch is point type\n\n\n\n\nWe now transform it to a line by specifying type=\"l\"\n\nBy default, R creates a plot with type=p for points. R also has type=b which has both a line and points.\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE, type=\"l\") # makes it a line\n\n\n\n\nLet us change the aesthetics a bit by adding labels and removing the border with bty=\"n\".\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80), # y-axis limits\n     las=1, # orientation of axis labels\n     lwd=2, # line width\n     bty=\"n\") # removes border\n\n\n\n\nLet’s add a comparison line with the lines() function to look at trends for the south.\n\nNote that this is outside of the plot() function, but the inputs are very similar. We supply a set of x and y coordindates.\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80), # y-axis limits\n     las=1, # orientation of axis labels\n     lwd=2, # line width\n     bty=\"n\") # removes border\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n\n\n\n\nLet’s create our own axis for the plot to add detail. To do this, we add xaxt to the plot function and then use axis() below the function.\nThe labels we will add are the actual months in the data. These happen to be the labels or names of our vectors:\n\nnames(mobilitybymonthNE)\n\n [1] \"2019-01\" \"2019-02\" \"2019-03\" \"2019-04\" \"2019-05\" \"2019-06\" \"2019-07\"\n [8] \"2019-08\" \"2019-09\" \"2019-10\" \"2019-11\" \"2019-12\" \"2020-01\" \"2020-02\"\n[15] \"2020-03\" \"2020-04\" \"2020-05\" \"2020-06\" \"2020-07\" \"2020-08\" \"2020-09\"\n[22] \"2020-10\" \"2020-11\" \"2020-12\" \"2021-01\" \"2021-02\" \"2021-03\" \"2021-04\"\n[29] \"2021-05\" \"2021-06\" \"2021-07\" \"2021-08\" \"2021-09\" \"2021-10\" \"2021-11\"\n[36] \"2021-12\" \"2022-01\" \"2022-02\" \"2022-03\" \"2022-04\"\n\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80),\n     las=1, \n     lwd=2, \n     bty=\"n\",\n     xaxt=\"n\") # removes original x-axis\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n## add the axis the \"1\" means x-axis. A \"2\" would create a y-axis\naxis(1, at = 1:length(mobilitybymonthNE), \n     labels=names(mobilitybymonthNE), las=2)\n\n\n\n\nFinally, let’s add a legend(). Now we’re here!\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80),\n     las=1, \n     lwd=2, \n     bty=\"n\",\n     xaxt=\"n\") # removes original x-axis\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n## add the axis the \"1\" means x-axis. A \"2\" would create a y-axis\naxis(1, at = 1:length(mobilitybymonthNE), \n     labels=names(mobilitybymonthNE), las=2)\n\n## Add legend, \"bottomleft\" indicates where on the plot to locate it\n## Could use \"topright\" instead, for example\nlegend(\"bottomleft\",  col=c(\"red3\", \"black\"), \n       c(\"South\", \"Northeast\"), \n       cex = .7, # size of legend\n       lwd=2,\n       bty=\"n\")"
  },
  {
    "objectID": "04-Visualization.html#visual-tips-and-tricks",
    "href": "04-Visualization.html#visual-tips-and-tricks",
    "title": "4  Visualization",
    "section": "4.9 Visual tips and tricks",
    "text": "4.9 Visual tips and tricks\nRecall we said the goals of visualization are to communicate information\n\nTransparently (show me the data!)\nQuickly\nSimply\nAccurately\nAnd with a little work: beautifully\n\nWhat NOT to communicate?\n\nClaus Wilke provides an overview of rules of thumb to fall when creating a data visualization on the Serial Mentor website.\nAn example is below\n\nOverall, the best thing to do is to look at your visual from a consumer’s point of view. You want your visuals to be intuitive enough for a viewer to be able to interpret it without too much help from you or explanatory text elsewhere in a paper or presentation. Our goal is to help consumers of our data understand the main takeaways of our research easily and accurately.\n\nWe want to make sure our visuals always have informative labels that a lay person can understand (instead of technical variable names, we can use plain language)\n\nWe may need to add a legend or additional text to a visual to help with this\n\nWe want to choose colors to convey information. We want to avoid colors that are hard to see or might distract consumers.\nThe axis dimensions should not be misleading. If the goal is to compare two or more plots to each other, we would want them to have similar axes, for example."
  },
  {
    "objectID": "04-Visualization.html#common-r-plotting-functions-and-arguments",
    "href": "04-Visualization.html#common-r-plotting-functions-and-arguments",
    "title": "4  Visualization",
    "section": "4.10 Common R plotting functions and arguments",
    "text": "4.10 Common R plotting functions and arguments\nHere is a refresher of several of the functions and arguments we have come across.\nCreate a plot\n\nplot(): for scatterplots and trend plots\nbarplot(): for barplot comparisons across categories\nboxplot(): boxplot for summaries of numeric variables\nhist(): for histogram summaries of a single numeric variable\n\nAesthetic arguments within a plot\n\nmain =: Specifies the main title of the plot. Supply text (e.g., main = \"my title\")\nylab =: Specifies the title of the y-axis. Supply text (e.g., ylab = \"Mean of variable\")\nxlab =: Specifies the title of the x-axis. Supply text (e.g., xlab = \"X variable name\")\nylim =: Specifies the range of the y-axis. Supply vector of two numbers (e.g., ylim = c(0, 100))\nxlim =: Specifies the range of the x-axis. Supply vector of two numbers (e.g., xlim = c(0, 100))\nbty=\"n\": Removes the border box around the plot\ncex, cex.main, cex.names, cex.lab, cex.axis: Changes the size of different elements of a plot. Default is 1, so a value of .8 would be smaller than default, and 1.2 would be bigger than normal.\ntype =: Specifies the type of plot (e.g., type=\"l\" is a line plot, type=\"b\" is a plot with points and lines connecting them)\nlwd=: Specifies the width of a line on a plot. Default is 1. E.g., lwd=3 makes a line much thicker\npch=: Specifies the point type. E.g., pch=15\nlty=: Specifies the line type. E.g., lty=2 is a dashed line\ncol=: Specifies the color of the central element of the plot. Can take a single color or vector of colors. Use colors() in the console to see all R colors.\nnames: Specifies a set of labels in a barplot\n\nWays to annotate a plot (generally added below the initial plotting function)\n\nabline(): Adds a line to the plot at a particular point on the x- or y- intercept, either horizontal, vertical, or of a particular slope\n\nExample: Adding a horizontal line at a particular at a y value of 2 abline(h=2)\nExample: Adding a vertical line at a particular at a x value of 2 abline(v=2)\n\nlines(x=, y=): Adds a line connecting pairs of x- and y-coordinates. We used this to add the South line to the social mobility plot.\naxis(): Used to replace the default x- or y- axis that R will create with a customized axis\n\nTo create an original y-axis, use axis(2, vectorofvalues, labels) and specify yaxt=\"n\" inside the plotting function to remove the original y-axis.\nTo create an original x-axis, use axis(1, vectorofvalues, labels) and specify xaxt=\"n\" inside the plotting function to remove the original x-axis.\n\nlegend(): Adds a legend to a plot. Can specify the location as the first argument (e.g., \"bottomleft\" or \"topright\")\ntext(): Adds text to a plot at specific x- and y- locations. (E.g., text(x=3, y=4, \"Here is a point\"). The x and y arguments can be single numbers or a vector of numbers. x and y need to be the same length.\npoints(): Adds points to a plot at specific x- and y- locations. Inputs are much like plot"
  },
  {
    "objectID": "04-Visualization.html#a-note-on-ggplot",
    "href": "04-Visualization.html#a-note-on-ggplot",
    "title": "4  Visualization",
    "section": "4.11 A note on ggplot",
    "text": "4.11 A note on ggplot\nR has a number of open-source packages that people can use to expand the set of capabilities for visualization and analysis. These can be installed through RStudio. We will look at one of these packages: ggplot2.\nUsing ggplot will be extra-credit at this point in the course. We may return to it later in the semester as part of the main curriculum. Reviewing this section of the notes is optional.\nThe “gg” in ggplot2 stands for the “Grammar of Graphics.” This program provides another framework for creating figures in R. According to Hadley Wickham, “ggplot2 provides beautiful, hassle-free plots that take care of fiddly details like drawing legends.”\nPractically speaking, ggplot() is another tool to plot the same types of figures we have been making in class. Some people prefer ggplot2 because they find the logic of building figures more intuitive using this framework and/or more aesthetically pleasing. However, both ggplot() and the plots we have been making in class can accomplish the same ultimate goals of data visualization– to communicate information transparently, quickly, accurately, simply, and beautifully. Which types of plots you may prefer is up to your own taste.\nThink of packages like apps on a smartphone.\n\nIf RStudio is our smartphone, we install a package like you install an app on the phone. You only have to do this once, though occasionally you may want or need to update the installation to a new version.\n\n\n## Run this line in your R console\ninstall.packages(\"ggplot2\")\n\n\nOn a smartphone, every time you want to use an app after you have installed it, you have to open the app. Similarly, every time we want to open a package in RStudio, we have to open it by using the library() command\n\n\n## Add and run this line in your R script, above the code where you will use functions from the package\nlibrary(ggplot2)\n\nThe main plotting function in ggplot2 is the ggplot() function. It will give you access to barplots, boxplots, scatterplots, histograms, etc.\n\nThe syntax within this package is a little different from the base R plotting functions. We will investigate below. For now, here is an example of using ggplot to create a boxplot using the experiment on social status from earlier in this section.\n\n\nggplot(data=status, mapping = aes(x=condition, y=econcon)) +\n  geom_boxplot()\n\n\n\n\nThe three primary components of a ggplot() are a dataframe (data =), a set of mapping aesthetics (aes()), and geoms (e.g., geom boxplot, geom bar, geom point, geom line, etc.).\n\nThe function ggplot() first takes a dataframe that includes the values you would like to plot (e.g., data = status).\nThe aesthetics then include the variable names that you want to plot on the x and y axis (e.g., aes(x=condition, y=econcon))\n\nAdditional mapping aesthetics can be specified. For example, a third variable (or a repeat of a previous variable) can also be specified (e.g., fill =, colour =, shape =), which acts as a grouping variable. If this is specified, ggplot() will create a corresponding legend for the plot and will color/make different shapes for different groups within this third variable (See the boxplot below for an example of grouping by condition).\n\nAfter closing out the first ggplot() parentheses, you then annotate the plot by adding (+) a geometric layer. This is essentially where you specify the type of plot (though it is possible to have multiple geometric layers).\nJust like with the other plotting functions in R, you can also specify a number of other arguments to make your plot more informative and aesthetically pleasing. Here, you do this by adding (+) additional arguments. See examples below (e.g., ggtitle, xlab, ylab for titles, ylim for y-axis limits, etc.)\nLikewise, just like with the other plotting functions, you can save your plots as a pdf or png. To do so here, you include the line ggsave() just below your plot.\n\nThere are many more possibilities for plotting with ggplot(), but these should get you started. For additional resources on all that is gg, I recommend the R Graphics Cookbook.\nHere is a second version of the boxplot with more aesthetics specified.\n\nWe will color in the boxes based on the collapsed condition variable.\n\n\nggplot(data=status, mapping = aes(x=condition, y=econcon, fill=conditionnew)) +\n  ## Specifies plot type. E.g., also have geom_point(), geom_bar()\n  geom_boxplot()+\n  ## Note many arguments are similar to other R functions but the syntax is a little different\n  ggtitle(\"Economic Views by Experimental Condition\")+\n  ylab(\"Economic Views\")+\n  xlab(\"Experimental Condition\")+\n  ylim(0,1)+\n  ## Changes the overall theme (i.e., color scheme, borders, etc.)\n  theme_bw()+\n  theme(legend.position=\"bottom\")\n\n\n\nggsave(\"myboxplot.pdf\", width=7, height=5)\n\nHere is an example of a histogram from the application on views toward gay couples.\n\nggplot(controlonly, aes(x=therm1)) +\n  geom_histogram(binwidth = 1) +\n  ggtitle(\"W1 Histogram\") +\n  theme_minimal()\n\n\n\n\nInstead of displaying multiple categories through different shapes or colors, we could also create multiple mini plots instead. This is done through facet. Let’s look at a histogram for each condition for the thermometers in wave 2.\n\nggplot(marriage1, aes(x=therm2)) +\n  geom_histogram(binwidth = 1) +\n  ggtitle(\"W2 Histogram by Condition\") +\n  xlab(\"Feeling Thermometer Wave 2\")+\n  theme_bw()+\n  facet_wrap(~treatmentnew)\n\nWarning: Removed 1042 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWe can similarly create a scatter and line plot. Let’s use the social mobility data. Here we see geom_point and geom_line.\n\n## Scatterplot\nggplot(covidsub, aes(x=Dates, y=avg_USA)) +\n  geom_point() +\n  ggtitle(\"Average Social Mobility in US\") +\n  xlab(\"Date\")+\n  ylab(\"Avg Social Mobility\")\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\n## Line plot\nggplot(covidsub, aes(x=Dates, y=avg_USA)) +\n  geom_line() +\n  ggtitle(\"Average Social Mobility in US\") +\n  xlab(\"Date\")+\n  ylab(\"Avg Social Mobility\")"
  },
  {
    "objectID": "05-Causalityii.html#why-cant-we-always-experiment",
    "href": "05-Causalityii.html#why-cant-we-always-experiment",
    "title": "5  Causality with Non-Experimental Data",
    "section": "5.1 Why can’t we always experiment?",
    "text": "5.1 Why can’t we always experiment?\nExample: Do political leaders tend to matter for democracy?\n\nOur outcome: how democratic nations are\nOur causal effect of interest:\n\nOn average, how democratic nations are with their current leaders -\nOn average, how democratic nations would be with different leaders\n\nPossible Experimental Designs to randomly assign half of countries to receive a different political leader\n\nRig elections (I.e., Election fraud- Illegal, unethical)\nForcibly remove half from office (Probably illegal)\nAssassinations (Illegal, Immoral, Unethical, etc.)\n\n\nAgain, we have problems!!\n\n5.1.1 What can we do instead?\nLet’s say we want to make a causal claim about the effect of one variable on an outcome, but we can’t think of an experimental design that will help us estimate this.\n\nWhat do you do?"
  },
  {
    "objectID": "05-Causalityii.html#causal-identification-strategies",
    "href": "05-Causalityii.html#causal-identification-strategies",
    "title": "5  Causality with Non-Experimental Data",
    "section": "5.2 Causal Identification Strategies",
    "text": "5.2 Causal Identification Strategies\nOur goal: Try to “identify” the causal effect of one variable on an outcome. As Montell Jordan once said, this is how we do it:\n\nUse data we have (that exist out in the world)\nCompare those who are “treated” to a relevant comparison group who is not treated\n\nHowever, we can’t randomize treatment so….\n\nWe do our best to try to choose a good comparison (one very similar to the treatment group, but happens not to be treated)\n\nWe want to rule out all possible confounding variables and “alternative explanations” for the outcomes we observed.\n\n5.2.1 Example: Travis Kelce Jersey Sale and Instagram Gains\nWhy did Travis Kelce experience an increase in social media followers?\n\nLet’s use the plot() function to visualize this\n\nKelce saw an increase in followers on Fri, Sept 22 of 8786\nKelce saw an increase in followers on Sat, Sept 23 of 7242\nKelce saw an increase in followers on Sun, Sept 24 of 27249\n\nWhen visualizing a trend, we put time on the x-axis\n\n\ntime &lt;- c(\"Fri, Sept 22\", \"Sat, Sept 23\", \"Sun, Sept 24\")\n\n\nWe put the values of the outcome on the y-axis\n\n\nfollowers &lt;- c(8786, 7242, 27249)\n\n\n5.2.1.1 Line Plots\nTo make it a line plot, we add type = \"l\" or type = \"b\".\n\nNote: Because our time is text-based, we cannot add it to the plot directly. Instead, we use a placeholder 1:length(time).\nInstead, we add xaxt=\"n\" to remove the default x-axis and add axis() below the plot code to add our own custom axis. By adding the “1”, we are indicating it should be drawn on the x-axis\n\n\nplot(x=1:length(time), y=followers,\n     type=\"b\",\n     main = \"Travis Kelce Instagram Follower Gains over Time\",\n     ylab=\"Instagram Follower Gains\",\n     xlab=\"Date\",\n     xaxt=\"n\",\n     las=2, cex.axis=.8)\naxis(1, at=1:length(time), labels=time, cex.axis=.8)\n\n\n\n\nYOUR TURN: Make a causal claim about the increase in Kelce’s followers\n\nWhat is the outcome? the number of followers\nWhat is the treatment? what do you think caused the increase\nWhat are the two counterfactual states of the world under treatment vs. not under treatment?\n\nIs this a Taylor Swift effect?\n\nHow could we prove it? What are possible confounders?\n\nMaybe it’s just the effect of playing a game on Sunday?\nMaybe all NFL players experienced a similar increase?\nMaybe Kelce had a particularly good game relative to other players?\n\n\n\n\n\n\n5.2.2 Example: Social Mobility Data\nSince the onset of the pandemic in 2020, researchers have evaluated attitudinal and behavioral responses to policy changes, political messages, and COVID case/hospitalization/death rates.\n\nSurvey data on attitudes and self-reported behavior\nHealth care provider administrative data\nMobile phone data to track locations\nSocial media data to track attitudes and mobility\n\nExample: Using Survey data from over 1.1 million responses to measure concern about the coronavirus over time.\n\nClinton, Joshua, et al. “Partisan pandemic: How partisanship and public health concerns affect individuals’ social mobility during COVID-19.” Science advances 7.2 (2021): eabd7204.\n\n\nExample: Using the geotracking data of 15 million smartphones per day to compute percentage reduction in general movement and visiting non-essential services relative to before COVID-19 (before March 9).\n\nGollwitzer, Anton, et al. “Partisan differences in physical distancing are linked to health outcomes during the COVID-19 pandemic.” Nature human behaviour 4.11 (2020): 1186-1197.\n\n\nExample: Using Twitter geolocation data to track how much movement users have by looking at the distances from all locations where a given user has tweeted.\n\nPaiheng Xu, Mark Dredze, David A Broniatowski. “The Twitter Social Mobility Index: Measuring Social Distancing Practices from Geolocated Tweets.” Journal of Medical Internet Research (JMIR), 2020.\n\n\nWe will use the Twitter social mobility index to study how the movement of geo-located Twitter users changed from 2019 into April 2022.\n\nWe will compare this movement for users located in the Northeast vs. South\n\nEach row of the dataset represents a week of the year. Each column represents a particular geography for which social mobility was calculated by the researchers.\n\nDates indicates the date\nNortheast: social mobility data for those in the northeast of the U.S.\nSouth: social mobility data for those in the south of the U.S.\n\n\n## Load the data from the author Mark Dredze's website\ncovid &lt;- read.csv(\"https://raw.githubusercontent.com/mdredze/covid19_social_mobility.github.io/master/data/longitudinal_compiled.csv\")\n\nJust like we have encountered numeric, factor, and character variables, R also has the ability to treat variables specifically as dates. We will want R to treat the date variable we read in as a date, and not as raw text or some other variable type. To do this, we will use the as.Date function.\n\n## Date variable original format and class\nhead(covid$Dates)\n\n[1] \"2019-01-01\" \"2019-01-07\" \"2019-01-14\" \"2019-01-21\" \"2019-01-28\"\n[6] \"2019-02-04\"\n\nclass(covid$Dates)\n\n[1] \"character\"\n\n## Convert to class Date\ncovid$Dates &lt;- as.Date(covid$Date)\nhead(covid$Dates)\n\n[1] \"2019-01-01\" \"2019-01-07\" \"2019-01-14\" \"2019-01-21\" \"2019-01-28\"\n[6] \"2019-02-04\"\n\nclass(covid$Dates)\n\n[1] \"Date\"\n\n\nThe researchers continue to add to these data. Let’s look at the portion of data from 2019 to April 2022.\n\nNote the use of as.Date again to make sure R knows our text should be treated as a date\nNote the use of the greater than or equal to &gt;= and less than or equal signs &lt;= to specify which rows we want to keep in the data. We want rows that are in dates after January 1, 2019 and (&) on or before April 25, 2022.\n\n\ncovidsub &lt;- subset(covid, Dates &gt;= as.Date(\"2019-01-01\") &\n                     Dates &lt;= as.Date(\"2022-04-25\"))\n\nThese data are collected by week. That is very detailed. While that may be useful, let us create another variable that contains just the month and year, which will allow us to calculate the average per month. With a date variable, we can use the format function to change the format to just year and month.\n\ncovidsub$monthyear &lt;- format(covidsub$Dates, \"%Y-%m\")\nrange(covidsub$monthyear)\n\n[1] \"2019-01\" \"2022-04\"\n\n\nWhere we are going …\n\n\n\n\n\nStarting from the bottom …\n\nLet’s first create a scatterplot by providing R with our two variables\nIn a trend/line plot, we want each month on the x-axis\nWe want our outcome on the y-axis, in this case, average social mobility by month\nUltimately we will want to compare the Northeast with the South. We will plot one line at a time, starting with the Northeast\n\nWe first need to find the average by month. Recall our tapply() function.\n\nmobilitybymonthNE &lt;- tapply(covidsub$Northeast, covidsub$monthyear, mean,\n                          na.rm=T)\n\nmobilitybymonthSO &lt;- tapply(covidsub$South, covidsub$monthyear, mean,\n                          na.rm=T)\n\nLet’s look at the output for the Northeast. Each value is what we ultimately want on the y-axis– the average social mobility in a given month.\n\nmobilitybymonthNE\n\n 2019-01  2019-02  2019-03  2019-04  2019-05  2019-06  2019-07  2019-08 \n51.22066 52.26420 63.20130 61.38417 61.27622 60.49753 58.91779 61.20730 \n 2019-09  2019-10  2019-11  2019-12  2020-01  2020-02  2020-03  2020-04 \n54.44546 54.93814 56.59830 55.44538 51.12414 45.80660 34.55917 18.15076 \n 2020-05  2020-06  2020-07  2020-08  2020-09  2020-10  2020-11  2020-12 \n23.29190 28.71901 27.02149 32.73828 29.07536 32.07877 29.83641 30.56208 \n 2021-01  2021-02  2021-03  2021-04  2021-05  2021-06  2021-07  2021-08 \n28.75507 28.76227 35.35340 45.02537 48.19897 50.18401 52.96105 51.19241 \n 2021-09  2021-10  2021-11  2021-12  2022-01  2022-02  2022-03  2022-04 \n45.81695 49.15654 48.69051 51.24941 46.96813 53.55241 59.70933 54.04312 \n\n\nWe want to plot them each at their own point on the x-axis, from the first month to the last month. We can start by creating a vector of the same length as we have months:\n\n1:length(mobilitybymonthNE)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n\n\nThese become our two inputs in the plot.\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE, pch=16) # pch is point type\n\n\n\n\nWe now transform it to a line by specifying type=\"l\"\n\nBy default, R creates a plot with type=p for points. R also has type=b which has both a line and points.\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE, type=\"l\") # makes it a line\n\n\n\n\nLet us change the aesthetics a bit by adding labels and removing the border with bty=\"n\".\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80), # y-axis limits\n     las=1, # orientation of axis labels\n     lwd=2, # line width\n     bty=\"n\") # removes border\n\n\n\n\nLet’s add a comparison line with the lines() function to look at trends for the south.\n\nNote that this is outside of the plot() function, but the inputs are very similar. We supply a set of x and y coordindates.\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80), # y-axis limits\n     las=1, # orientation of axis labels\n     lwd=2, # line width\n     bty=\"n\") # removes border\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n\n\n\n\nLet’s create our own axis for the plot to add detail. To do this, we add xaxt to the plot function and then use axis() below the function.\nThe labels we will add are the actual months in the data. These happen to be the labels or names of our vectors:\n\nnames(mobilitybymonthNE)\n\n [1] \"2019-01\" \"2019-02\" \"2019-03\" \"2019-04\" \"2019-05\" \"2019-06\" \"2019-07\"\n [8] \"2019-08\" \"2019-09\" \"2019-10\" \"2019-11\" \"2019-12\" \"2020-01\" \"2020-02\"\n[15] \"2020-03\" \"2020-04\" \"2020-05\" \"2020-06\" \"2020-07\" \"2020-08\" \"2020-09\"\n[22] \"2020-10\" \"2020-11\" \"2020-12\" \"2021-01\" \"2021-02\" \"2021-03\" \"2021-04\"\n[29] \"2021-05\" \"2021-06\" \"2021-07\" \"2021-08\" \"2021-09\" \"2021-10\" \"2021-11\"\n[36] \"2021-12\" \"2022-01\" \"2022-02\" \"2022-03\" \"2022-04\"\n\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80),\n     las=1, \n     lwd=2, \n     bty=\"n\",\n     xaxt=\"n\") # removes original x-axis\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n## add the axis the \"1\" means x-axis. A \"2\" would create a y-axis\naxis(1, at = 1:length(mobilitybymonthNE), \n     labels=names(mobilitybymonthNE), las=2)\n\n\n\n\nFinally, let’s add a legend(). Now we’re here!\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80),\n     las=1, \n     lwd=2, \n     bty=\"n\",\n     xaxt=\"n\") # removes original x-axis\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n## add the axis the \"1\" means x-axis. A \"2\" would create a y-axis\naxis(1, at = 1:length(mobilitybymonthNE), \n     labels=names(mobilitybymonthNE), las=2)\n\n## Add legend, \"bottomleft\" indicates where on the plot to locate it\n## Could use \"topright\" instead, for example\nlegend(\"bottomleft\",  col=c(\"red3\", \"black\"), \n       c(\"South\", \"Northeast\"), \n       cex = .7, # size of legend\n       lwd=2,\n       bty=\"n\")\n\n\n\n\n\n\n5.2.3 Causal claims from before vs. after comparisons\nWhat types of research questions could these trends generate?\n\n\n\n\n\nWhat would you want to know about how movement has changed over time. Think about examples of causal claims you might make:\n\nExample: X caused mobility to decline\nExample: Z caused mobility to decrease\nExample: W caused mobility to increase at different rates across different regions.\n\nSo what can we do to test causal claims?\n\nWhat is the fundamental problem of causal inference in this case?\nCan we do an experiment?\nResearchers try to form comparison groups, in a strategic way, with the data they have (i.e., “observational” or “non-experimental” data).\nBecause they cannot randomly assign two different experiences of the world, instead they choose two cases or two groups of cases that\n\nSeem extremely similar except\nOne has the treatment of interest, and one does not\n\n\nExample: Before vs. After Comparison\nLet’s examine social mobility just before vs. just after the federal announcement of social distancing guidelines to stop the spread of COVID-19.\n\nTo do so, we will draw a vertical line at March 2020\n\nNote we use abline(v=) to indicate a vertical line at a location to cross the x-axis\n\n\nThis is the 15th entry in our vector, which means at point 15 on the x-axis.\n\nmobilitybymonthNE[\"2020-03\"]\n\n 2020-03 \n34.55917 \n\nmobilitybymonthNE[15]\n\n 2020-03 \n34.55917 \n\n\n\nWe will also add text to inform views what that line represents\n\nNote we use text(x= , y=, labels) to indicate where to put text\n\n\n\nplot(x=1:length(mobilitybymonthNE),\n     y=mobilitybymonthNE,\n     type=\"l\", \n     main=\"Social Mobility by Month and Region\",\n     ylab=\"Twitter Social Mobility Index\",\n     xlab=\"\",\n     ylim = c(0, 80),\n     las=1, \n     lwd=2, \n     bty=\"n\",\n     xaxt=\"n\") # removes original x-axis\n## Add line to the plot \nlines(x=1:length(mobilitybymonthSO),\n     y=mobilitybymonthSO, col=\"red3\", lwd=2)\n\n## add the axis the \"1\" means x-axis. A \"2\" would create a y-axis\naxis(1, at = 1:length(mobilitybymonthNE), \n     labels=names(mobilitybymonthNE), las=2)\n## add dashed blue vertical line\nabline(v=15, lty=2, col=\"dodgerblue\", lwd=1.5)\n\n## add text near the line\n## the \\n breaks the text into different lines\ntext(x=15, y=65, labels = \"Federal \\n Announcement\", cex=.6)\n\n\n\n\nWe see mobility does appear to be lower after the announcement relative to before the announcement. Is this causal?\n\nAssumption: We would want to be able to argue that social mobility in the weeks following the announcement (after time period) would look similar to social mobility in the weeks prior to the announcement (before period) if not for the federal announcement\n\nThat the before vs. after time periods would be similar in any meaningful way if not for the presence of the treatment in the after period.\n\n\nDoes this seem like a plausible argument? Could other things (confounders) occurring around the time of the federal announcement also have caused the steep decline in social mobility?\n\nIf we think something else happened around the same time that might have caused mobility to go down anyway, then we may be doubtful that this is a causal effect."
  },
  {
    "objectID": "05-Causalityii.html#three-common-identification-strategies",
    "href": "05-Causalityii.html#three-common-identification-strategies",
    "title": "5  Causality with Non-Experimental Data",
    "section": "5.3 Three Common Identification Strategies",
    "text": "5.3 Three Common Identification Strategies\nExample: Does drinking Sprite make a person a better basketball player? (Inspired by 1990s commercial where a kid believes drinking Sprite will cause him to play basketball better.)\n\n\nCross-section comparison: Compare Grant Hill (who drinks Sprite) to others (who don’t)\nBefore-and-after: Compare Grant Hill after he started drinking Sprite to Grant Hill before\nDifference-in-differences: Compare Grant Hill before and after drinking Sprite and subtract from this the difference for some other person (who never drank Sprite) during the same two periods\n\n(Note: “drinking Sprite” is our treatment.)\n\n5.3.1 Threats to Cross-Section Designs\nAssumption: Must assume no confounders and any alternative explanations related to differences between the treated and control subjects that also relate to the outcome. The Threat: Your two groups may differ in ways beyond the “treatment” in ways that are relevant to the outcome you care about.\n\nCompare Grant Hill, a tall NBA player who currently drinks Sprite (treatment group) to\nYourself, assuming you and they do not drink Sprite (control group)\nCompare your basketball skill levels (the outcome).\nSuppose Grant Hill is better (a positive treatment effect).\n\nCan we conclude Sprite causes a person to be a better player?\n\n\nNope, because other things that affect basketball talent differ between you and Grant Hill, and these things, not Sprite, may explain the difference in basketball talent.\nMoreover, even if we compared just among NBA players (Grant Hill vs. non-Sprite drinking players of his era), it’s possible that Sprite targeted all-stars to recruit to drink Sprite. In this way, pre-existing basketball talent (a confounder) both explains why Grant Hill drank Sprite (relates to the treatment) and explains his higher level of basketball talent (relates to the outcome) in the time period after drinking Sprite.\n\nFor a cross-sectional comparison to be plausible, we need to choose a very similar comparison in order to isolate the treatment as the main variable that is causing a change in an outcome.\n\n\n\n5.3.2 Threats to Before-After Designs\nAssumption: Must assume no confounding time trend. Threat: Something else may be changing over time, aside from the treatment, that is affecting your outcome.\n\nCompare Grant Hill in the years after he started drinking Sprite (treated) to\nGrant Hill the years before he started drinking Sprite (control)\nCompare his basketball skill levels (outcome).\nSuppose Grant Hill after Sprite is better (a positive treatment effect).\nCan we conclude Sprite causes a person to be a better player?\n\nNot if something else Grant Hill started doing during that time period made him better (e.g., maybe during that time the NBA provided higher quality coaches and trainers, and everyone (including Grant Hill) got better).\n\nYou want your treatment to be the only thing relevant to basketball talent changing over time.\n\n\n\n5.3.3 Threats to Diff-in-Diff Designs\nAssumption: Must assume parallel trends: That in the absence of treatment, your treatment group would have changed in the same way as your control\n\nCompare Grant Hill in the years before vs. after he started drinking Sprite to Grant Hill’s teammate, who never drank sprite, in the same two time periods (before Hill drinks Sprite vs. after Hill drinks Sprite)\nCompare the change in each player’s basketball skill levels. Suppose Grant Hill’s skills increased to a greater degree than his teammate’s over the same time period.\nCan we conclude Sprite causes a person to be a better player?\n\nIf we are confident that Grant Hill did not have a unique (non-Sprite) advantage over that time period relative to other players, then our assumption might be plausible– that Grant Hill and other players would have experienced a similar growth in their skills if not for Grant Hill getting the extra benefit of Sprite.\nInstead, if, for example, Grant Hill got a new trainer during this period AND his teammate did not, then we might have expected Grant Hill to see more improvement even if he didn’t start drinking Sprite. A violation of the parallel trends assumption!\n\nCausality is hard!"
  },
  {
    "objectID": "05-Causalityii.html#application-economic-effects-of-basque-terrorism",
    "href": "05-Causalityii.html#application-economic-effects-of-basque-terrorism",
    "title": "5  Causality with Non-Experimental Data",
    "section": "5.4 Application: Economic Effects of Basque Terrorism",
    "text": "5.4 Application: Economic Effects of Basque Terrorism\nResearch Question: What is the economic impact of terrorism?\n\nFactual (\\(Y(1)\\)): Economy given Basque region hit with terrorism in early 1970s\n\nFrom 1973 to late 1990s, ETA killed almost 800 people\nActivity localized to Basque area\n\nCounterfactual (\\(Y(0)\\)): How would Basque economy have fared in the absence of the terrorism?\n\nBasque was the 3rd richest region in Spain at onset\nDropped to the 6th position by late 1990s\nWould this fall have happened in the absence of terrorism?\n\n\nProblem: We can’t observe the counterfactual. We can’t go back in time to manipulate the experience of terrorism.\n\n5.4.1 Applying 3 Identification Strategies\n\nCompare Basque to others after 1973 (Cross-section comparison)\nCompare Basque before and after 1973 (Before-and-after)\nCompare others before and after 1973 and subtract the difference from Basque’s difference (Difference-in-differences)\n\nFor a video explainer of the code for this application, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\n\nbasque &lt;- read.csv(\"basque.csv\")\n\n\nhead(basque)\n\n     region year   gdpcap\n1 Andalucia 1955 1.688732\n2 Andalucia 1956 1.758498\n3 Andalucia 1957 1.827621\n4 Andalucia 1958 1.852756\n5 Andalucia 1959 1.878035\n6 Andalucia 1960 2.010140\n\n\nVariables\n\nregion: 17 regions including Basque\nyear: 1955 – 1997\ngdpcap: real GDP per capita (in 1986 USD, thousands)\n\nSubset Basque Data into Four Groups\n\n## Basque before terrorism\nbasqueBefore &lt;- subset(basque, (year &lt; 1973) &\n                            (region == \"Basque Country\"))\n## Basque after terrorism\nbasqueAfter &lt;- subset(basque, (year &gt;= 1973) &\n                           (region == \"Basque Country\"))\n## others before terrorism\nothersBefore &lt;- subset(basque, (year &lt; 1973) &\n                            (region != \"Basque Country\"))\n## others after terrorism\nothersAfter &lt;- subset(basque, (year &gt;= 1973) &\n                           (region != \"Basque Country\"))\n\nWhat is the economic impact of terrorism?\nCross-section comparison\n\nmean(basqueAfter$gdpcap) - mean(othersAfter$gdpcap)\n\n[1] 1.132917\n\n\nBefore-and-after design\n\nmean(basqueAfter$gdpcap) - mean(basqueBefore$gdpcap)\n\n[1] 2.678146\n\n\nDifference-in-Differences design\n\ntreatDiff &lt;- mean(basqueAfter$gdpcap) -\n    mean(basqueBefore$gdpcap)\ncontrolDiff &lt;- mean(othersAfter$gdpcap) -\n    mean(othersBefore$gdpcap)\ntreatDiff - controlDiff\n\n[1] -0.48316\n\n\nHere is a way to visualize this difference-in-differences. Our estimated causal effect is the difference between the observed post-1973 economy in the Basque region mean(basqueAfter$gdpcap) and what we assume the economy would have been in the absence of terrorism (the treatment) using the dotted line– adding the control group’s trajectory to the pre-1973 Basque economy (mean(basqueBefore$gdpcap) + controlDiff).\n\n\n\n\n\nWhat should we conclude from each approach?\n\nEach approach resulted in a different estimate of the impact of terrorism on the economy. We should choose the approach for which we think the underlying assumptions are most plausible."
  },
  {
    "objectID": "05-Causalityii.html#placebo-tests",
    "href": "05-Causalityii.html#placebo-tests",
    "title": "5  Causality with Non-Experimental Data",
    "section": "5.5 Placebo Tests",
    "text": "5.5 Placebo Tests\nWhich Results Should We Believe? Role of Placebo Tests\nCross-section comparison\n\n## were there pre-existing differences between the groups?\nmean(basqueBefore$gdpcap) - mean(othersBefore$gdpcap)\n\n[1] 1.616077\n\n\nBefore-and-After design\n\n## was there a change in a group we don't think should have changed?\nmean(othersAfter$gdpcap) - mean(othersBefore$gdpcap)\n\n[1] 3.161306\n\n\nWhat about the Difference-in-Differences design?\n\n## here we go back in time even further to examine \"pre-treatment\" trends\n## we want them to be similar\n(basqueBefore$gdpcap[basqueBefore$year == 1972] -\n basqueBefore$gdpcap[basqueBefore$year == 1955]) -\n    (mean(othersBefore$gdpcap[othersBefore$year == 1972]) -\n     mean(othersBefore$gdpcap[othersBefore$year == 1955]))\n\n[1] 0.07147071\n\n\nThese “placebo” checks are closest to zero for diff-in-diff, so we may believe that the most.\nThanks to Will Lowe and QSS for providing the foundations for this example"
  },
  {
    "objectID": "05-Causalityii.html#wrapping-up-causality",
    "href": "05-Causalityii.html#wrapping-up-causality",
    "title": "5  Causality with Non-Experimental Data",
    "section": "5.6 Wrapping Up Causality",
    "text": "5.6 Wrapping Up Causality\nDo you get this joke?"
  },
  {
    "objectID": "06-Loops.html#the-anatomy-of-a-loop",
    "href": "06-Loops.html#the-anatomy-of-a-loop",
    "title": "6  Loops in R",
    "section": "6.1 The anatomy of a loop",
    "text": "6.1 The anatomy of a loop\nA short video introduction to the anatomy of a loop \nIn many situations, we want to repeat the same calculations with different inputs. Loops allow you to avoid writing many similar code chunks.\n\nThe function for(i in X){}} will create a loop in your programming code where i is a counter and\nX is a placeholder for a vector for the possible values of the counter.\n\nWe use the following syntax:\nfor (i in X) {\ncommand1...\ncommand2...\n...\n}\nto indicate we want to repeat command1 and command2 and …. as many commands as we want, for each i in the set of possible values for i stored in X.\n\n6.1.1 The key parts of a loop\nThe meat: the command or set of commands you want to do over and over.\n\n## the meat\nresult &lt;- 6 + 2\nresult &lt;- 8 + 2\nresult &lt;- 4 + 2\nresult &lt;- 7 + 2\nresult &lt;- 11 + 2\n\nNote the pattern: we take some number and + 2 each time.\n\nIt is the number that is changing -&gt; what we will iterate.\n\nFor a loop, you want to:\n\nThe Meat: Write down the code for one version.\n\n\nresult &lt;- 6 + 2\n\n\nThe Bread: Embed this code in the loop syntax (for(i in X){})\n\n\nfor(i in X){\n    result &lt;- 6 + 2\n    }\n\n\nCreate a vector that contains the values you want to loop through\n\n\nsomenumbers &lt;- c(6, 8, 4, 7, 11)\n\n\nCreate a storage vector that will contain the results\n\n\nresult &lt;- rep(NA, length(somenumbers))\n\n\nModify the meat and bread to iterate by using [i], and replace X.\n\n\nfor(i in 1:length(somenumbers)){\n  result[i] &lt;- somenumbers[i] + 2\n}\n\n  where `1:length(somenumbers)` reflects possible values `i` will take \n\n1:length(somenumbers)\n\n[1] 1 2 3 4 5\n\n\n\n\n6.1.2 A short example\nLet’s put these parts together:\nSuppose we want to add 2 to a set of numbers c(6, 8, 4, 7, 11)\n\nsomenumbers &lt;- c(6, 8, 4, 7, 11) # iteration vector\nresult &lt;- rep(NA, length(somenumbers)) # container vector\n\nfor(i in 1:length(somenumbers)){\n  result[i] &lt;- somenumbers[i] + 2\n}\nresult\n\n[1]  8 10  6  9 13\n\n\nHow does this work? Every iteration, the value of i changes.\n\nFor example, when i is 1, we take the first value in our somenumbers vector somenumbers[1], add 2 to it, and store it in the first position of our container vector result[1]. When i is 2, we switch the number in the brackets to 2, corresponding to the second entry in each vector, and so on.\n\n\n# Suppose i is 1\nresult[1] &lt;- somenumbers[1] + 2\nresult[1]\n\n[1] 8\n\n# Suppose i is 2\nresult[2] &lt;- somenumbers[2] + 2\nresult[2]\n\n[1] 10\n\n# Suppose i is 3\nresult[3] &lt;- somenumbers[3] + 2\nresult[3]\n\n[1] 6\n\n\n\n\n6.1.3 Troubleshooting a loop\nThe inside part of the loop should run if we set i to a particular value.\n\ni &lt;- 1\nresult[i] &lt;- somenumbers[i] + 2\n\nIf you get an error here, there is something wrong with the meat! (and not necessarily the loop)\n\nresult[i]\n\n[1] 8\n\n\nFor example, if we had a typo, we’d get an error. Try running the below!\n\ni &lt;- 1\nresult[i] &lt;- somenumberz[i] + 2\n\n\n\n6.1.4 Your turn\nUsing a loop, for each value in our poll results, add 10 and divide by 100. Store in a vector called adjustedpollresults.\n\npollresults &lt;- c(70, 40, 45, 60, 43, 80, 23)\n\nRemember the steps:\n\nThe Meat: Write down the code for one version.\nThe Bread: Embed this code in the loop syntax (for(i in X){})\nCreate a vector that contains the values you want to loop through (here it’s pollresults)\nCreate a storage vector that will contain the results (here it’s adjustedpollresults)\nModify the meat and bread to iterate by using [i] and replace X.\n\n\n\nTry on your own, then expand for the solution.\n\n\npollresults &lt;- c(70, 40, 45, 60, 43, 80, 23)\nadjustedpollresults &lt;- rep(NA, length(pollresults))\n\nfor(i in 1:length(pollresults)){\n  adjustedpollresults[i] &lt;- (pollresults[i] + 10)/100 \n}\nadjustedpollresults\n\n[1] 0.80 0.50 0.55 0.70 0.53 0.90 0.33"
  },
  {
    "objectID": "06-Loops.html#application-u.s.-supreme-court",
    "href": "06-Loops.html#application-u.s.-supreme-court",
    "title": "6  Loops in R",
    "section": "6.2 Application: U.S. Supreme Court",
    "text": "6.2 Application: U.S. Supreme Court\nA video explainer of the loop in this section using a similar dataset that goes through 2020. We now have data going through 2021! \nThe Court has changed a lot recently in its composition (and will continue to do so next term).\n\n \n\n\n\n\nIdeology on the U.S. Supreme Court: With Kennedy out, Kavanaugh in, did the Court have a Conservative shift? What about with the death of Ruth Bader Ginsburg and confirmation of Amy Coney Barrett?\nMany people predicted it would. See this FiveThirtyEight article as an example. The graph from the article shows Kavanaugh’s predicted ideology.\n\n\nWe will explore how the Court has changed ideologically, with a focus on how the location of the median U.S. Supreme Court Justice shifted over time.\nWhy does the median matter? A refresher on the Court\n\nPresident nominates the justice. Senate must confirm.\nJustices serve lifetime appointments.\nTrump nominated Gorsuch, following Scalia death, confirmed 2017.\nTrump nominated Kavanaugh, following Kennedy retirement, confirmed 2018.\nThe Court typically has 9 justices, so whichever justice is the median in terms of ideology, can act as the “swing” vote in cases where the Court is divided\n\nAnthony Kennedy was often the “swing” justice for a decade.\n\nWith Kennedy out, the prediction was that the Court would return to similar balance as when O’Connor was the median.\nIn 2020, Ruth Bader Ginsburg died and Amy Coney Barrett was confirmed to Court late that year, likely shifting the Court again.\nWe can continue to update the data to examine what happened. We don’t yet have data on changes to the Court that have come about since Justice Ketanji Brown Jackson was confirmed following the retirement of Justice Breyer.\n\nLet’s load and explore our data.\n\nterm: is the year of the SC (1991-2021 except for 2005),\njusticeName: contains the name of the Justice, and\n\npost_mn: includes the “ideal point”– this is the estimated ideology\n\nMartin-Quinn Scores assess ideology based on how judges “cluster” together in their voting patterns. Every Justice gets an ideology score, and this score can change each SC term (year) they are on the Court. Higher scores are more conservative justices, and lower, more liberal. More information is available at the MQScores website\n\njustices &lt;- read.csv(\"justices23.csv\", stringsAsFactors = T)\n\n## alternative\njustices &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/justices23.csv\", stringsAsFactors = T)\n\nWe are going to make the name variable a character class. This will make R treat the names as raw text rather than valued categories. This will be useful later on in the application.\n\n## justice Name as character\njustices$justiceName &lt;- as.character(justices$justiceName)\n\nWe can use tapply() to see the median “ideal point” (ideology score) each term in our data.\n\n## Note: we use tapply like before but replace mean with median\nmedians &lt;- tapply(justices$post_mn, justices$term, median)\n\nplot(x =names(medians),\n     y= medians, \n    ylim = c(-.4, 1),\n     type = \"b\",\n    cex=1.5,\n     ylab=\"Conservatism\",\n     xlab=\"Term\",\n     main=\"Median of US Supreme Court over Time\")\n\n\n\n\nWe see a conservative shift at the end of the plot. However, we cannot tell whether this represents a shift within a particular justice’s ideology or a shift in which justice has become the median, due perhaps, to the change in the Court’s composition.\nWe need to find which justice is the median!\nLoops to the rescue!\nWe will start our process by defining the meat of the operation.\n\nWe want to find the median SC Justice for each term. To get started, let’s pretend we only have to find the median Supreme Court Justice for one term.\n\n\nSCterms &lt;- sort(unique(justices$term))\nSCterms\n\n [1] 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2006\n[16] 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021\n\n\nNote that where you have a vector where some entries in the vector are repeated (such as terms on the Supreme Court), you can extract the unique elements of that vector using the unique() function. You can also sort() them in numeric or alphabetical order. This won’t be necessary most times.\nFirst, let’s think about how we would do this for just one of the Supreme Court terms. Well we would first subset our data frame to contain only that one Supreme Court term.\n\n## Example for the first term\nSCterms[1]\n\n[1] 1991\n\n## Subset data to include only rows from 1991\nsubterm &lt;- subset(justices, term == 1991)\n\nThen, we would take the median of these ideal points\n\nmedian.ip &lt;- median(subterm$post_mn)\n\nFinally, we would figure out which justice has this median.\n\nresult &lt;- subterm$justiceName[subterm$post_mn == median.ip]\nresult\n\n[1] \"SDOConnor\"\n\n\nNow let’s put it into our loop syntax\n\n# for(i  in . . . ){\n#  subterm &lt;- subset(justices, term == 1991)\n#  median.ip &lt;- median(subterm$post_mn)\n#  result &lt;- subterm$justiceName[subterm$post_mn == median.ip]\n#}\n\nNow, we need our container vector and iteration vectors.\n\nSCterms &lt;- sort(unique(justices$term))\nresults &lt;- rep(NA, length(SCterms))\nnames(results) &lt;- SCterms\n\nFinally, we would modify our loop syntax with i and [i]\n\nfor(i  in 1:length(SCterms)){\n  subterm &lt;- subset(justices, term == SCterms[i])\n  median.ip &lt;- median(subterm$post_mn)\n  results[i] &lt;- subterm$justiceName[subterm$post_mn == median.ip]\n}\n\nDid it work?\n\nresults\n\n         1991          1992          1993          1994          1995 \n  \"SDOConnor\"   \"SDOConnor\"   \"AMKennedy\"   \"SDOConnor\"   \"AMKennedy\" \n         1996          1997          1998          1999          2000 \n  \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\"   \"SDOConnor\"   \"SDOConnor\" \n         2001          2002          2003          2004          2006 \n  \"SDOConnor\"   \"SDOConnor\"   \"SDOConnor\"   \"SDOConnor\"   \"AMKennedy\" \n         2007          2008          2009          2010          2011 \n  \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\" \n         2012          2013          2014          2015          2016 \n  \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\"   \"AMKennedy\" \n         2017          2018          2019          2020          2021 \n  \"AMKennedy\"   \"JGRoberts\"   \"JGRoberts\" \"BMKavanaugh\" \"BMKavanaugh\" \n\n\nOur evidence aligns with others:\n\n\n6.2.1 Troubleshooting the loop\nRecall, the inside part of the loop should run if we set i to a particular value.\n\n  i &lt;- 1\n  subterm &lt;- subset(justices, term == SCterms[i])\n  median.ip &lt;- median(subterm$post_mn)\n  results[i] &lt;- subterm$justiceName[subterm$post_mn == median.ip]\n  \n  results[i]\n\n       1991 \n\"SDOConnor\" \n\n\nWe are in good shape! If we had a typo, we’d get an error message there, and that would be a sign that we need to work on the inside part of the code before putting it back into the loop structure.\n\n\n6.2.2 Visualizing the Results\nTo get a bit more practice with plots, let’s visualize the results and make our interpretations.\n\nmedians &lt;- tapply(justices$post_mn, justices$term, median)\nplot(x =names(medians),\n     y= medians, \n     ylim = c(-.4, 1),\n     type = \"b\",\n     cex=1.5,\n     ylab=\"Conservatism\",\n     xlab=\"Term\",\n     main=\"Median of US Supreme Court over Time\")\n\n## Add the names to the plot\n## Note: we want to make sure medians and results are in the same order for this to work\ntext(x=names(results), y=(medians - .14), labels=results, cex=.35)\n\n\n\n\nWe have now used the text() function. Similar to plot, the text() takes a set of x and y coordinates that tells R the location of where you want to add a piece(s) of text to the plot. The third input is the actual text.\nWhy did the Court shift more conservative at the end of the time trend?\n\nWell we see that Justice Roberts and then Justice Kavanaugh became the median!\n\n\n\n\nFiveThirtyEight\n\n\nAs FiveThirtyEight notes, just because Justice Roberts is the new median, does not mean he has become more liberal. The Court composition is shifting, and the MQ scores also depend on the issues being heard before the Court.\n\nRecall, the Martin-Quinn scores measure justice ideology based on voting patterns. What are the strengths and weaknesses of using this type of information to score the ideology of a justice?\n\n\n6.2.3 Enhancing the plot\nLet’s make the plot more beautiful by color coding.\n\nmedians &lt;- tapply(justices$post_mn, justices$term, median)\nplot(x =names(medians),\n     y= medians, \n     ylim = c(-.4, 1),\n     type = \"b\",\n     ylab=\"Conservatism\",\n     xlab=\"Term\",\n     main=\"Median of US Supreme Court over Time\",\n     xaxt=\"n\", ## removes the x-axis\n     las=1)\n\n## Adds text\ntext(x=names(results), y=(medians - .14), results, cex=.35)\n\n## Adds color-coded points on top of existing points\npoints(x =names(medians),\n     y= medians,\n     \n     ## Adds colors according to how results is coded\n     col= ifelse(results == \"AMKennedy\", \"orange\", \n                 ifelse(results ==\"SDOConnor\", \"light blue\", \n                        ifelse(results == \"JGRoberts\", \"purple\",\n                        \"red3\"))), \n     pch=15, # point type- squares \n     cex=1.5) # size of points\n\n## Adds custom x-axis at the specific years included in names(medians)\naxis(1, names(medians), cex.axis=.6)\n\n\n\n\nWe have used the points() function. This adds an additional layer of points to a plot. It works much like the plot function in that in takes a set of x and y coordinates.\nWe could change the look of the plot even more by adding a legend and altering the borders and look of the plot.\n\nmedians &lt;- tapply(justices$post_mn, justices$term, median)\nplot(x =names(medians),\n     y= medians, \n     ylim = c(-.4, 1),\n     type = \"b\",\n     ylab=\"Conservatism\",\n     xlab=\"Term\",\n     main=\"Median of US Supreme Court over Time\",\n     xaxt=\"n\", # removes x axis\n     las=1, # changes the orientation of the axis labels\n     lwd=2, # increases the thickness of the lines\n     tick=F, # removes the tick marks from the axis\n     bty=\"n\") # removes the plot border\n\n## adds horizontal dashed gray lines\nabline(h=seq(-.4, 1, .2), lty=2, col=\"light gray\")\n\n## Adds a legend\nlegend(\"bottomleft\", pch=15, col = c(\"orange\", \"light blue\", \"purple\", \"red3\"),\n       c(\"Kennedy\", \"O'Connor\", \"Roberts\", \"Kavanaugh\"), bty=\"n\")\n\n## Adds the color-coded points\npoints(x =names(medians), y= medians,\n          col= ifelse(results == \"AMKennedy\", \"orange\", \n                 ifelse(results ==\"SDOConnor\", \"light blue\", \n                        ifelse(results == \"JGRoberts\", \"purple\",\n                        \"red3\"))), \n     pch=15, cex=2)\n\n## Adds our custom x-axis\naxis(1, names(medians), cex.axis=.6, tick=F)\n\n\n\n\n\n\n6.2.4 Wrapping Up\nWe have calculated and visualized how the median U.S. Supreme Court Justice and Justice’s ideology has changed over the past three decades.\n\nThis gave us additional practice with loops and visualization\nWe also gained exposure to an example of how political scientists take a large amount of information– votes on all Supreme Court cases– and try to summarize it using a single number that represents how liberal or conservative a justice is\n\nThis type of information can be used for many social science goals: 1) To describe trends in the Court 2) To help explain why the Court has voted a particular way on recent cases 3) To predict how the Court will vote in the future as new justices arrive.\nWith Amy Coney Barrett now on the Court for multiple terms and Ketanji Brown Jackson beginning to decide cases, the MQ scores will continue to be updated to allow for future exploration of these dynamics.\n\n\n\nFiveThirtyEight"
  },
  {
    "objectID": "07-Prediction.html#prediction-overview",
    "href": "07-Prediction.html#prediction-overview",
    "title": "7  Prediction",
    "section": "7.1 Prediction Overview",
    "text": "7.1 Prediction Overview\nOur goal: Predict (estimate/guess) some unknown using information we have as accurately and precisely as possible\n\nPrediction could involve estimating a numeric outcome. Alternatively, prediction also involves classification– predicting a categorical outcome (e.g., prediction of who wins vs. who loses).\n\nSome political science examples of this might include\n\nCategorizing comments on social media as being toxic/nasty/uncivil\n\n\n\n\nWired\n\n\n\nDetecting Fake news and misinformation\n\n\n\n\nPBS\n\n\n\nForecasting election results\n\n\nOther examples\n\nTrying to detect hate speech online\nPredicting where or when an attack might occur\nTrying to classify a large amount of text into subject or topic categories for analysis\n\nWhat other types of things might we try to predict or classify in political science?"
  },
  {
    "objectID": "07-Prediction.html#process-of-prediction",
    "href": "07-Prediction.html#process-of-prediction",
    "title": "7  Prediction",
    "section": "7.2 Process of Prediction",
    "text": "7.2 Process of Prediction\nPredict (estimate/guess) some unknown using information we have – and do so as accurately and precisely as possible.\n\nChoose an approach\n\nUsing an observed (known) measure as a direct proxy to predict an outcome\nUsing one or more observed (known) measures in a regression model to predict an outcome\n(Beyond the course) Using a statistical model to select the measures to use for predicting an outcome\n\nAssess accuracy and precision\n\nPrediction error: \\(Prediction - Truth\\)\nBias: Average prediction error: \\(\\text{mean}(Prediction - Truth)\\)\n\nA prediction is `unbiased’ if the bias is zero (If the prediction is on average true)\n\nRoot-mean squared error: \\(\\sqrt{\\text{mean}((Prediction - Truth)^2)}\\)\n\nLike `absolute’ error– the average magnitude of the prediction error\nthe typical distance the prediction is from the truth\n\nConfusion Matrix\n\nA cross-tab of predictions you got correct vs. predictions you got wrong (misclassified)\nGives you true positives and true negatives vs. false positives and false negatives\n\n\nIterate to improve the prediction/classification\n\nOften, we repeat steps 1-3 until we are confident in your method for predicting.\n\nDanger Zone: Eventually, after you have tested the approach and are satisfied with the accuracy, you may start applying it to new data for which you do not know the right answer."
  },
  {
    "objectID": "07-Prediction.html#example-forecasting-2020-us-election-based-on-2016-results",
    "href": "07-Prediction.html#example-forecasting-2020-us-election-based-on-2016-results",
    "title": "7  Prediction",
    "section": "7.3 Example: Forecasting 2020 US Election based on 2016 Results",
    "text": "7.3 Example: Forecasting 2020 US Election based on 2016 Results\nLet’s try to predict the 2020 election results using just the 2016 results.\nFor a video explainer of the code for this application, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\n\nresults2020 &lt;- read.csv(\"elecresults2020.csv\", stringsAsFactors = T)\n\nVariables\n\nstate: state or state and district\ncalled: result of 2020 election\nmargin2016: two-party margin in 2016. Positive values indicate Democratic win, negative indicate Republican win\nEV: Electoral votes associated with a state/ district of a state\n\n\n\nsum(results2020$EV[results2020$called == \"R\"])\n\n[1] 232\n\nsum(results2020$EV[results2020$called == \"D\"])\n\n[1] 306\n\n\n\n7.3.1 Choose Approach\n\nChoose an approach: Using an observed (known) measure as a direct proxy to predict an outcome\n\n\nLet’s use the 2016 result as a direct proxy to predict 2020.\n\n\nresults2020$predicted2020 &lt;- ifelse(results2020$margin2016 &lt; 0, \"R\", \"D\")\nresults2020$predicted2020 &lt;- as.factor(results2020$predicted2020)\n\n\n\n7.3.2 Assess Accuracy\n\nAssess accuracy\n\nWhat proportion of states did we get correct?\n\nmean(results2020$predicted2020 == results2020$called)\n\n[1] 0.8928571\n\n\nClassification\nWe want to correctly predict the winner of each state\nPrediction of binary outcome variable = classification problem\n\ntrue positive: correctly predicting Biden to be the winner\nfalse positive: incorrectly predicting Biden to be the winner (misclassification)\ntrue negative: correctly predicting Biden to be the loser\nfalse negative: incorrectly predicting Biden to be the loser (misclassification)\n\nWe define one outcome as the “positive” and one as the “negative.” Here we will say a Biden win is the positive and a Trump win is the negative. You could flip this and make a Trump win the positive and a Biden win the negative. This terminology comes from settings where there is a more objective positive vs. negative result (e.g., a positive medical test result) than most social science settings. The key thing is that we are trying to identify different types of correct classifications vs. misclassifications.\nConfusion Matrix: Tells us how we went right, how we went wrong.\n\ntable(predicted=results2020$predicted2020, actual = results2020$called)\n\n         actual\npredicted  D  R\n        D 22  0\n        R  6 28\n\n\nWhich states did we get wrong?\n\nresults2020$state[results2020$predicted2020 != results2020$called]\n\n[1] Arizona               Georgia               Michigan             \n[4] Nebraska 2nd District Pennsylvania          Wisconsin            \n56 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming\n\n\n\n\n7.3.3 Iterate to improve predictions\nStart back at step one. We continue to repeat steps 1 and 2 until we are confident in our predictions.\nHow could we improve our predictions of elections? What other information could we use?"
  },
  {
    "objectID": "07-Prediction.html#example-using-polls-to-predict-the-2020-election-results",
    "href": "07-Prediction.html#example-using-polls-to-predict-the-2020-election-results",
    "title": "7  Prediction",
    "section": "7.4 Example: Using polls to predict the 2020 election results",
    "text": "7.4 Example: Using polls to predict the 2020 election results\nFor a video explainer of the code for this application, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nMany forecasters use pre-election polls in their models to predict election outcomes. In 2016 and 2020, polling-based forecasts received a lot of criticism\nPrior to the 2016 elections, forecasts that used polls seemed confident that Hillary Clinton would win the election. Political analysts also seemed to think the polls were favorable to Clinton.\n\n\n\nNY Upshot\n\n\nWe all know that afterwards, Clinton did not win.\n\n\n\nPew\n\n\nThis led public opinion scholars and practitioners to do a deep investigation into the quality of pre-election polling. Like 2016, following the 2020 election, a similar team investigated the quality of pre-election polling in 2020. Here, while many polls pointed to a favorable outcome for Biden, the results seemed closer than one might have anticipated.\n\nThe results of these findings are in the AAPOR report.\n\n7.4.1 Choose an approach: Let’s analyze some polls\nWe are going to do our own analysis of pre-election polls as a prediction of the 2020 election results. We will use a large number of state polls conducted from May-November 2020 that were made available to the public on FiveThirtyEight.\n\npolls2020 &lt;- read.csv(\"pollsandresults2020.csv\", stringsAsFactors = T)\n\nVariables\n\nTrumpPoll, BidenPoll: Poll-based vote share for Biden or Trump\nTrumpResult, BidenResult: Actual vote share for Biden or Trump\nEV: Electoral votes associated with state/CD\ndays_to_election: Days until Election Day\nstateid: state abbreviation\nfte_grade: FiveThirtyEight Pollster grade\nsample_size: Poll sample size\n\nCan we predict the outcome of an election using polls?\nLet’s create our outcome variables.\n\n## Biden's margin of victory (or defeat) in  the polls\npolls2020$polldiff &lt;- polls2020$BidenPoll - polls2020$TrumpPoll\n\n## Biden's margin of victory (or defeat) in the actual election result\npolls2020$resultdiff &lt;- polls2020$BidenResult - polls2020$TrumpResult\n\nPositive numbers mean Biden was ahead/won. Negative mean Trump was ahead/won.\n\nLet’s predict the amount of electoral votes for Biden based on polls in each state close to Election Day.\nLet’s start with 1 state.\n\nLet’s grab all polls within 2 weeks of the election or the most recent day polled (for areas that did not have recent polls)\n\n\n## Iteration vector\nstates &lt;- unique(polls2020$stateid)\nstates[1]\n\n[1] AL\n55 Levels: AK AL AR AZ CA CO CT DC DE FL GA HI IA ID IL IN KS KY LA MA ... WY\n\n## Subset to just Alabama\nsubdata &lt;- subset(polls2020, stateid == states[1])\n\n## Further subset to the \"latest polls\"\nsubdata &lt;- subset(subdata, days_to_election &lt; 15 | \n                      days_to_election == min(subdata$days_to_election) )\n\nNow let’s extract the actual margin for Biden, the poll-based predicted margin, and finally, let’s assign electoral votes based on our prediction.\n\n## Find the margin for the actual result\nresult.marginAL &lt;- mean(subdata$resultdiff)\nresult.marginAL\n\n[1] -25.4\n\n## Find the margin for our prediction\npolls.marginAL &lt;- mean(subdata$polldiff)\npolls.marginAL\n\n[1] -21.16667\n\n## Allocate votes for Biden according to the margin\nbidenvotesAL &lt;- ifelse(mean(subdata$polldiff) &gt; 0, \n                            unique(subdata$EV), 0)\nbidenvotesAL\n\n[1] 0\n\n\nWe predicted Biden would lose Alabama because the polls.marginAL is negative. Therefore, we assigned Biden 0 electoral votes in this example.\n\n\n7.4.2 Loop through all states\n\n## Iteration vector\nstates &lt;- unique(polls2020$stateid)\n## Container vector\npolls.margin &lt;- result.margin &lt;- bidenvotes &lt;- \n  rep(NA, length(states))\n\nnames(polls.margin) &lt;- names(result.margin) &lt;- \n  names(bidenvotes)  &lt;-as.character(unique(states))\n\n## Loop\nfor(i in 1:length(states)){\n  subdata &lt;- subset(polls2020, stateid == states[i] )\n  subdata &lt;- subset(subdata, days_to_election &lt; 15 | \n                      days_to_election == min(subdata$days_to_election) )\n  result.margin[i] &lt;- mean(subdata$resultdiff)\n  polls.margin[i] &lt;- mean(subdata$polldiff)\n  bidenvotes[i] &lt;- ifelse(mean(subdata$polldiff) &gt; 0, \n                            unique(subdata$EV), 0)\n }\nsum(bidenvotes) # predicted\n\n[1] 351\n\n\n\n\n7.4.3 Check Accuracy\n\n7.4.3.1 Quantitative Measures of Accuracy\nLet’s calculate two common measures of prediction error: bias (the average prediction error) and root-mean-squared error (a typical magnitude of the prediction error).\n\n## Calculate Bias (Predicted Biden - True Biden)\npredictionerror &lt;- polls.margin -result.margin \nbias &lt;- mean(predictionerror)\n\n## Root Mean Squared Error\nsqrt(mean((predictionerror)^2))\n\n[1] 6.052873\n\n\nOn average, the poll-based prediction was more than 4 points larger for Biden’s margin than the actual result.\nWe can create a plot similar to the left plot from the AAPOR report.\n\n\n## Histogram of Prediction Errors to Show Bias\nhist(predictionerror, \n     xlab = \"Prediction Error (Predicted Biden Margin - Actual)\",\n     main = \"Histogram of Prediction Error in Latest Polls\")\nabline(v=mean(predictionerror), col=\"red\")\nabline(v=0)\n\n\n\n\n\n\nBonus: Another way to visualize the prediction error\n\n\nLet’s create our own version of this AAPOR Plot\n\n\nWe will plot the prediction error on the x-axis, and list the corresponding states on the y-axis.\n\nWe will sort the prediction error to make it easier to see the pattern of results.\n\n\nplot(x=sort(predictionerror), y=1:length(predictionerror),\n     main=\"Average Prediction Error by State \\n Biden - Trump Margin\",\n     ylab=\"State\",\n     xlab=\"Prediction Error (Predicted Biden Margin - Actual Margin)\",\n     yaxt=\"n\",\n     bty=\"n\",\n     xlim = c(-5, 15)) \nabline(v=0, lty=2)\naxis(2, 1:length(predictionerror), labels=names(sort(predictionerror)), las=1, \n     cex.axis=.5,tick=F)\naxis(1, seq(-5, 15, 5), seq(-5, 15, 5))\ntext(-3, 15, \"Poll Margin \\n Skewed Toward Trump\", cex=.7)\ntext(8, 15, \"Poll Margin \\n Skewed Toward Biden\", cex=.7)\n\n\n\n\n\n\n\n7.4.3.2 Classification\nInstead of quantifying how far we were off, let’s see where we were right vs. where we were wrong.\nClassification\n\ntrue positive: correctly predicting Biden to be the winner\nfalse positive: incorrectly predicting Biden to be the winner\ntrue negative: correctly predicting Biden to be the loser\nfalse negative: incorrectly predicting Biden to be the loser\n\nConfusion Matrix\nLet’s classify our predictions.\n\nactualwins &lt;- ifelse(result.margin &gt; 0, \"Biden Won\", \"Trump Won\")\npredictedwins &lt;- ifelse(polls.margin &gt; 0, \"Biden Won\", \"Trump Won\")\n\n\ntable(predictedwins, actualwins)\n\n             actualwins\npredictedwins Biden Won Trump Won\n    Biden Won        28         3\n    Trump Won         0        24\n\n\nWhere did the polls get it wrong?\n\nactualwins[actualwins != predictedwins]\n\n         FL         ME2          NC \n\"Trump Won\" \"Trump Won\" \"Trump Won\" \n\n\nWhat’s your conclusion?\n\nAre the polls alright?\nHow could you improve the prediction?\nWait a second… why even poll?"
  },
  {
    "objectID": "08-Regression.html#regression-in-the-wild.",
    "href": "08-Regression.html#regression-in-the-wild.",
    "title": "8  Prediction with Regression",
    "section": "8.1 Regression in the wild.",
    "text": "8.1 Regression in the wild.\nRegression is used across many domains for prediction and classification, from fantasy football to making World Cup predictions, or even predicting how far a contestant will go on The Bachelor or The Bachelorette.\n\nUsing data to predict reality TV outcomes.\n \nIn politics, we might use regression to build campaign models– predicting which voters are persuadable, which supporters will volunteer at campaign events, which supporters will turn out to vote, etc."
  },
  {
    "objectID": "08-Regression.html#application-baseball-predictions",
    "href": "08-Regression.html#application-baseball-predictions",
    "title": "8  Prediction with Regression",
    "section": "8.2 Application: Baseball Predictions",
    "text": "8.2 Application: Baseball Predictions\nFor our first example, we will stay outside of politics and use regression to predict the success of a baseball team.\nMoneyball is a $100 million Hollywood movie that is all about linear regression… and some baseball… and Brad Pitt, but really… it’s MOSTLY about linear regression\n\nThe movie describes the Oakland A’s shift to start using data to build their team. They make two observations 1) To win baseball games, you need runs. 2) To score runs, you need to get on base. We can estimate what on base percentage we would need as a team to score enough runs to make the playoffs in a typical season.\nWe will use regression to make these predictions.\nFor a video explainer of the code for this application, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nWe use baseball.csv data\n\nRS: runs scored;\nRA: runs allowed;\nW: wins;\nPlayoffs: whether team made playoffs;\nOBP: on base percentage;\nBA: batting average;\nSLG: Slugging Percentage\n\n\nbaseball &lt;- read.csv(\"baseball.csv\")\n\n\nhead(baseball)\n\n  Team League Year  RS  RA  W   OBP   SLG    BA Playoffs RankSeason\n1  ARI     NL 2012 734 688 81 0.328 0.418 0.259        0         NA\n2  ATL     NL 2012 700 600 94 0.320 0.389 0.247        1          4\n3  BAL     AL 2012 712 705 93 0.311 0.417 0.247        1          5\n4  BOS     AL 2012 734 806 69 0.315 0.415 0.260        0         NA\n5  CHC     NL 2012 613 759 61 0.302 0.378 0.240        0         NA\n6  CHW     AL 2012 748 676 85 0.318 0.422 0.255        0         NA\n  RankPlayoffs   G  OOBP  OSLG\n1           NA 162 0.317 0.415\n2            5 162 0.306 0.378\n3            4 162 0.315 0.403\n4           NA 162 0.331 0.428\n5           NA 162 0.335 0.424\n6           NA 162 0.319 0.405\n\n\nBelow we can see the first observation made: Runs scored are highly correlated with team wins\n\n\n\n\n\nWhat the A’s noticed is that a team’s On Base Percentage is also highly correlated with runs scored. This aligns with conventional wisdom. Players get a lot of hype when they achieve a high OBP.\n\nHarper is one of only 11 players with at least 100 plate appearances and a postseason on base percentage plus slugging percentage greater than 1.000. - USA Today\nThis correlation shows up in our data, too."
  },
  {
    "objectID": "08-Regression.html#step-1-approach--regression-in-r",
    "href": "08-Regression.html#step-1-approach--regression-in-r",
    "title": "8  Prediction with Regression",
    "section": "8.3 Step 1: Approach- Regression in R",
    "text": "8.3 Step 1: Approach- Regression in R\nA regression draws a “best fit line” between the points. This allows us – for any given OBP – to estimate the number of runs scored.\n\nOur best prediction of the number of runs scored would be the spot on the purple line directly above a given OBP.\n\n\n\n\n\n\nThe regression model is \\(Y = \\alpha + \\beta X + \\epsilon\\). Let’s demystify this. See also this interactive tutorial: https://ellaudet.iq.harvard.edu/linear_model.\n\nA regression model describes the relationship between one or more independent variables \\(X\\) (explanatory variables) and an outcome variable \\(Y\\) (dependent variable)\n\nFor example, the relationship between our independent variable, On Base Percentage, and our dependent variable, Runs Scored\n\nWe want to know what happens with our dependent variable \\(Y\\) if our independent variable \\(X\\) increases.\n\nAs we increase our On Base Percentage, a regression model will help us estimate how much we should expect our Runs Scored to increase (or decrease)\n\n\\(\\alpha\\) and \\(\\beta\\) are considered “parameters” – things we don’t know but want to estimate. These two numbers will define exactly how we think \\(X\\) and \\(Y\\) are related (the shape of the line).\nNo two variables are perfectly related, so we also have the \\(\\epsilon\\) term, which describes the error in the model\n\nWhen we have data, we estimate \\(Y\\), \\(\\alpha\\), and \\(\\beta\\): \\(\\hat Y = \\hat \\alpha + \\hat \\beta X\\).\n\nThe \\({\\hat{hat}}\\) over the letters means those are our estimated values.\n\nIn R, the regression syntax is similar to what we use in making a boxplot: fit &lt;- lm(y ~ x, data = mydata)\n\nfit is just whatever you want to call the output of the model,\ny is the name of the dependent variable,\nx is the name of the independent variable, and\nmydata is whatever you have called your dataframe. E.g.:\n\n\nfit &lt;- lm(RS ~ OBP, data = baseball)\n\nWhen we have data, we estimate \\(Y\\), \\(\\alpha\\), and \\(\\beta\\): \\(\\hat Y = \\hat \\alpha + \\hat \\beta X\\).\n\nOur model gives us the “coefficient” estimates for \\(\\hat \\alpha\\) and \\(\\hat \\beta\\).\n\n\ncoef(fit)\n\n(Intercept)         OBP \n  -1076.602    5490.386 \n\n\nThe first coefficient is \\(\\hat \\alpha\\), this represents the intercept – the estimated value our dependent variable will take if our independent variable (\\(x\\)) is 0.\n\nThe value the estimated runs scored would be if a team had a 0.000 on base percentage. In our case, this value is estimated to be negative, which is impossible (but it would also be unusual for a team to have a 0.000 on base percentage). Therefore, the intercept isn’t inherently substantively interesting to us.\n\nThe second coefficient is \\(\\hat \\beta\\) is the slope This represents the expected change in our dependent variable for a 1-unit increase in our independent variable.\n\nFor example, if we go from a 0.000 on base percentage to a 1.000 on base percentage, we would expect a 5490.4 increase in runs scored!\nNote: slope can be positive or negative similar to correlation … BUT …\nNote: slope is in the units of the dependent variable (e.g., runs). It is not constrained to be between -1 and 1.\nIt is telling us that the greater the OBP, the better!\n\n\n8.3.1 Visualizing a regression\nWe can plot the regression using a scatterplot and abline().\n\nplot(x=baseball$OBP, y=baseball$RS, \n     ylab = \"Runs Scored\",\n     xlab =  \"On Base Percentage\", \n     main=\"Runs Scored by On Base Percentage\",\n     pch=20)\n\n## Add regression line\nabline(fit, lwd=3, col = \"purple\") # add line\n\n\n\n\n\n\n8.3.2 Making predictions with regression\nA regression model allows us to estimate or “predict” values of our dependent variable for a given value of our independent variable.\n\n\n\n\n\nThe red dot represents our estimate (best prediction) of the number of runs scored if a team has an on base percentage of .300. In R, we can calculate this value using predict().\n\nThe syntax is predict(fit, data.frame(x = value)) where fit is the name of the model, x is the name of the independent variable, and value represents the value for the independent variable for which you want to predict your outcome (e.g., .300).\n\n\npredict(fit, data.frame(OBP=.300))\n\n       1 \n570.5137 \n\n\nUnder the hood, this is just using the regression formula described above. For example, to estimate the number of runs scored for a .300 on base percentage, we take \\(\\hat \\alpha + \\hat \\beta * .300\\)\n\nNote that below we compare the output of the predict function to our output if we manually calculated the estimated value.\n\n\npredict(fit, data.frame(OBP=.300))\n\n       1 \n570.5137 \n\n# a + b*.300\ncoef(fit)[1] +  coef(fit)[2]*.300\n\n(Intercept) \n   570.5137 \n\n\nLet’s say a team thought they needed about 900 runs scored to get to the playoffs, and they were pretty sure they could get a team on base percentage of .500. How many runs would they be expected to score with that OBP? Do you think they will make the playoffs?\n\n\nTry on your own, then expand for the solution.\n\n\npredict(fit, data.frame(OBP=.500))\n\n       1 \n1668.591 \n\n\nIt’s greater than 900, so we should feel good about our chances."
  },
  {
    "objectID": "08-Regression.html#step-2-checking-accuracy-of-model",
    "href": "08-Regression.html#step-2-checking-accuracy-of-model",
    "title": "8  Prediction with Regression",
    "section": "8.4 Step 2: Checking accuracy of model",
    "text": "8.4 Step 2: Checking accuracy of model\nUnderstanding prediction error: Where do \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) come from? Recall that a regression tries to draw a “best fit line” between the points of data.\nUnder the hood of the regression function, we are searching for the values of \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) that try to minimize the distance between the individual points and the regression line.\n\nThis distance is called the residual: \\(\\hat \\epsilon_i = Y_i - \\hat Y_i\\).\n\nThis is our prediction error: How far off our estimate of Y is (\\(\\hat Y_i\\)) from the true value of Y (\\(Y_i\\))\nLinear regressions choose \\(\\hat \\alpha\\) and \\(\\hat \\beta\\) to minimize the “squared distance” of this error (think of this as the magnitude of the distance). This is why we tend to call this type of linear regression ordinary least squares (OLS regression).\n\nIf instead we chose the red line in the image below to be the regression line, you can see that the typical prediction error would be much larger. That’s why we end up with the purple line.\n\n\n8.4.1 Root Mean Squared Error\nJust like we had root mean squared error in our poll predictions, we can calculate this for our regression.\n\nJust like with the polls, this is the square root of the mean of our squared prediction errors, or “residuals” in the case of regression.\n\nR will give us this output using this formula where fit is the hypothetical name we have provided for the regression model: sqrt(mean(fit$residuals^2))\n\n\n\nsqrt(mean(fit$residuals^2))\n\n[1] 39.78956\n\n\n\nIn our case, using on based percentage to predict runs scored, our estimates are off typically, by about 40 runs scored.\nOn the graph, this means that the typical distance between a black point and the purple line is about 40."
  },
  {
    "objectID": "08-Regression.html#step-3-iterate-and-compare-models",
    "href": "08-Regression.html#step-3-iterate-and-compare-models",
    "title": "8  Prediction with Regression",
    "section": "8.5 Step 3: Iterate and Compare Models",
    "text": "8.5 Step 3: Iterate and Compare Models\nWhen building predictive models, often researchers want to minimize this Root-Mean Squared Error – minimizing the magnitude of the typical prediction error (the distance between the actual value of our outcome, and the true value)\nExample: Let’s compare the RMSE from two different models:\n\n## Predicting Runs Scored with OBP\nfit &lt;- lm(RS ~ OBP, data = baseball)\n\n## Predicting Runs Scored with Batting Average\nfit2 &lt;- lm(RS ~ BA, data = baseball)\n\nThe Oakland A’s noticed that OBP was a more precise predictor than BA, and RMSE gives us one way to assess this.\n\n8.5.1 Regression with Multiple Predictors\nYou can also add more than 1 predictor to a regression using the + sign.\n\n## Predicting Runs Scored with OBP and Slugging Percentage\nfit3 &lt;- lm(RS ~ OBP + SLG, data = baseball)\nsqrt(mean(fit3$residuals^2))\n\n[1] 25.09135\n\n\nLook how the RMSE dropped again, improving our prediction."
  },
  {
    "objectID": "08-Regression.html#application-predicting-campaign-donations",
    "href": "08-Regression.html#application-predicting-campaign-donations",
    "title": "8  Prediction with Regression",
    "section": "8.6 Application: Predicting Campaign Donations",
    "text": "8.6 Application: Predicting Campaign Donations\nCan we predict campaign donations?\n\nData from Barber, Michael J., Brandice Canes‐Wrone, and Sharece Thrower. “Ideologically sophisticated donors: Which candidates do individual contributors finance?.” American Journal of Political Science 61.2 (2017): 271-288\n\nload(\"donationdata.RData\")\n\nVariables\n\ndonation: 1=made donation to senator, 0=no donation made\ntotal_donation: Dollar amount of donation made by donor to Senator\nsameparty: 1=self-identifies as being in the candidate’s party; 0 otherwise\nNetWorth: Donor’s net worth. 1=less than 250k, 2=250-500k; 3=500k-1m; 4=1-2.5m; 5=2.5-5m; 6=5-10m; 7=more than 10m\nIncomeLastYear: Donor’s household annual income in 2013. 1=less than 50k; 2=50-100k; 3=100-125k; 4=125-150k; 5=150-250k; 6=250-300k; 7=300-350k; 8=350-400k; 9=400-500k; 10=more than 500k\nperagsen: percent issue agreement between donor and senator\nper2agchal: percent issue agreement between donor and the senator’s challenger\ncook: Cook competitiveness score for the senator’s race. 1 = Solid Dem or Solid Rep; 2 = Likely\nmatchcommf: 1=Senator committee matches donor’s profession as reported in FEC file; 0=otherwise\nEdsum: Donor’s self-described educational attainment. 1=less than high school; 2=high school; 3=some college; 4=2-year college degree; 5=4-year college degree; 6=graduate degree\n\nData represent information on past donors to campaigns across different states. The key dependent variable that we want to predict is total_donation: the total dollar amount a particular person in the data gave to their senator in the 2012 election campaign.\nCan we predict how much someone donates to a U.S. Senate campaign?\n\nChoose approach: regression of donations on donor characteristics\nCheck accuracy: calculate root-mean-squared error\nIterate: try different regression model specifications\n\nLet’s try a prediction based on a person’s income.\n\nfit &lt;- lm(total_donation ~ IncomeLastYear, data = donationdata)\n\nFrom this, we can\n\nPlot the relationship\nMake specific predictions at different levels of income\nCheck accuracy by calculating the prediction errors and RMSE\n\n\n8.6.1 Visualizing the results\nNote that the correlation is a bit weaker here.\n\nplot(x=donationdata$IncomeLastYear, \n     y=donationdata$total_donation,\n     ylab= \"Total Donation ($)\",\n     xlab = \"Income Last Year\",\n     main = \"Predicting Total Donations Using Income\")\nabline(fit, col=\"green4\", lwd=2)\n\n\n\n\n\n\n8.6.2 Step 1: Calculate Predictions\nWe can calculate predictions based on a level of income. Example: Level 5 of income represents an income of $150k-250k. What level of donation would we expect?\n\npredict(fit, data.frame(IncomeLastYear = 5))\n\n       1 \n348.8581 \n\n## alternative using coef()\ncoef(fit)[1] + coef(fit)[\"IncomeLastYear\"]*5\n\n(Intercept) \n   348.8581 \n\n\n\n\n8.6.3 Step 2: Check Accuracy\nWe can calculate the Root Mean Squared Error\n\nsqrt(mean(fit$residuals^2))\n\n[1] 914.5273\n\n\n\n\n8.6.4 Step 3: Iterate\nYOUR TURN: Change the model and see if it improves the prediction using RMSE using sqrt(mean(fit$residuals^2)).\n\n\n8.6.5 Adding Model Predictors\nNew Model Example\n\nfitnew &lt;- lm(total_donation ~ IncomeLastYear + NetWorth + sameparty, \n             data=donationdata)\n\nNew Predictions: note how we add more variables\n\npredict(fitnew, data.frame(IncomeLastYear = 5, NetWorth = 4, sameparty = 1))\n\n       1 \n406.9705 \n\n## alternative using coef()\ncoef(fitnew)[1] + coef(fitnew)[\"IncomeLastYear\"]*5 + \n  coef(fitnew)[\"NetWorth\"]*4 + coef(fitnew)[\"sameparty\"]*1\n\n(Intercept) \n   406.9705 \n\n\nRoot Mean Squared Error\n\nsqrt(mean(fitnew$residuals^2))\n\n[1] 908.9817\n\n\nWhen we have multiple predictors, this changes our interpretation of the coefficients slightly.\n\nWe now interpret the slope as the change in the outcome expected with a 1-unit change in the independent variable– holding all other variables constant (or ``controlling” for all other variables)\nFor example, for a 1-unit change in Income, we would expect about a $68 increase in estimated donations, holding constant Net Worth and whether the person shared partisanship with the senator.\n\n\ncoef(fitnew)\n\n   (Intercept) IncomeLastYear       NetWorth      sameparty \n    -242.02780       67.96825       29.55847      190.92323 \n\n\nThink of this like a set of light switches. How does adjusting one light switch affect the light in the room– holding constant all other switches.\n\nWhen we make predictions with multiple variables, we have to tell R where we want to set each variable’s value.\n\npredict(fitnew, data.frame(IncomeLastYear = 5, NetWorth = 4, sameparty = 1))\n\n       1 \n406.9705 \n\n\nSee how the prediction changes if you shift IncomeLastYear but keep Net Worth and partisanship where they are. That’s the idea of “controlling” for the other variables!\nHow could we keep improving the predictions?\nEventually, we would want to apply this prediction model in a real-world setting.\n\nHow could campaigns use these types of prediction models?"
  },
  {
    "objectID": "08-Regression.html#uncertainty-with-prediction",
    "href": "08-Regression.html#uncertainty-with-prediction",
    "title": "8  Prediction with Regression",
    "section": "8.7 Uncertainty with Prediction",
    "text": "8.7 Uncertainty with Prediction\nRegression (and other prediction algorithms) give us our best guess\n\nBut any guess has some uncertainty, prediction error, and potential outliers\nSometimes these errors can be systematic\nEven when we use more advanced statistical models\nA “best guess” is often better than a random guess– but shouldn’t necessarily be treated as “ground truth.”\n\nPrediction helps us guess unknowns with observed data, but MUST PROCEED WITH CAUTION\n\n8.7.1 Example: Butterfly Ballot in Florida\nIn the U.S. 2000 presidential election, the race came down to Florida, which was extremely close. As part of the contest, different counties in Florida came under a microscope. One result that seemed unusual was the amount of votes Buchanan received in certain areas, which seemed to be a result of an odd ballot design choice. In this exercise, we examine voting patterns in Florida during the 2000 election.\n\nFor more on the 2000 race, you can watch this video.\nLoad the data and explore the variables\n\ncounty: county name\nClinton96: Clinton’s votes in 1996\nDole96: Dole’s votes in 1996\nPerot96: Perot’s votes in 1996\nBush00: Bush’s votes in 2000\nGore00: Gore’s votes in 2000\nBuchanan00: Buchanan’s votes in 2000\n\n\nflorida &lt;- read.csv(\"florida.csv\")\n\nChapter 4 in QSS also discusses this example.\nUsing what you learned from the last section, try to complete the following steps:\n\nRegress Buchanan 2000 votes (your Y) on Perot 1996 (your X) votes\nCreate a scatterplot of the two variables and add the regression line\nFind and interpret the slope coefficient for the relationship between Perot and Buchanan votes\nCalculate the root-mean-squared error for the regression and interpret this\n\n\n\nTry on your own, then expand for the solution.\n\nFor every 1 additional vote Perot received in 1996, we expect Buchanan to receive .036 additional votes in 2000.\n\nfit &lt;- lm(Buchanan00 ~ Perot96, data = florida)\ncoef(fit)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nIn 1996, Perot received 8 million votes as a third-party candidate. Buchanan received less than 1/2 a million. Overall Perot received more votes, but where Perot received votes in 1996 was positively correlated with where Buchanan received votes in 2000.\n\nplot(x=florida$Perot96,\n     y=florida$Buchanan00,\n     ylab=\"Buchanan Votes 2000\",\n     xlab=\"Perot Votes 1996\",  \n     pch=20)\nabline(fit, lwd=3, col=\"purple\")\n\n\n\n\n\nsqrt(mean(fit$residuals^2))\n\n[1] 311.6187\n\n\nA typical prediction error is about 316.4 votes above or below the Buchanan total.\n\n\n\n8.7.2 Multiple Predictors\nCan we reduce the error by adding more variables?\n\nfitnew &lt;- lm(Buchanan00 ~ Perot96 + Dole96 + Clinton96, data = florida)\ncoef(fitnew)\n\n (Intercept)      Perot96       Dole96    Clinton96 \n20.572650070  0.030663207 -0.001559196  0.001865809 \n\n\nAgain, when we have multiple predictors, this changes our interpretation of the coefficients slightly.\n\nWe now interpret the slope as the change in the outcome expected with a 1-unit change in the independent variable– holding all other variables constant (or ``controlling” for all other variables)\nFor example, a 1-unit increase (a 1-vote increase) in the number of Perot voters in 1996 is associated with a 0.03 vote increase in the number of Buchanan votes in 2000, holding constant the number of Clinton and Dole votes a county received.\n\nWhen we make predictions with multiple variables, we have to tell R where we want to set each variable’s value.\n\npredict(fitnew, data.frame(Perot96=20000, Clinton96=300000, Dole96=300000))\n\n       1 \n725.8208 \n\n\nSee how the prediction changes if you shift Perot96 but keep the other variables where they are. That’s the idea of “controlling” for the other variables!\nThe addition of the new variables, in this case, made very little difference in the RMSE.\n\nsqrt(mean(fit$residuals^2))\n\n[1] 311.6187\n\nsqrt(mean(fitnew$residuals^2))\n\n[1] 308.7296\n\n\nWith little change from the addition of predictors, let’s stick with the more simple model and explore the prediction errors.\n\nplot(x=fitted(fit), # predicted outcome\n     y=resid(fit),  # prediction error\n     type=\"n\", # makes the plot blank\n     xlim = c(0, 1500), \n     ylim = c(-750, 2500), \n     xlab = \"Predicted Buchanan Votes\", \n     ylab = \"Prediction Error\")\nabline(h = 0) # adds horizontal line\ntext(x=fitted(fit), y=resid(fit), labels = florida$county, cex=.8)\n\n\n\n\nHow does the prediction error change if we remove Palm Beach County?\n\nflorida.pb &lt;- subset(florida, subset = (county != \"PalmBeach\"))\nfit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida.pb)\nsqrt(mean(fit2$residuals^2))\n\n[1] 86.41017\n\n\nMy, oh my, our RMSE also goes way down if we remove Palm Beach. Something unique seems to be happening in that county. See this academic paper for an elaboration of the evidence that “The Butterfly [ballot] Did it.”\n\n\n8.7.3 Confidence Intervals\nSocial scientists like to characterize the uncertainty in their predictions using what is called a “confidence interval.”\n\nConfidence intervals show a range of values that are likely to contain the true value\n\n\npredict(fit, data.frame(Perot96 = 13600), interval = \"confidence\")\n\n       fit      lwr      upr\n1 489.7903 394.8363 584.7443\n\n\nBy default, R supplies the 95% confidence interval.\n\nFor example, our estimate is for a county with 13,600 votes for Perot in 1996, the expected Buchanan vote is 489.79 votes.\n\nThe confidence interval is 394.84 to 584.74 votes, which means we believe there is a 95% chance that this interval contains the true value of the Buchanan 2000 vote share.\n\nInstead of thinking about our prediction as just 489.79, we should think about the entire interval as having a good chance of including the true value.\n\nSimilarly, our coefficients also have uncertainty.\n\ncoef(fit)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\nconfint(fit)\n\n                   2.5 %       97.5 %\n(Intercept) -98.03044506 100.72194929\nPerot96       0.02724733   0.04458275\n\n\nFor every 1 vote increase in the Perot 1996 vote, we expect a \\(\\hat \\beta =.036\\) increase in Buchanan votes. However, the confidence interval is 0.027 to 0.045.\n\nWe think there is a 95% chance that this interval 0.027 to 0.045 includes the true \\(\\beta\\), describing the rate of change in Buchanan votes for a given change in Perot 1996 votes"
  },
  {
    "objectID": "08-Regression.html#cross-validation",
    "href": "08-Regression.html#cross-validation",
    "title": "8  Prediction with Regression",
    "section": "8.8 Cross-Validation",
    "text": "8.8 Cross-Validation\nSo far, we’ve been “cheating.” We’ve been analyzing a prediction in cases where we know the right answer. Now we will focus more squarely on how to develop an “out-of-sample” prediction.\nProblem: Models that fit our existing (“in-sample”) data might not be the best for predicting out-of-sample data.\nExample: Compare regression line with vs. without Palm Beach included in the sample.\n\nOutliers “in-sample” can lead to overfitting to weird, idiosyncratic data points\n\n\nExample: Error Rates in Speech Recognition. See study here\n\nPredictions/Classification that works well for one group might not work well for all groups\n\n\nProblem: Models that fit our existing (“in-sample”) data might not be the best for predicting out-of-sample data. Approaches to diagnose the problem or help address it:\n\nDetect potential outliers within existing data by exploring the prediction errors\nMake sure the training (in-sample) data is as representative as possible\nIncorporate out-of-sample testing in prediction process\n\n\n8.8.1 Cross-Validation Process\nCross-validation incorporates the idea of out-of-sample testing into the process of how we evaluate the accuracy of prediction and classification approaches.\nCross-validation (train vs. test data)\n\nSubset your data into two portions: Training and Test data.\nRun a model based on the training data.\nMake a prediction and test the accuracy on the test data.\nRepeat process training and testing on different portions of the data.\nSummarize the results and choose a preferred model\n\nEventually: Apply this model to entirely new data\n\n\nGoal: Test accuracy in a way that can help detect overfitting. See how well our model will generalize to new data (data the model hasn’t seen).\n\n\n8.8.2 Application: Forecasting Election Results\nMacro political and economic fundamentals are sometimes used for early forecasting of an election. We will build a version of this model and test its accuracy using a process of “Leave-one-out” cross-validation.\nBelow is a video explainer of this application, which uses cross-validation.\n\nThe data and model are based on the FAIR model of forecasting.\n\nfair &lt;- read.csv(\"fair.csv\")\n\nKey Variables:\n\nVP: Democratic share of the two-party presidential vote\nt: Year (presidential years only)\nG: growth rate of real per capita GDP in the first 3 quarters\nP: growth rate of the GDP deflator in the first 15 quarters of the recent administration\nZ: number of quarters in the first 15 quarters of recent administration in which the growth rate of real per capita GDP is greater than 3.2 percent at an annual rate\nI: 1 if Democrats in WH and -1 if Republicans in WH\nWAR: 1 if 1920, 1944, 1948 (denoting the ``WAR” elections, which are believed to be particular)\nDUR: indicating how many consecutive terms Democrats/Republicans have been office (e.g., in 2020 it will be 0 because Republicans will have been in office for only 1 term.)\n\nLet’s propose a model\n\nfit &lt;- lm(VP ~ DUR, data = fair)\n\nLet’s propose an alternative model and see which one we think is better.\n\nfit2 &lt;- lm(VP ~ G*I +  DUR, data = fair)\n\nNote: The asterisk represents an “interaction.” See QSS Chapter 4. We use this when we think the effect of one variable (growth) may depend on the values of another variable (the party of who is in office).\n\n8.8.2.1 Steps 1 and 2\nWe are going to run a model where each time we `leave out’ one row of data (in our case, one election). Let’s try this once:\n\nyears &lt;- fair$t\n\n## Step 1: Subset data into two portions\ntraindata &lt;- subset(fair, t != years[1])\ntestdata &lt;- subset(fair, t == years[1])\n\n## Step 2: Run model on training data\nfit &lt;- lm(VP ~ DUR, data = traindata)\nfit2 &lt;- lm(VP ~  G*I +  DUR, data = traindata)\n\n\n\n8.8.2.2 Step 3: Predict and assess accuracy with test data\nOut-of-Sample prediction\n\n## Step 3: Make a Prediction using test data and\nyhat.fit &lt;- predict(fit, testdata)\nyhat.fit2 &lt;- predict(fit2, testdata)\n\nPrediction error (Truth - Prediction)\n\n## Step 3: Test accuracy of prediction\nerror.fit &lt;- testdata$VP - yhat.fit\nerror.fit2 &lt;- testdata$VP - yhat.fit2\n\n\n\n8.8.2.3 Step 4: Repeat process across all data\nStep 4: Let’s do this for each row, storing the prediction errors.\n\n## Iteration vector\nyears &lt;- fair$t\n## Empty container vectors\nerrors.fit &lt;- rep(NA, length(years))\nerrors.fit2 &lt;- rep(NA, length(years))\n\n## Loop (copy paste meat from  above)\nfor(i in  1:length(years)){\n  traindata &lt;- subset(fair, t != years[i])\n  testdata &lt;- subset(fair, t == years[i])\n  fit &lt;- lm(VP ~  DUR, data = traindata)\n  fit2 &lt;- lm(VP ~ G*I +  DUR , data = traindata)\n  yhat.fit &lt;- predict(fit, testdata)\n  yhat.fit2 &lt;- predict(fit2, testdata)\n  errors.fit[i] &lt;- testdata$VP - yhat.fit\n  errors.fit2[i] &lt;- testdata$VP - yhat.fit2\n}\n\n\n\n8.8.2.4 Step 5: Summarize performance\nStep 5: Summarize the model performance\n\n## RMSE\nsqrt(mean((errors.fit)^2))\n\n[1] 7.170149\n\nsqrt(mean((errors.fit2)^2))\n\n[1] 3.793135\n\n## Mean Absolute Error\nmean(abs(errors.fit))\n\n[1] 5.937542\n\nmean(abs(errors.fit2))\n\n[1] 3.363163\n\n\nWhich model tends to have less error?\n\n\n8.8.2.5 Applying Model to New Data\nEventually, you might further test the model on data that has been “held out”– data that neither your train/test has seen. How good was our model? We can do this for the 2016 election, which was not in the data.\nTruth: 2016 VP was 51.1 Democratic “two-party” vote share.\n\n## Let's use the winner of our two models\nfit2 &lt;- lm(VP ~ G*I +  DUR, data = fair)\n51.1-predict(fit2, data.frame(G=0.97, I=1, DUR=1))\n\n       1 \n2.472147 \n\n\n2016 values based on the FAIR site\n\n\n8.8.2.6 Challenge\nCan you build a better model? What would your prediction for 2020 have been?\n\n-5.07: growth rate of real per capita GDP in the first 3 quarters of 2020 (annual rate) (G)\n1.80: growth rate of the GDP deflator in the first 15 quarters of the Trump administration, (annual rate) (P)\n3: number of quarters in the first 15 quarters of the Trump administration in which the growth rate of real per capita GDP is greater than 3.2 percent at an annual rate (Z)\nDUR=0\nI = -1\n\nValues based on the FAIR site"
  },
  {
    "objectID": "09-Fairness.html#application-criminal-justice",
    "href": "09-Fairness.html#application-criminal-justice",
    "title": "9  Fairness and Ethics",
    "section": "9.1 Application: Criminal Justice",
    "text": "9.1 Application: Criminal Justice\nThis application is based on Dressel, Julia, and Hany Farid. “The accuracy, fairness, and limits of predicting recidivism.” Science advances 4.1 (2018): eaao5580.\nPrediction and classification models are used all of the time in public policy, including in the criminal justice system: “where crimes will most likely occur, who is most likely to commit a violent crime, who is likely to fail to appear at their court hearing, and who is likely to reoffend at some point in the future” (Dressel and Farid)\nDressel and Farid develop and examine models that predict whether someone will recidivate– based on a measure of rearrest. They compare their own models to COMPAS (a well-known proprietary algorithm that generates risk scores) and to human-based predictions\nWe will develop a model similar to the one Dressel and Farid use in their paper, which seems to closely approximate COMPAS predictions.\nNote: These algorithms have generated a lot of debate, concern and controversy, which we will discuss.\n\n9.1.1 Load data\nBelow is a video explainer of this application, which uses classification and cross-validation.\n\nData include information about 7214 arrests in Broward County Florida in 2013-2014\n\nbroward &lt;- read.csv(\"browardsub.csv\")\n\nVariables\n\nsex: 0 male; 1: female\nage\njuv_fel_count: total number of juvenile felony criminal charges\njuv_ misd_count: total number of juvenile misdemeanor criminal charges\npriors_count: total number of non-juvenile criminal charges\ncharge_degree: a numeric indicator of the degree of the charge: 0: misdemeanor; 1: felony\ntwo_year_recid: a numeric indicator of whether the defendant recidivated two years after previous charge: 0: no, did not recidivate; 1: yes, did recidivate\n\n\n\n9.1.2 Prediction/Classification process\nRecall the steps for prediction/classification\n\nChoose Approach\n\nWe will use a regression to try to classify subjects as those who will / will not recidivate\n\nCheck accuracy\n\nWe will calculate false positive rates and false negative rates\nWe will use cross-validation to do so\n\nIterate\n\n\n\n9.1.3 Step 1: Regression Model\nStep 1: Choose Approach\n\nfit &lt;- lm(two_year_recid ~ age + sex + juv_misd_count + juv_fel_count + \n            priors_count + charge_degree, \n          data = broward)\n\nNote: our outcome is binary\n\ntable(broward$two_year_recid)\n\n\n   0    1 \n3963 3251 \n\n\nWhen you use linear regression with a binary outcome, it is called a linear probability model. We estimate the probability of recidivism– a number between 0 and 1.\n\nThere are downsides to using linear regression with this type of outcome. Data scientists may often use a different model called logistic regression for this.\n\nMake Prediction.\n\n## estimates a predicted probability of recidivism for each subject\nbroward$predictedrec &lt;- predict(fit)\n\n## Range of predicted probabilities\nrange(broward$predictedrec)\n\n[1] -0.1463835  1.6059606\n\n\nNote: One downside of linear models is they can generate probabilities below 0 or above 1. Logistic regression will constrain these due to a transformation it makes when estimating the coefficients.\n\n9.1.3.1 Detour: Logistic Regression\nAs an alternative, you could use logistic regression, which data scientists may often use when trying to do a classification task– predicting which category a subject belongs to (e.g., recidivate vs. not recidivate; turned out to vote vs. did not turn out to vote). We won’t focus on logistic regression in this class, but you can know it is out there for future study.\nStep 1: Choose Approach- Let’s try logistic regression instead\n\n\nFor details, expand.\n\n\n## Logistic regression\nfitl &lt;- glm(two_year_recid ~ age + sex + juv_misd_count + juv_fel_count + \n            priors_count + charge_degree, \n          data = broward, \n          family=binomial(link = \"logit\"))\n\n## estimates a predicted probability of recidivism for each subject\nbroward$predictedrecl &lt;- predict(fitl, type=\"response\") # need type=\"response\" to make them probabilities\n\n## Range of predicted probabilities of recidivism\nrange(broward$predictedrecl)\n\n[1] 0.04176846 0.99891808\n\n\nLogistic regression keeps probabilities between 0 and 1 due to a transformation it applies to our standard regression formula. As a result, our coefficient units (coef(fitl)) are in log-odds units, which are hard to interpret. We can transform our predictions of the model into probabilities using the predict() function with type = \"response\".\n\nWe can compare the predictions of the probability of recidivism between the linear and logistic regression models. Note how the linear model blows past 0 and 1, while the logistic-based predictions can keep them within those bounds.\n\n\n\n\n\nWe don’t have time to go into the math of logistic regression in this course, but know that it is a desirable option for classification.\nFor now, let’s stick with the linear model.\n\n\n9.1.3.2 Change Prediction into a Classification\nRecall: we are trying to classify\n\nWe need to make our estimates of the probability of recidivism categorical, into simply a prediction of recidivate vs. not recidivate\n\nFor now, we will use .5 as a threshold (a probability of more than .5)\n\n# Need to make prediction binary.\n# We use .5, but there are other methods for choosing this threshold\nbroward$predictedrecclass &lt;- ifelse(broward$predictedrec &gt; .5, 1, 0)\n\nPredicted Recidivism\n\ntable(predicted=broward$predictedrecclass)\n\npredicted\n   0    1 \n4683 2531 \n\n\n\n\n\n9.1.4 Step 2: Check Accuracy\nWe are going to get extra practice with cross-validation as a way to check accuracy. Recall:\nCross-validation (train vs. test data)\n\nSubset your data into two portions: Training and Test data.\nRun a model based on the training data.\nMake a prediction and test the accuracy on the test data.\nRepeat process training and testing on different portions of the data.\nSummarize the results and choose a preferred model\n\nEventually: Apply this model to entirely new data\n\n\nGoal: Test accuracy in a way that can help detect overfitting. See how well our model will generalize to new data (data the model hasn’t seen).\nWe will use leave-one-out cross-validation again, but there are other methods, such as splitting data into “folds” of multiple observations at once (i.e., leaving out 100 or 1000 observations for testing instead of just 1).\n\n## Step 1: Subset Data\ntraindata &lt;- broward[-1,] # all but first row\ntestdata &lt;- broward[1,] # just the first row\n\n## Step 2: Run model on training data\nfittrain &lt;- lm(two_year_recid ~ age + sex + juv_misd_count \n               + juv_fel_count + \n            priors_count + charge_degree, \n          data = traindata)\n\n## Step 3: Predict with test data\npredictedrec &lt;- predict(fittrain, testdata)\n\n## Step 3: Change predicted probability into a classification\ncvpredictions &lt;- ifelse(predictedrec &gt; .5, 1, 0)\n\nStep 4: Repeat across all observations and summarize accuracy.\nWe want to repeat this process for every row of our data– leaving out a different row each time. To construct our loop, we embed the above process in the loop syntax.\n\n## Iteration vector\n## 1:nrow(broward)\n\n## Container vector\nbroward$cvpredictions &lt;- NA\n\nfor(i in 1:nrow(broward)){\n  ## Step 1: Subset Data\n  traindata &lt;- broward[-i,] # all but ith row\n  testdata &lt;- broward[i,] # just the ith row\n  \n  ## Step 2: Run model on training data\n  fittrain &lt;- lm(two_year_recid ~ age + sex + juv_misd_count \n                 + juv_fel_count + \n            priors_count + charge_degree, \n          data = traindata)\n  \n  ## Step 3: Predict with test data\n  predictedrec &lt;- predict(fittrain, testdata)\n  broward$cvpredictions[i] &lt;- ifelse(predictedrec &gt; .5, 1, 0)\n}\n\n\n9.1.4.1 Confusion Matrix\nCheck Accuracy: Confusion Matrix\n\nconfmatrix &lt;- table(actual = broward$two_year_recid, \n                    predicted = broward$cvpredictions)\nconfmatrix\n\n      predicted\nactual    0    1\n     0 3156  807\n     1 1528 1723\n\n\nHow should we interpret each cell?\n\nLet’s Consider 1 = Recidivate = Positive outcome; 0 = Not Recidivate = Negative Outcome\nWhat is a false positive? false negative? true positive? true negative?\n\n\n\n9.1.4.2 False Positive Rate\nFalse Positive Rate: \\(\\frac{\\text{False Positive}}{\\text{(False Positive + True Negative)}}\\)\n\nOut of those who do not recidivate, how often did we predict recidivate?\n\n\n## One Approach\nsum(broward$cvpredictions == 1 & broward$two_year_recid == 0) / \n  sum(broward$two_year_recid == 0)\n\n[1] 0.2036336\n\n## Alternative Approach\n## predicted recidivism, actual not\nfp &lt;- confmatrix[1, 2]\n# predicted not, actual not\ntn &lt;- confmatrix[1, 1]\n\n## False Positive Rate\nfp / (fp + tn)\n\n[1] 0.2036336\n\n\n\n\n9.1.4.3 False Negative Rate\nFalse Negative Rate: \\(\\frac{\\text{False Negative}}{\\text{(False Negative + True Positive)}}\\)\n\nOut of those who did recidivate, how often did we predict not recidivate?\n\n\n# Out of those who recidivate, how often does it predict not recidivate?\nsum(broward$cvpredictions== 0 & broward$two_year_recid == 1) / \n  sum(broward$two_year_recid == 1)\n\n[1] 0.4700092\n\n## Alternative Approach\n##  predicted to not recidivate, actual yes\nfn &lt;- confmatrix[2, 1]\ntp &lt;- confmatrix[2, 2]\n\n## False Negative Rate\nfn / (fn + tp)\n\n[1] 0.4700092"
  },
  {
    "objectID": "09-Fairness.html#taking-fairness-seriously",
    "href": "09-Fairness.html#taking-fairness-seriously",
    "title": "9  Fairness and Ethics",
    "section": "9.2 Taking Fairness Seriously",
    "text": "9.2 Taking Fairness Seriously\nRecall the steps for prediction/classification\n\nChoose Approach\n\nWe will use a regression to try to classify subjects as those who will / will not recidivate\n\nCheck accuracy\n\nWe will calculate false positive rates and false negative rates\nWe will use cross-validation to do so\nWhat about fairness?\n\nIterate\n\nThe performance of a prediction/classification may be different for different groups in the population. Dressel and Farid point to this when it comes to different racial groups.\n\n\n\nDressel and Farid\n\n\nLet’s see how our predictions perform across racial groups\nrace: 1: White (Caucasian); 2: Black (African American); 3: Hispanic; 4: Asian; 5: Native American; 6: Other\nWait a second– we didn’t use race in our model. Why could the performance still differ across racial groups?\n\nThink about how the inputs in our regression model could be correlated with race.\nThink about how existing human biases and inequalities that lead to differential arrest rates by racial groups could be reproduced in our model.\nEven if a model does not have the intent to include race, its impact may nonetheless vary according to race. This could be true for any number of characteristics depending on the application.\n\nWe will subset our data by race.\n\nblack &lt;- subset(broward, race == 2)\nwhite &lt;- subset(broward, race == 1)\n\nWe can first check overall accuracy\n\nmean(black$cvpredictions == black$two_year_recid)\n\n[1] 0.6696429\n\nmean(white$cvpredictions == white$two_year_recid)\n\n[1] 0.6805216\n\n\nBut are we making the same types of errors?\n\n## False positive rate- Black\n## Out of those who do not recidivate, how often did we predict recidivate?\nfprate.black &lt;- sum(black$cvpredictions == 1 & black$two_year_recid == 0) / \n  sum(black$two_year_recid == 0)\n\n## False negative rate- Black\n## Out of those who recidivate, how often does it predict not recidivate?\nfnrate.black &lt;-sum(black$cvpredictions== 0 & black$two_year_recid == 1) / \n  sum(black$two_year_recid == 1)\n\n## False positive rate- white\nfprate.white &lt;- sum(white$cvpredictions == 1 & white$two_year_recid == 0) / \n  sum(white$two_year_recid == 0)\n\n## False negative rate- white\nfnrate.white &lt;-sum(white$cvpredictions== 0 & white$two_year_recid == 1) / \n  sum(white$two_year_recid == 1)\n\nLet’s see how our predictions perform across racial groups\n\n## False positive rates\nfprate.black\n\n[1] 0.2846797\n\nfprate.white \n\n[1] 0.1323925\n\n## False negative rates\nfnrate.black\n\n[1] 0.3734876\n\nfnrate.white \n\n[1] 0.6076605\n\n\nWe see asymmetries in the types of errors the model is making across racial groups. Black subjects have higher false positives– more likely as being predicted to recidivate (a predicted “positive”) when they do not (the “false” in false positive). White subjects have higher false negatives– predicted not to recidivate (the negative) when they do (the false in false negative).\nBased on these results, reflect on the following:\n\nShould we use this type of algorithm in public policy?\n\nWhat might be desirable about this process over alternatives?\nWhat are possible concerns?\nDoes your answer depend on accuracy or other considerations?\n\nWhat should we care more about? False positives or false negatives?\nWhat measures of fairness should be considered?\nAre there ways to avoid an unfair/biased model?\n\n\n9.2.1 Extended Learning\nNote: There are many debates about the use of these algorithms\n\nExample of Initial Critique from ProPublica\nExample of Rejoinder\nDiscussion of what fairness means: J. Kleinberg, S. Mullainathan, M. Raghavan, Inherent trade-offs in the fair determination of risk scores. (2016).\n\nNotes that goals of fairness can be in competition:\nWell-calibrated: if the algorithm identifies a set of people as having a probability z of constituting positive instances, then approximately a z fraction of this set should indeed be positive instances\nBalance for positive and negative instances across groups: the chance of making a mistake on should not depend on which group they belong to.\n\n\nFor more on fairness and machine learning\n\nFAIRNESS AND MACHINE LEARNING: Limitations and Opportunities by Solon Barocas, Moritz Hardt, Arvind Narayanan.\nVivek Singh’s Research Lab\nRutgers Critical AI group\nSee the “Gender Shades” project from Joy Buolamwini\nSee Brookings Report on Bias in AI"
  },
  {
    "objectID": "10-Uncertainty.html#hypothesis-testing-overview-of-process",
    "href": "10-Uncertainty.html#hypothesis-testing-overview-of-process",
    "title": "10  Uncertainty",
    "section": "10.1 Hypothesis Testing Overview of Process",
    "text": "10.1 Hypothesis Testing Overview of Process\nProcess for Hypothesis Testing\n\nStart with a research question: Are job applicants with criminal records less likely to receive call backs for interviews than applicants without criminal records?\nDevelop a theory of how the world works\n\nE.g., “Job prospects are, in part, a function of someone’s criminal record.”\n\nConstruct “null” and “alternative” hypotheses\n\nE.g., \\(H_o\\): “Applicants with a criminal record will receive call backs at similar rates as those without a criminal record” (I.e., no difference)\nE.g., \\(H_A\\): “Applicants with a criminal record will be less likely to receive call backs than those without a criminal record” or “Applicants with a criminal record will receive call backs at different rates than those without a criminal record” (I.e., some nonzero difference)\n\n\nExample of Implied Framework\nHealth Savings Experiment (Dupas and Robinson 2013): Researchers conducted a field experiment in rural Kenya in which they randomly varied access to four innovative saving technologies and observed the impact on asset accumulation. Participants who were given a box locked with a padlock and key saved about 150 Kenyan Shillings more after 12 months relative to those who were simply encouraged to save money for health. Did the lock box work?\n\nWhat are the implied null and alternative hypotheses?\n\nProcess for Hypothesis Testing\n\nCarry out a test of the hypothesis, such as a difference-in-means.\n\nApplicants with a criminal record receive 12.5 percentage points fewer call backs than those without a criminal record\n\nCalculate the uncertainty around this estimate.\nDecide whether you can reject or fail to reject the hypothesis of no difference"
  },
  {
    "objectID": "10-Uncertainty.html#sampling-and-uncertainty",
    "href": "10-Uncertainty.html#sampling-and-uncertainty",
    "title": "10  Uncertainty",
    "section": "10.2 Sampling and Uncertainty",
    "text": "10.2 Sampling and Uncertainty\nFlip a coin 10 times here. Report how many times it lands on heads.\n\nImagine repeating this process over and over again.\nWe know that a fair coin should land on heads 50% of the time- 5 out of 10 times or 50 out of 100 times.\nHowever, in any given sample of coin flips, you might get a slightly different result. If you repeated the sample a bunch of times, sometimes you might get 4 heads, 5 heads, 6 heads, 3 heads, etc.\n\nThis does not mean the coin is unfair. Instead, just due to chance, we ended up with 6 out of 10 heads in a world where the true proportion of times that coin would land on heads is .5.\nHow much evidence would we need to reject the idea that the coin is unfair? What if we got 90 heads out of 100 coin flips? Would that be enough to make us skeptical of the coin?\n\nThis is the idea of null hypothesis testing. We gather evidence and make a judgment about whether we can reject a null hypothesis. How likely would it be that by chance we could flip 90 heads in a world where that coin was actually fair?\n\nFor example, when we find a relationship between two variables (e.g., a correlation, a difference-in-means, a regression coefficient), this could be due to chance in a single sample.\n\n\nWhen we conduct an experiment and find that applicants with a criminal record were called back 12.5 percentage points less often than those without a criminal record, we want to know…\n\nIs that a real difference, or is the real difference 0, and we just happened to get our 12.5-point difference in our sample due to chance?\n\nIn statistical hypothesis testing, what we will try to do is quantify how likely it is that we could observe a difference as big as 12.5 in a given sample if, in fact, the real difference is 0. We want to be able to make a judgment about how likely the relationship we observed in a sample of applications/hiring decisions could exist in a world where criminal records really have no impact on job prospects.\nWe are going to use this example help us break down a few concepts.\n\n10.2.1 Sampling Distribution\nThe number of heads you generate over repeated samples is the “sampling distribution.”\n\n\n\n\n\nThe higher the curve, the more likely we would observe that number of heads. For example, if you flip a coin 100 times, it is likely you will get close to 50 heads (50%), and very unlikely you will flip more than 80 heads (80% heads).\n\nSo if you flip a coin and get 55 heads, we might still think the coin is fair, but once you start moving to the tails of this curve . . .\nIt is highly unlikely we could flip a coin 100 times and get 80 heads just by random chance i.e., if the coin were fair.\nThe “bell” shape of this distribution isn’t an anomaly. The “Central Limit Theorem” tells us that over repeated samples (so long as your sample is sufficiently large), the distribution of “means” will be normal\n\nThis is incredibly important because we know a lot about normal distributions, such as that 95% of the sample mean estimates fall within two (technically: 1.96) standard errors of the mean.\nOnly if we observe a number of heads outside those lines would we think perhaps it is not a fair coin.\n\n\nLet’s now imagine our study on criminal records and job prospects. In a world where the true difference in call back rates between those with and without criminal records is 0, in any given sample, we might end up with a 3 percentage point difference, -5 percentage point difference, 2 point difference, -1 point difference, and so on.\n\nThe shape of the bell curve would be centered on 0 difference (Central Limit Theorem), meaning on average, over repeated samples of applicants/hiring decisions, there would be 0 difference, but occasionally we would still find some differences across samples.\nWhat we want to know is if a 12.5 point difference is within two standard errors of 0 or if it is pretty unusual. (Is it more like 55 heads or 80 heads?)\n\n\n10.2.1.1 Details: Standard Errors\nA standard error is the standard deviation of the sampling distribution\n\nWhere a standard deviation is the typical distance between a given observation and the mean.\nCompare the two photos below showing 0 standard deviation vs. a large standard deviation\n\n \nFrom the Cartoon Guide to Statistics\nIn real life studies, we don’t know the actual sampling distribution because we only have 1 sample (we only had one study of applications/hiring decisions).\nSo we estimate our standard error using the standard deviation of our sample (\\(S\\)) and sample size (\\(N\\)).\n\\({\\hat {SE}} = \\frac{S}{\\sqrt{N}}\\)\nThe bigger the sample, the SMALLER the standard error (which is good) because it means less uncertainty."
  },
  {
    "objectID": "10-Uncertainty.html#z-scores-and-p-values",
    "href": "10-Uncertainty.html#z-scores-and-p-values",
    "title": "10  Uncertainty",
    "section": "10.3 Z-scores and p-values",
    "text": "10.3 Z-scores and p-values\nRecall the Overview of Hypothesis Testing. Now, we are going to add details to help us make a final decision about the null hypothesis.\n\nCarry out a test of the hypothesis, such as a difference-in-means.\n\nApplicants with a criminal record receive 12.5 percentage points fewer call backs than those without a criminal record\n\nCalculate the uncertainty around this estimate.\n\nWe will estimate the standard error of the estimate using the sample size and sample spread (standard deviation)\nWe can also estimate the confidence intervals\n\nDecide whether you can reject or fail to reject the hypothesis of no difference\n\nStandarize the estimate and find the z-score (or t-statistic, which is similar)\n\nThe z score is an example of a “test statistic.” The type of statistic might vary across applications, but its purpose will remain similar. Others include t-statistics and Chi-squared statistics.\n\nHow likely is it you would observe the z-score you found under this null distribution? (p-value)\nIf the p-value is small (\\(&lt; 0.05\\)), reject the null.\n\n\nA z-score helps us standardize the size of estimates across any units of study by quantifying the size of the estimate in terms of standard errors.\nz-score = \\(\\frac{\\text{Estimate - Null}}{\\hat{SE}} = \\frac{\\text{Estimate - 0}}{\\hat{SE}}\\)\nThe z-score represents a ratio of the estimate over the standard errors. We can visualize the distribution of z-scores in the image below.\n\nThe bell curve is centered on 0 and represents our null distribution.\n\nInstead of the axis being the number of heads out of 100 coin flips, centered on the null hypothesis of 50 heads for a fair coin, the standardized scale is centered on 0. We can then visualize how far 80 coin flips is away from 50 in terms of standard errors.\n\nRecall, we asked: When we conduct an experiment and find that applicants with a criminal record were called back 12.5 percentage points less often than those without a criminal record, we want to know…\n\nIs that a real difference, or is the real difference 0, and we just happened to get our 12.5-point difference in our sample due to random chance?\n\nIt so happens that our 12.5 percentage difference represents more than z=7.\nWe can visualize where z=7 is in our distribution below with the dashed purple line:\n\nIt is well outside of the red lines representing 1.96 standard errors.\nInterpretation: It is really unlikely we would have observed this extreme in magnitude of difference if the true difference were 0.\n\nThis likelihood represents the p-value, which is essentially 0 in this case. There is essentially 0 chance we would have observed a difference as large or larger than 12.5 (or -12.5) in a world where the true difference is 0.\nIf a p-value is &lt; 0.05, we reject the null hypothesis.\nIf instead, the p-value is larger than 0.05, we fail to reject the null. That would mean that we think it is reasonably possible to have observed a difference as big as 12.5 if in fact the true difference is 0.\n\nWe are going to focus on two-sided p-values, which focus on the magnitude of the z-score in either direction, instead of whether it is a positive or negative p-value. In other classes, you may also cover one-sided p-values.\n\n10.3.1 Relationship to Confidence Intervals\nRelationship to Confidence Intervals: Our 12.5 percentage difference has a 95% confidence interval of 6.8 to 18.3\n\nIt is constructed by taking \\(12.5 - 1.96*SE\\) and \\(12.5 + 1.96*SE\\)\nThis means there is a 95% chance that this interval contains the true population difference.\n\nIt gives us another way of describing our estimate that includes our uncertainty. We recognize that over repeated samples, we are not always going to get a 12.5 point difference. In any given sample, this estimate will differ a bit over the “sampling distribution.”"
  },
  {
    "objectID": "10-Uncertainty.html#wrapping-up-the-process",
    "href": "10-Uncertainty.html#wrapping-up-the-process",
    "title": "10  Uncertainty",
    "section": "10.4 Wrapping up the Process",
    "text": "10.4 Wrapping up the Process\nLet’s Review the Process\n\nStart with a research question: Are job applicants with criminal records less likely to receive call backs for interviews than applicants without criminal records?\nDevelop a theory of how the world works\n\nE.g., “Job prospects are, in part, a function of someone’s criminal record.”\n\nConstruct “null” and “alternative” hypotheses\n\nE.g., \\(H_o\\): “Applicants with a criminal record will receive call backs at similar rates as those without a criminal record” (I.e., no difference)\nE.g., \\(H_A\\): “Applicants with a criminal record will be less likely to receive call backs than those without a criminal record” or “Applicants with a criminal record will receive call backs at different rates than those without a criminal record” (I.e., some nonzero difference)\n\nCarry out a test of the hypothesis, such as a difference-in-means.\n\nApplicants with a criminal record receive 12.5 percentage points fewer call backs than those without a criminal record\n\nCalculate the uncertainty around this estimate.\n\nWe could estimate the standard error with the sample standard deviation and sample size\nWe could also estimate the confidence intervals.\n\nDecide whether the result is significant. Can you reject or do you fail to reject the hypothesis of no difference?\n\nUse the z-score/t-statistic and p-value\nThe z score is an example of a “test statistic.” The type of statistic might vary across applications, but its purpose will remain similar. Others include t-statistics and Chi-squared statistics."
  },
  {
    "objectID": "10-Uncertainty.html#application-health-savings-study",
    "href": "10-Uncertainty.html#application-health-savings-study",
    "title": "10  Uncertainty",
    "section": "10.5 Application: Health Savings Study",
    "text": "10.5 Application: Health Savings Study\nFor a video explainer of the code in this section, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nHealth Savings Experiment (Dupas and Robinson 2013): Field experiment in rural Kenya in which they randomly varied access to four innovative saving technologies and observed the impact on asset accumulation.\n1. Start with a research question\nCan savings technologies help people accumulate assets?\n2. Develop a theory of how the world works\nProviding people with a safe place to store money will help them save.\n3. Construct “null” and “alternative” hypotheses\nWhat are our null/alternative hypotheses?\n4. Carry out a test of the hypothesis, such as a difference-in-means.\n\nIndividuals in all study arms were encouraged to save for health and were asked to set a health goal for themselves at the beginning of the study.\n\nIn the first treatment group (Safe Box), respondents were given a box locked with a padlock and the key\nThe dependent variable is the amount saved after 12 months fol2_amtinvest\n\nWe will compare average savings between treatment conditions (a difference in means).\n\nrosca &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/rosca.csv\",\n                  stringsAsFactors = T)\n\n\n## Compare means\nmean.safebox &lt;- mean(rosca$fol2_amtinvest[rosca$safe_box == 1], na.rm=T)\nmean.encouragement &lt;- mean(rosca$fol2_amtinvest[rosca$encouragement== 1], \n                           na.rm=T)\ndiff.means &lt;- mean.safebox - mean.encouragement\ndiff.means\n\n[1] 150.3816\n\n\n5. Calculate the uncertainty around this estimate.\nTo get uncertainty when calculating a difference in means, we can use the t.test function in R.\n\nThe first input is the vector of values from one group.\nThe second input is the vector of values from the other group.\n\nTo get the t-statistic, underneath the hood of the function, R is estimating the standard error by calculating the standard deviation in the sample and the sample size (the number of people in each condition).\n\n## Compare amount saved for those in Safe Box vs. \n## Encouragement Only conditions\ntest &lt;- t.test(rosca$fol2_amtinvest[rosca$safe_box == 1],\n               rosca$fol2_amtinvest[rosca$encouragement== 1])\n\ntest\n\n\n    Welch Two Sample t-test\n\ndata:  rosca$fol2_amtinvest[rosca$safe_box == 1] and rosca$fol2_amtinvest[rosca$encouragement == 1]\nt = 2.1083, df = 150.38, p-value = 0.03666\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n   9.445604 291.317636\nsample estimates:\nmean of x mean of y \n 408.2150  257.8333 \n\n\n6. Decide whether you can reject or fail to reject the hypothesis of no difference\nWe can extract the group means, the p-value of the difference and confidence interval of the difference.\n\ntest$estimate\n\nmean of x mean of y \n 408.2150  257.8333 \n\ntest$conf.int\n\n[1]   9.445604 291.317636\nattr(,\"conf.level\")\n[1] 0.95\n\ntest$p.value\n\n[1] 0.03666403\n\n\nWas the treatment significant? We say something is significant if the p-value is small, such as less than 0.05. We also use this criteria to assess if we should reject the null hypothesis.\n\nIn a “t test”, the t-statistic serves as the z-score. It is also a ratio of standard errors. The t-statistic and z-scores differ slightly in how we calculate the corresponding p-value, but with a large enough sample size, these are also very similar. The t.test function in R calculates the p-value for you."
  },
  {
    "objectID": "10-Uncertainty.html#additional-applications",
    "href": "10-Uncertainty.html#additional-applications",
    "title": "10  Uncertainty",
    "section": "10.6 Additional Applications",
    "text": "10.6 Additional Applications\nWe will carry out the same type of hypothesis testing process for a range of different scenarios. Though the process will remain very similar, the underlying statistical test/calculation/R function will change.\n\nIf dependent variable is numeric / continuous, can use regression and lm() (type of independent variable does not matter)\nIf dependent variable is numeric and want to compare between two specific groups, can use t.test()\nIf dependent variable is binary, such as assessing the proportion of call back rates, and want to compare between two specific groups, can use prop.test()\n\nOther tests exist for different situations\n\n10.6.1 Example Using Regression\n1. Start with a research question\nDo past election results help explain future election results?\n2. Develop a theory of how the world works\nA third-party candidate’s performance in one election will help us predict the success of a future third-party candidate.\n3. Construct “null” and “alternative” hypotheses and 4. Carry out a test of the hypothesis, such as a regression.\nIn a regression, our key hypothesis test is about whether there is a significant, non-zero relationship between an independent variable and the outcome.\n\nThe null hypothesis is that \\(\\beta = 0\\). The alternative is that \\(\\beta \\neq 0\\).\n\nGoing back to our Florida example: The null hypothesis would be the 1996 Perot vote does not help us explain the Buchanan 2000 vote (that \\(\\beta = 0\\)). Our alternative is \\(\\beta \\neq 0\\), that there is some relationship between 1996 Perot votes and the 2000 Buchanan vote in Florida counties.\n\nflorida &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv\")\n\nWe estimate \\(\\hat \\beta\\): for every 1 additional vote Perot received in 1996, we expect Buchanan to receive .036 additional votes in 2000.\n\nfit &lt;- lm(Buchanan00 ~ Perot96, data = florida)\ncoef(fit)\n\n(Intercept)     Perot96 \n 1.34575212  0.03591504 \n\n\nIs that relationship significant? In other words, is it helpful to know the Perot 1996 vote to help explain the Buchanan 2000 vote? Or should we treat the 0.036 number as essentially 0, just noise?\nRecall that the \\(\\hat \\beta\\) represents the estimated slope of the relationship.\n\n\n\n\n\nWe ask: Is that relationship (the slope) significant (i.e., statistically different from 0 slope)?\n5. Calculate the uncertainty around this estimate.\nOur regression lm function will also generate estimates of uncertainty related to hypothesis testing\n\nround(summary(fit)$coefficients, digits=4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.3458    49.7593  0.0270   0.9785\nPerot96       0.0359     0.0043  8.2752   0.0000\n\n\n6. Decide whether you can reject or fail to reject the hypothesis of no difference\nWe see the p-value for Perot96 is essentially 0– well less than 0.05. Therefore, it is highly unlikely we would observe a slope as big or bigger (in magnitude) as 0.0359 if Perot96 and Buchanan00 were actually unrelated.\n\nWe consider the effect “statistically significant.”\nWe reject a null hypothesis that the Perot 1996 vote is unrelated to the Buchanan 2000 vote.\n\nIn social science papers, regressions are often presented in tables:\n\n\n\n=======================\n             Model 1   \n-----------------------\n(Intercept)    1.35    \n             (49.76)   \nPerot96        0.04 ***\n              (0.00)   \n-----------------------\nR^2            0.51    \nAdj. R^2       0.51    \nNum. obs.     67       \n=======================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\n\n10.6.2 Example Using prop.test()\nSometimes our outcomes are 0 vs. 1 variables, which become proportions when we take the mean()\n\nFor example, applicants who get a call back vs. do not\nFor example, voters who turn out to vote vs. do not\n\nWhen you have this type of outcome variable, you may want to use a test designed specifically for testing the differences in proportions of two groups.\nExperimental Example: Going Back to Resume and Race study\n1. Start with a research question\nDoes race influence hiring decisions?\n2. Develop a theory of how the world works\nTheory: Black applicants face discrimination in hiring.\n3. Construct “null” and “alternative” hypotheses\nWe can conduct a two-sided hypothesis test\n\n\\(H_o\\): No difference in call back rates for Black and white applicants\n\\(H_a\\): Some difference in call back rates for Black and white applicants\n\nThe two-sided means we aren’t specifying a direction of our alternative hypothesis. Instead, we are conducting test just trying to reject the idea of no difference between racial groups. Sometimes researchers may specify the alternative hypothesis in a directional way, such as Black applicants will have a lower call back rate than white applicants. However, it is more common to use a two-sided test, even if researchers have a theoretical hypothesis in a particular direction.\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\n\nhead(resume)\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n5    Carrie female white    0\n6       Jay   male white    0\n\n\n4. Carry out a test of the hypothesis, such as a test of proportions.\nDoes being black (vs. white) decrease call backs?\n\ntable(resume$race, resume$call)\n\n       \n           0    1\n  black 2278  157\n  white 2200  235\n\ntest &lt;- prop.test(x=c(157, 235), n=c(157+2278, 235+2200))\n\n\nThe prop.test() function includes 4 inputs\nThe number of 1’s from the first group, the number of 1’s in the second group c(n1, n2)\nThe total observations from the first group, the total observations in the second group c(total1, total2)\n\n5. Calculate the uncertainty around this estimate.\n\ntest\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(157, 235) out of c(157 + 2278, 235 + 2200)\nX-squared = 16.449, df = 1, p-value = 4.998e-05\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.04769866 -0.01636705\nsample estimates:\n    prop 1     prop 2 \n0.06447639 0.09650924 \n\n\nYou can extract the p.value for the difference in proportions and confidence interval\n6. Decide whether you can reject or fail to reject the hypothesis of no difference\nIn the prop.test function, you see a X-squared statistic. You can treat this like the z-score. When you are doing a test of two groups, the X-squared is essentially the z-score squared. The X-squared refers to a Chi-squared test. In our case, the equivalent z-score would be about 4– well more than our threshold of about 2.\n\nround(test$p.value, digits=3)\n\n[1] 0\n\n\nWhat do you conclude about the hypothesis? Is it significant?"
  },
  {
    "objectID": "10-Uncertainty.html#in-class-exercise-questions",
    "href": "10-Uncertainty.html#in-class-exercise-questions",
    "title": "10  Uncertainty",
    "section": "10.7 In-Class Exercise Questions",
    "text": "10.7 In-Class Exercise Questions\nDo negative ads increase voter turnout? In an experiment, suppose researchers found that a negative political ad intending to make voters angry toward the opposing candidate increased voter turnout by 5 percentage points relative to a control condition in which a non-political ad was shown. The difference between treatment and control had a p-value of 0.25, and the confidence interval was -1 to 11 percentage points.\n\nWhat are the implied null and alternative hypotheses?\nWhat is the difference in voter turnout between the groups?\nWhat do you conclude about the results? Is it significant? Do you reject or fail to reject the null hypothesis?\nHow do you interpret the p-value?\n\nIf we imagine that we could conduct the negative advertising study over and over again, estimating voter turnout each time. The set of estimates of voter turnout across these hypothetical studies would be called the (circle one):\n\nStandard error\nSampling distribution\nConfidence Interval\nStandard deviation\n\nAre natural resources like oil negatively related to how democratic a country is? In a study, researchers looked at the correlation between the amount of oil available to a country and how democratic the country is according to expert ratings of democracy. The researchers ran a regression analysis and found that for every one-unit increase in the amount of oil a country has, the country has a b=.3 decrease in its democracy rating. The standard error of the coefficient is se = 0.05. The t-statistic is 6, with a p-value &lt; 0.001.\n\nWhat are the implied null and alternative hypotheses?\nWhat do you conclude about the results? Is it significant? Do you reject or fail to reject the null hypothesis?"
  },
  {
    "objectID": "11-TextasData.html#why-text",
    "href": "11-TextasData.html#why-text",
    "title": "11  Text as Data",
    "section": "11.1 Why text?",
    "text": "11.1 Why text?\nWords (can) matter. Patterns of word usage can be suggestive of deeper divides.\n\nArticle from Deadspin\n\nArticle from NY Times\nWhy Use R to analyze text?\n\nAssist in reading large amounts of text\n\n \n\nEfficiently summarize text through quantifying text attributes\n(Can) remove some subjectivity in coding text, allow to discover aspects of text unknown a priori"
  },
  {
    "objectID": "11-TextasData.html#r-packages-for-text",
    "href": "11-TextasData.html#r-packages-for-text",
    "title": "11  Text as Data",
    "section": "11.2 R Packages for text",
    "text": "11.2 R Packages for text\nPackages are like apps on your phone. They give you additional functionality. To use the tools in a package you first have to install it.\n\ninstall.packages(\"sotu\", dependencies = T)\ninstall.packages(\"tm\", dependencies = T)\ninstall.packages(\"SnowballC\", dependencies = T)\ninstall.packages(\"wordcloud\", dependencies = T)\ninstall.packages(\"stringr\", dependencies = T)\n\nAfter you install it, just like on a phone, anytime you want to use the app, you need to open it. In R, we do that with library().\n\nlibrary(sotu)\nlibrary(tm)\nlibrary(SnowballC)\nlibrary(wordcloud)\nlibrary(stringr)"
  },
  {
    "objectID": "11-TextasData.html#application-state-of-the-union",
    "href": "11-TextasData.html#application-state-of-the-union",
    "title": "11  Text as Data",
    "section": "11.3 Application: State of the Union",
    "text": "11.3 Application: State of the Union\nFor a video explainer of the code for the State of the Union application on pre-processing text and dictionary analysis, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nThe sotu package includes a dataset with the text of every U.S. State of the Union speech. It also includes second dataset with information about the speech. When datasets are stored in a package, you can add them to your environment through the data() function.\n\ndata(sotu_meta)\ndata(sotu_text)\n\nWe are going to “bind” these together into a new dataframe. That way, the sotu_text is a variable inside of our speeches dataframe.\n\nspeeches &lt;- cbind(sotu_meta, sotu_text)\nnames(speeches)\n\n[1] \"X\"            \"president\"    \"year\"         \"years_active\" \"party\"       \n[6] \"sotu_type\"    \"sotu_text\"   \n\n\n\n11.3.1 Cleaning Text\nNote that when working with raw text data, we usually do want our variables to be character variables and not factor variables. Here, every cell is not a category. Instead, it is a speech!\n\nclass(speeches$sotu_text)\n\n[1] \"character\"\n\n\nText is messy data. We may want to spruce it up a bit by removing some of the non-essential characters and words, and moving everything to lowercase.\n\n## Example of speech\nspeeches$sotu_text[1]\n\n[1] \"Fellow-Citizens of the Senate and House of Representatives: \\n\\nI embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity.\\n\\nIn resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom.\\n\\nAmong the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace.\\n\\nA free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies.\\n\\nThe proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy.\\n\\nThere was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors.\\n\\nThe interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs.\\n\\nVarious considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization.\\n\\nUniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.\\n\\nThe advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads.\\n\\nNor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential.\\n\\nTo the security of a free constitution it contributes in various ways - by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness - cherishing the first, avoiding the last - and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws.\\n\\nWhether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature.\\n\\nGentlemen of the House of Representatives: \\n\\nI saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature.\\n\\nIt would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration. \\n\\nGentlemen of the Senate and House of Representatives: \\n\\nI have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford.\\n\\nThe welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government. GEORGE WASHINGTON\\n\"\n\n\n\n## clean text\nspeeches$sotu_text &lt;- tolower(speeches$sotu_text)\nspeeches$sotu_text &lt;- stripWhitespace(speeches$sotu_text)\nspeeches$sotu_text &lt;- removeWords(speeches$sotu_text, stopwords(kind=\"en\"))\nspeeches$sotu_text &lt;- removePunctuation(speeches$sotu_text)\nspeeches$sotu_text &lt;- removeNumbers(speeches$sotu_text)\n#speeches$sotu_text &lt;- stemDocument(speeches$sotu_text) # we will hold off\n\nNote: What you might consider non-essential could differ depending on your application. Maybe you want to keep numbers in your text, for example.\n\n\n11.3.2 Preparing a Corpus\n\n## turn text into corpus\nsotu.corpus &lt;- VCorpus(VectorSource(speeches$sotu_text))\n\n## Add meta data into corpus\nmeta(sotu.corpus, tag= names(sotu_meta), type=\"indexed\") &lt;- sotu_meta\nmeta(sotu.corpus)\n\n      X             president year years_active                 party sotu_type\n1     1     George Washington 1790    1789-1793           Nonpartisan    speech\n2     2     George Washington 1790    1789-1793           Nonpartisan    speech\n3     3     George Washington 1791    1789-1793           Nonpartisan    speech\n4     4     George Washington 1792    1789-1793           Nonpartisan    speech\n5     5     George Washington 1793    1793-1797           Nonpartisan    speech\n6     6     George Washington 1794    1793-1797           Nonpartisan    speech\n7     7     George Washington 1795    1793-1797           Nonpartisan    speech\n8     8     George Washington 1796    1793-1797           Nonpartisan    speech\n9     9            John Adams 1797    1797-1801            Federalist    speech\n10   10            John Adams 1798    1797-1801            Federalist    speech\n11   11            John Adams 1799    1797-1801            Federalist    speech\n12   12            John Adams 1800    1797-1801            Federalist    speech\n13   13      Thomas Jefferson 1801    1801-1805 Democratic-Republican   written\n14   14      Thomas Jefferson 1802    1801-1805 Democratic-Republican   written\n15   15      Thomas Jefferson 1803    1801-1805 Democratic-Republican   written\n16   16      Thomas Jefferson 1804    1801-1805 Democratic-Republican   written\n17   17      Thomas Jefferson 1805    1805-1809 Democratic-Republican   written\n18   18      Thomas Jefferson 1806    1805-1809 Democratic-Republican   written\n19   19      Thomas Jefferson 1807    1805-1809 Democratic-Republican   written\n20   20      Thomas Jefferson 1808    1805-1809 Democratic-Republican   written\n21   21         James Madison 1809    1809-1813 Democratic-Republican   written\n22   22         James Madison 1810    1809-1813 Democratic-Republican   written\n23   23         James Madison 1811    1809-1813 Democratic-Republican   written\n24   24         James Madison 1812    1809-1813 Democratic-Republican   written\n25   25         James Madison 1813    1813-1817 Democratic-Republican   written\n26   26         James Madison 1814    1813-1817 Democratic-Republican   written\n27   27         James Madison 1815    1813-1817 Democratic-Republican   written\n28   28         James Madison 1816    1813-1817 Democratic-Republican   written\n29   29          James Monroe 1817    1817-1821 Democratic-Republican   written\n30   30          James Monroe 1818    1817-1821 Democratic-Republican   written\n31   31          James Monroe 1819    1817-1821 Democratic-Republican   written\n32   32          James Monroe 1820    1817-1821 Democratic-Republican   written\n33   33          James Monroe 1821    1821-1825 Democratic-Republican   written\n34   34          James Monroe 1822    1821-1825 Democratic-Republican   written\n35   35          James Monroe 1823    1821-1825 Democratic-Republican   written\n36   36          James Monroe 1824    1821-1825 Democratic-Republican   written\n37   37     John Quincy Adams 1825    1825-1829 Democratic-Republican   written\n38   38     John Quincy Adams 1826    1825-1829 Democratic-Republican   written\n39   39     John Quincy Adams 1827    1825-1829 Democratic-Republican   written\n40   40     John Quincy Adams 1828    1825-1829 Democratic-Republican   written\n41   41        Andrew Jackson 1829    1829-1833            Democratic   written\n42   42        Andrew Jackson 1830    1829-1833            Democratic   written\n43   43        Andrew Jackson 1831    1829-1833            Democratic   written\n44   44        Andrew Jackson 1832    1829-1833            Democratic   written\n45   45        Andrew Jackson 1833    1833-1837            Democratic   written\n46   46        Andrew Jackson 1834    1833-1837            Democratic   written\n47   47        Andrew Jackson 1835    1833-1837            Democratic   written\n48   48        Andrew Jackson 1836    1833-1837            Democratic   written\n49   49      Martin Van Buren 1837    1837-1841            Democratic   written\n50   50      Martin Van Buren 1838    1837-1841            Democratic   written\n51   51      Martin Van Buren 1839    1837-1841            Democratic   written\n52   52      Martin Van Buren 1840    1837-1841            Democratic   written\n53   53            John Tyler 1841    1841-1845                  Whig   written\n54   54            John Tyler 1842    1841-1845                  Whig   written\n55   55            John Tyler 1843    1841-1845                  Whig   written\n56   56            John Tyler 1844    1841-1845                  Whig   written\n57   57         James K. Polk 1845    1845-1849            Democratic   written\n58   58         James K. Polk 1846    1845-1849            Democratic   written\n59   59         James K. Polk 1847    1845-1849            Democratic   written\n60   60         James K. Polk 1848    1845-1849            Democratic   written\n61   61        Zachary Taylor 1849    1849-1850                  Whig   written\n62   62      Millard Fillmore 1850    1850-1853                  Whig   written\n63   63      Millard Fillmore 1851    1850-1853                  Whig   written\n64   64      Millard Fillmore 1852    1850-1853                  Whig   written\n65   65       Franklin Pierce 1853    1853-1857            Democratic   written\n66   66       Franklin Pierce 1854    1853-1857            Democratic   written\n67   67       Franklin Pierce 1855    1853-1857            Democratic   written\n68   68       Franklin Pierce 1856    1853-1857            Democratic   written\n69   69        James Buchanan 1857    1857-1861            Democratic   written\n70   70        James Buchanan 1858    1857-1861            Democratic   written\n71   71        James Buchanan 1859    1857-1861            Democratic   written\n72   72        James Buchanan 1860    1857-1861            Democratic   written\n73   73       Abraham Lincoln 1861    1861-1865            Republican   written\n74   74       Abraham Lincoln 1862    1861-1865            Republican   written\n75   75       Abraham Lincoln 1863    1861-1865            Republican   written\n76   76       Abraham Lincoln 1864    1861-1865            Republican   written\n77   77        Andrew Johnson 1865    1865-1869            Republican   written\n78   78        Andrew Johnson 1866    1865-1869            Republican   written\n79   79        Andrew Johnson 1867    1865-1869            Republican   written\n80   80        Andrew Johnson 1868    1865-1869            Republican   written\n81   81      Ulysses S. Grant 1869    1869-1873            Republican   written\n82   82      Ulysses S. Grant 1870    1869-1873            Republican   written\n83   83      Ulysses S. Grant 1871    1869-1873            Republican   written\n84   84      Ulysses S. Grant 1872    1869-1873            Republican   written\n85   85      Ulysses S. Grant 1873    1873-1877            Republican   written\n86   86      Ulysses S. Grant 1874    1873-1877            Republican   written\n87   87      Ulysses S. Grant 1875    1873-1877            Republican   written\n88   88      Ulysses S. Grant 1876    1873-1877            Republican   written\n89   89   Rutherford B. Hayes 1877    1877-1881            Republican   written\n90   90   Rutherford B. Hayes 1878    1877-1881            Republican   written\n91   91   Rutherford B. Hayes 1879    1877-1881            Republican   written\n92   92   Rutherford B. Hayes 1880    1877-1881            Republican   written\n93   93     Chester A. Arthur 1881    1881-1885            Republican   written\n94   94     Chester A. Arthur 1882    1881-1885            Republican   written\n95   95     Chester A. Arthur 1883    1881-1885            Republican   written\n96   96     Chester A. Arthur 1884    1881-1885            Republican   written\n97   97      Grover Cleveland 1885    1885-1889            Democratic   written\n98   98      Grover Cleveland 1886    1885-1889            Democratic   written\n99   99      Grover Cleveland 1887    1885-1889            Democratic   written\n100 100      Grover Cleveland 1888    1885-1889            Democratic   written\n101 101     Benjamin Harrison 1889    1889-1893            Republican   written\n102 102     Benjamin Harrison 1890    1889-1893            Republican   written\n103 103     Benjamin Harrison 1891    1889-1893            Republican   written\n104 104     Benjamin Harrison 1892    1889-1893            Republican   written\n105 105      Grover Cleveland 1893    1893-1897            Democratic   written\n106 106      Grover Cleveland 1894    1893-1897            Democratic   written\n107 107      Grover Cleveland 1895    1893-1897            Democratic   written\n108 108      Grover Cleveland 1896    1893-1897            Democratic   written\n109 109      William McKinley 1897    1897-1901            Republican   written\n110 110      William McKinley 1898    1897-1901            Republican   written\n111 111      William McKinley 1899    1897-1901            Republican   written\n112 112      William McKinley 1900    1897-1901            Republican   written\n113 113    Theodore Roosevelt 1901    1901-1905            Republican   written\n114 114    Theodore Roosevelt 1902    1901-1905            Republican   written\n115 115    Theodore Roosevelt 1903    1901-1905            Republican   written\n116 116    Theodore Roosevelt 1904    1901-1905            Republican   written\n117 117    Theodore Roosevelt 1905    1905-1909            Republican   written\n118 118    Theodore Roosevelt 1906    1905-1909            Republican   written\n119 119    Theodore Roosevelt 1907    1905-1909            Republican   written\n120 120    Theodore Roosevelt 1908    1905-1909            Republican   written\n121 121   William Howard Taft 1909    1909-1913            Republican   written\n122 122   William Howard Taft 1910    1909-1913            Republican   written\n123 123   William Howard Taft 1911    1909-1913            Republican   written\n124 124   William Howard Taft 1912    1909-1913            Republican   written\n125 125        Woodrow Wilson 1913    1913-1917            Democratic    speech\n126 126        Woodrow Wilson 1914    1913-1917            Democratic    speech\n127 127        Woodrow Wilson 1915    1913-1917            Democratic    speech\n128 128        Woodrow Wilson 1916    1913-1917            Democratic    speech\n129 129        Woodrow Wilson 1917    1917-1921            Democratic    speech\n130 130        Woodrow Wilson 1918    1917-1921            Democratic    speech\n131 131        Woodrow Wilson 1919    1917-1921            Democratic   written\n132 132        Woodrow Wilson 1920    1917-1921            Democratic   written\n133 133     Warren G. Harding 1921    1921-1923            Republican    speech\n134 134     Warren G. Harding 1922    1921-1923            Republican    speech\n135 135       Calvin Coolidge 1923    1923-1925            Republican    speech\n136 136       Calvin Coolidge 1924    1923-1925            Republican   written\n137 137       Calvin Coolidge 1925    1925-1929            Republican   written\n138 138       Calvin Coolidge 1926    1925-1929            Republican   written\n139 139       Calvin Coolidge 1927    1925-1929            Republican   written\n140 140       Calvin Coolidge 1928    1925-1929            Republican   written\n141 141        Herbert Hoover 1929    1929-1933            Republican   written\n142 142        Herbert Hoover 1930    1929-1933            Republican   written\n143 143        Herbert Hoover 1931    1929-1933            Republican   written\n144 144        Herbert Hoover 1932    1929-1933            Republican   written\n145 145 Franklin D. Roosevelt 1934    1933-1937            Democratic    speech\n146 146 Franklin D. Roosevelt 1935    1933-1937            Democratic    speech\n147 147 Franklin D. Roosevelt 1936    1933-1937            Democratic    speech\n148 148 Franklin D. Roosevelt 1937    1937-1941            Democratic    speech\n149 149 Franklin D. Roosevelt 1938    1937-1941            Democratic    speech\n150 150 Franklin D. Roosevelt 1939    1937-1941            Democratic    speech\n151 151 Franklin D. Roosevelt 1940    1937-1941            Democratic    speech\n152 152 Franklin D. Roosevelt 1941    1941-1945            Democratic    speech\n153 153 Franklin D. Roosevelt 1942    1941-1945            Democratic    speech\n154 154 Franklin D. Roosevelt 1943    1941-1945            Democratic    speech\n155 155 Franklin D. Roosevelt 1944    1941-1945            Democratic    speech\n156 156 Franklin D. Roosevelt 1945         1945            Democratic    speech\n157 157 Franklin D. Roosevelt 1945         1945            Democratic   written\n158 158        Harry S Truman 1946    1945-1949            Democratic   written\n159 159        Harry S Truman 1947    1945-1949            Democratic    speech\n160 160        Harry S Truman 1948    1945-1949            Democratic    speech\n161 161        Harry S Truman 1949    1949-1953            Democratic    speech\n162 162        Harry S Truman 1950    1949-1953            Democratic    speech\n163 163        Harry S Truman 1951    1949-1953            Democratic    speech\n164 164        Harry S Truman 1952    1949-1953            Democratic    speech\n165 165  Dwight D. Eisenhower 1953    1953-1957            Republican   written\n166 166        Harry S Truman 1953    1949-1953            Democratic   written\n167 167  Dwight D. Eisenhower 1954    1953-1957            Republican    speech\n168 168  Dwight D. Eisenhower 1955    1953-1957            Republican    speech\n169 169  Dwight D. Eisenhower 1956    1953-1957            Republican    speech\n170 170  Dwight D. Eisenhower 1956    1953-1957            Republican   written\n171 171  Dwight D. Eisenhower 1957    1957-1961            Republican    speech\n172 172  Dwight D. Eisenhower 1958    1957-1961            Republican    speech\n173 173  Dwight D. Eisenhower 1959    1957-1961            Republican    speech\n174 174  Dwight D. Eisenhower 1960    1957-1961            Republican    speech\n175 175       John F. Kennedy 1961    1961-1963            Democratic   written\n176 176  Dwight D. Eisenhower 1961    1957-1961            Republican   written\n177 177       John F. Kennedy 1962    1961-1963            Democratic    speech\n178 178       John F. Kennedy 1963    1961-1963            Democratic    speech\n179 179     Lyndon B. Johnson 1964    1964-1965            Democratic    speech\n180 180     Lyndon B. Johnson 1965    1965-1969            Democratic    speech\n181 181     Lyndon B. Johnson 1966    1965-1969            Democratic    speech\n182 182     Lyndon B. Johnson 1967    1965-1969            Democratic    speech\n183 183     Lyndon B. Johnson 1968    1965-1969            Democratic    speech\n184 184     Lyndon B. Johnson 1969    1965-1969            Democratic   written\n185 185      Richard M. Nixon 1970    1969-1973            Republican    speech\n186 186      Richard M. Nixon 1971    1969-1973            Republican    speech\n187 187      Richard M. Nixon 1972    1969-1973            Republican    speech\n188 188      Richard M. Nixon 1972    1969-1973            Republican   written\n189 189      Richard M. Nixon 1974    1973-1974            Republican    speech\n190 190      Richard M. Nixon 1974    1973-1974            Republican   written\n191 191        Gerald R. Ford 1975    1974-1977            Republican    speech\n192 192        Gerald R. Ford 1976    1974-1977            Republican    speech\n193 193        Gerald R. Ford 1977    1974-1977            Republican   written\n194 194          Jimmy Carter 1978    1977-1981            Democratic    speech\n195 195          Jimmy Carter 1978    1977-1981            Democratic   written\n196 196          Jimmy Carter 1979    1977-1981            Democratic    speech\n197 197          Jimmy Carter 1979    1977-1981            Democratic   written\n198 198          Jimmy Carter 1980    1977-1981            Democratic    speech\n199 199          Jimmy Carter 1980    1977-1981            Democratic   written\n200 200         Ronald Reagan 1981    1981-1985            Republican    speech\n201 201          Jimmy Carter 1981    1977-1981            Democratic   written\n202 202         Ronald Reagan 1982    1981-1985            Republican    speech\n203 203         Ronald Reagan 1983    1981-1985            Republican    speech\n204 204         Ronald Reagan 1984    1981-1985            Republican    speech\n205 205         Ronald Reagan 1985    1985-1989            Republican    speech\n206 206         Ronald Reagan 1986    1985-1989            Republican    speech\n207 207         Ronald Reagan 1987    1985-1989            Republican    speech\n208 208         Ronald Reagan 1988    1985-1989            Republican    speech\n209 209           George Bush 1989    1989-1993            Republican    speech\n210 210           George Bush 1990    1989-1993            Republican    speech\n211 211           George Bush 1991    1989-1993            Republican    speech\n212 212           George Bush 1992    1989-1993            Republican    speech\n213 213    William J. Clinton 1993    1993-1997            Democratic    speech\n214 214    William J. Clinton 1994    1993-1997            Democratic    speech\n215 215    William J. Clinton 1995    1993-1997            Democratic    speech\n216 216    William J. Clinton 1996    1993-1997            Democratic    speech\n217 217    William J. Clinton 1997    1997-2001            Democratic    speech\n218 218    William J. Clinton 1998    1997-2001            Democratic    speech\n219 219    William J. Clinton 1999    1997-2001            Democratic    speech\n220 220    William J. Clinton 2000    1997-2001            Democratic    speech\n221 221        George W. Bush 2001    2001-2005            Republican    speech\n222 222        George W. Bush 2002    2001-2005            Republican    speech\n223 223        George W. Bush 2003    2001-2005            Republican    speech\n224 224        George W. Bush 2004    2001-2005            Republican    speech\n225 225        George W. Bush 2005    2005-2009            Republican    speech\n226 226        George W. Bush 2006    2005-2009            Republican    speech\n227 227        George W. Bush 2007    2005-2009            Republican    speech\n228 228        George W. Bush 2008    2005-2009            Republican    speech\n229 229          Barack Obama 2009    2009-2013            Democratic    speech\n230 230          Barack Obama 2010    2009-2013            Democratic    speech\n231 231          Barack Obama 2011    2009-2013            Democratic    speech\n232 232          Barack Obama 2012    2009-2013            Democratic    speech\n233 233          Barack Obama 2013    2013-2016            Democratic    speech\n234 234          Barack Obama 2014    2013-2016            Democratic    speech\n235 235          Barack Obama 2015    2013-2016            Democratic    speech\n236 236          Barack Obama 2016    2013-2016            Democratic    speech\n237 237          Donald Trump 2017    2016-2020            Republican    speech\n238 238          Donald Trump 2018    2016-2020            Republican    speech\n239 239          Donald Trump 2019    2016-2020            Republican    speech\n240 240          Donald Trump 2020    2016-2020            Republican    speech\n\n## turn into Document-Term-Matrix\nsotu.dtm &lt;- DocumentTermMatrix(sotu.corpus)\n\n\n## preview\ninspect(sotu.dtm[,10:20])\n\n&lt;&lt;DocumentTermMatrix (documents: 240, terms: 11)&gt;&gt;\nNon-/sparse entries: 53/2587\nSparsity           : 98%\nMaximal term length: 12\nWeighting          : term frequency (tf)\nSample             :\n     Terms\nDocs  abandonment abandons abate abated abatement abating abbas abbreviation\n  106           2        0     0      0         0       0     0            0\n  108           1        1     0      0         0       0     0            0\n  112           0        0     1      1         1       0     0            0\n  119           2        0     0      0         0       0     0            0\n  147           0        0     0      0         0       0     0            0\n  195           0        0     1      0         2       0     0            0\n  201           2        0     1      0         0       0     0            0\n  53            0        0     0      0         2       0     0            0\n  66            2        0     0      0         0       0     0            0\n  94            1        0     0      0         1       0     0            0\n     Terms\nDocs  abdicated abdicating\n  106         0          0\n  108         0          0\n  112         0          0\n  119         0          1\n  147         2          0\n  195         0          0\n  201         0          0\n  53          0          0\n  66          0          0\n  94          0          0\n\n\n\n\n11.3.3 Word Frequency\nConvert the “Document-Term-Matrix” into a matrix using as.matrix()\n\nsotu.dtm.mat &lt;- as.matrix(sotu.dtm)\n\n## Most frequent words\nhead(sort(sotu.dtm.mat[1,], decreasing=T), n=10)\n\n      will        may     public    country        end government      great \n        14          5          5          4          4          4          4 \n  measures     regard     states \n         4          4          4 \n\nhead(sort(sotu.dtm.mat[236,], decreasing=T), n=10)\n\n america      now   people     will     just american     work    world \n      28       27       27       26       25       22       22       22 \n    make      can \n      20       19 \n\n\nNote: these are somewhat generic words.\nWord Cloud\n\nwordcloud(words=names(sotu.dtm.mat[1,]),\n          freq=sotu.dtm.mat[1,], max.words = 20)"
  },
  {
    "objectID": "11-TextasData.html#word-importance",
    "href": "11-TextasData.html#word-importance",
    "title": "11  Text as Data",
    "section": "11.4 Word Importance",
    "text": "11.4 Word Importance\nWe use tf-idf (term frequency - inverse document frequency) as a way to pull out uniquely important/relevant words for a given character.\n\nRelative frequency of a term inversely weighted by the number of documents in which the term appears.\nFunctionally, if everyone uses the word “know,” then it’s not very important for distinguishing characters/documents from each other.\nWe want words that a speech used frequently, that other speeches use less frequently\n\n\n## words uniquely important to a character\nsotu.tfidf &lt;- weightTfIdf(sotu.dtm)\n\n## convert to matrix\nsotu.tfidf.mat &lt;- as.matrix(sotu.tfidf)\n\nWe can summarize the uniquely relevant words for each speech\n\nGw1790.tfidf &lt;-head(sort(sotu.tfidf.mat[1,], decreasing=T), n=8)\nBO2016.tfidf &lt;-head(sort(sotu.tfidf.mat[236,], decreasing=T), n=8)\n\n\nGw1790.tfidf\n\n    intimating licentiousness        discern     inviolable         derive \n    0.01532343     0.01532343     0.01338545     0.01225180     0.01181748 \n     persuaded     cherishing  comprehending \n    0.01181748     0.01082357     0.01082357 \n\n\n\nbarplot(Gw1790.tfidf, cex.axis=.7,\n         cex.names=.7,\n        main= \"Most `Important' 1790 SOTU Words (tf-idf)\", \n        horiz = T, las=2)\n\n\n\nbarplot(BO2016.tfidf,\n         cex.names=.7, cex.axis=.7,\n        main= \"Most `Important' 2016 SOTU Words (tf-idf)\", \n        horiz=T, las=2)"
  },
  {
    "objectID": "11-TextasData.html#additional-descriptive-statistics",
    "href": "11-TextasData.html#additional-descriptive-statistics",
    "title": "11  Text as Data",
    "section": "11.5 Additional Descriptive Statistics",
    "text": "11.5 Additional Descriptive Statistics\nAre the length of speeches changing? The nchar() function tells you the number of characters in a “string.”\n\nspeeches$speechlength &lt;- nchar(speeches$sotu_text)\n\nLet’s plot the length of speeches over time and annotate with informative colors and labels.\nIs the length of speeches changing?\n\nplot(x=1:length(speeches$speechlength), y= speeches$speechlength, \n    pch=15,\n     xaxt=\"n\",\n     xlab=\"\", \n     ylab = \"Number of Characters\")\n\n## add x axis\naxis(1, 1:length(speeches$speechlength), labels=speeches$year, las=3, cex.axis=.7)\n\n\n\n\nWe can add color to distinguish written vs. spoken speeches\n\nspeechcolor &lt;- ifelse(speeches$sotu_type == \"written\", \"black\", \"green3\")\nplot(x=1:length(speeches$speechlength), y= speeches$speechlength, \n     xaxt=\"n\", pch=15,\n     xlab=\"\", \n     ylab = \"Number of Characters\",\n     col = speechcolor)\n\n## add x axis\naxis(1, 1:length(speeches$speechlength), labels=speeches$year, las=3, cex.axis=.7)\n\n## add legend\nlegend(\"topleft\", c(\"spoken\", \"written\"), \n       pch=15, \n       col=c(\"green3\", \"black\"), bty=\"n\")\n\n\n\n\n\n11.5.1 Dictionary Analysis\nWe can characterize the content of speeches in different ways. For example, we can see if speeches mention specific words, such as `“terrorism.”\n\nThe function grepl() lets you search for a pattern of text in a character string\nThe function str_detect() works similarly with the opposite order of inputs\n\n\nspeeches$terrorism &lt;- ifelse(grepl(\"terror\", speeches$sotu_text), 1,0)\nspeeches$terrorism2 &lt;- ifelse(str_detect(speeches$sotu_text,\"terror\"), 1,0)\n\n\nsort(tapply(speeches$terrorism, speeches$president, sum), \n     decreasing=T)[1:10]\n\n       George W. Bush    William J. Clinton          Barack Obama \n                    8                     8                     7 \n        Ronald Reagan          Donald Trump Franklin D. Roosevelt \n                    6                     4                     4 \n       Andrew Jackson     Chester A. Arthur      Grover Cleveland \n                    2                     2                     2 \n       Harry S Truman \n                    2 \n\n\nWe can characterize the content of speeches in different ways. For example, we can see if speeches mention specific words, such as “terrorism.”\n\nThe function str_count() counts the number of times a piece of text appears in a character string\n\n\nspeeches$terrorismcount &lt;- str_count(speeches$sotu_text, \"terror\")\n\n\nsort(tapply(speeches$terrorismcount, speeches$president, sum), \n     decreasing=T)[1:10]\n\n       George W. Bush          Barack Obama    William J. Clinton \n                  171                    37                    29 \n         Donald Trump         Ronald Reagan Franklin D. Roosevelt \n                   24                    10                     6 \n    Lyndon B. Johnson        Harry S Truman          Jimmy Carter \n                    5                     3                     3 \n       Andrew Jackson \n                    2 \n\n\nWe can add multiple words with the | operator. This is often called a “dictionary analysis.”\n\nspeeches$warcount &lt;- str_count(speeches$sotu_text, \n                               \"terror|war|military|drone\")\nsort(tapply(speeches$warcount, speeches$president, sum), decreasing=T)[1:10]\n\n       Harry S Truman    Theodore Roosevelt Franklin D. Roosevelt \n                  554                   481                   441 \n        James K. Polk          Jimmy Carter  Dwight D. Eisenhower \n                  390                   348                   332 \n     William McKinley        George W. Bush      Grover Cleveland \n                  324                   323                   257 \n     Ulysses S. Grant \n                  233 \n\n\nWhat are possible limitations of this analysis?"
  },
  {
    "objectID": "11-TextasData.html#application-programming-interfaces",
    "href": "11-TextasData.html#application-programming-interfaces",
    "title": "11  Text as Data",
    "section": "11.6 Application Programming Interfaces",
    "text": "11.6 Application Programming Interfaces\nApplication programming interfaces (APIs) are tools that allow you to search a large database to extract specific types of information. Social scientists often work with APIs to extract data from social media platforms, government agencies (e.g., U.S. Census), and news sites, among others.\nOrganizations that develop these APIs can control what types of information researchers can access. Often, they set limits on the types and quantities of information someone can collect. Companies also often monitor who accesses the information by requiring people to sign up for access, apply for access, and/or pay for access.\nExample: Census API As an example of an API, the U.S. Census has an API that allows researchers to extract nicely formatted data summaries of different geographic units (e.g., all zip codes in the U.S.).\n\nResearchers can sign up here for an API “key” which allows the organization to monitor who is accessing what information.\n\nResearchers Kyle Walker and Matt Herman have made an R package that makes working with the API easier.\n\nExample: tidycensus found here allows you to search Census data by providing the variables you want to extract\n\n\nAPIs can make a social scientist’s life easier by providing an efficient way to collect data. Without an API, researchers might have to resort to manually extracting information from online or writing an ad hoc set of code to “scrape” the information off of websites. This can be time consuming, against an organization or company’s policy, or even impossible in some cases. APIs are powerful and efficient.\nHowever, because researchers cannot control the API, the downside is at any given time, an organization could change or remove API access. Researchers might also not have the full details of what information is included in the API, potentially leading to biased conclusions from the data. APIs are great, but we should use them with caution."
  },
  {
    "objectID": "11-TextasData.html#the-politics-of-song-choice",
    "href": "11-TextasData.html#the-politics-of-song-choice",
    "title": "11  Text as Data",
    "section": "11.7 The Politics of Song Choice",
    "text": "11.7 The Politics of Song Choice\nWhen deciding to run for office, political candidates often think strategically about how to introduce themselves. In the lead up to the 2024 presidential election in the United States, several Republicans announced their candidacy for the primary nomination.\nAs this article in The Hill notes, oftentimes, the candidate celebrates their announcement with a theme song / walkout music / or common song they bring with them on the campaign trail. Politico went even further to ask candidates to submit their top 20 songs. Only some candidates responded, and in this application, we will analyze the playlists of some of the top candidates who submitted their song choices: Chris Christie, Nikki Haley, and Vivek Ramaswamy.\nWe will analyze some of these songs drawing on the Spotify API.\n\n11.7.1 Setting Up the Spotify API\nIn order to follow along completely with the Spotify portion, you will need 1) a free account on Spotify https://open.spotify.com/, 2) a developer’s app on Spotify, and the 3) spotifyR package installed in RStudio.\nAfter signing up for a free Spotify account, let’s create the developer’s app by\n\ngoing to https://developer.spotify.com/dashboard when you are signed in.\nSelect “Create app”\n\nGive your app a name (can be anything) and description (e.g., For conducting political analysis)\nSet a redirect URI– this won’t matter much for our purposes, so you can use http://localhost:1410/.\nYou can leave “website” blank\nMark the check box for Web API\n\n\n\nAfter “saving” the information, click on the “Settings” for the app, where you can view your Client ID and a button called, “View client secret.” We will use these in a moment. Note: do not share these with anyone. Treat these like passwords.\nTo R we go! We will access the Spotify API through an R package spotifyr. The first time you use this package, you will need to install it.\n\ninstall.packages('spotifyr', dependencies = TRUE)\n\nEvery other time, you will need to use the following code:\n\nlibrary(spotifyr)\n\nNow, we need to “authenticate” our connection with Spotify using our Client ID and Client Secret credentials. Replace the xxxxxxx’s below with your own credentials and generate the access_token which will be stored in your RStudio environment.\n\nSys.setenv(SPOTIFY_CLIENT_ID ='xxxxxxxxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxx')\nauth_object &lt;- get_spotify_authorization_code(scope = scopes()[c(7,8,9,10,14,15)])\n\nTroubleshooting\n\nSometimes people get an error about not having “httpuv” installed. If that happens to you, you can also run install.packages(\"httpuv\") and then retry using library(spotifyr) and running the setup code.\nIf you get an error that says “cannot find function”, it may mean that spotifyr has not been installed or you have not yet run library(spotifyr). Make sure to run these before using the functions below.\nIf the Sys.setenv functions run properly, the first time you use them, they will likely open up a web browser page related to Spotify, asking you to agree to the terms of the API. Once you agree, it will say “Authentication complete.” If it does not run properly, it may open a web browser page that says “Invalid login” or something like that.To diagnose that error, I recommend doublechecking that you are\n\nSigned into Spotify and the developer’s Spotify page on the web browser opened via the RStudio session\nThat in the Settings page of your Spotify, you have entered the right redirect URI\nThat in RStudio, you have entered the correct ClientID and Client Secret without any typos (no extra spaces or accidental “x” left over from when you pasted it)\nAfter checking these, you can also restart your R session and try again to get a fresh chance of authenticating the API.\n\n\nNow we can use the API! The first time you try to run a function using the API, you might see a message asking you to “cache” your credentials.\nSelect 1 by writing 1 where the cursor is in your bottom-left RStudio Console window and hit enter/return .\n\n\n\n11.7.2 Candidate Danceability and Valence\nWe will retrieve the playlists from the candidates by providing the function get_playlist_audio_features() with the Spotify identifiers for each playlist. We store it in a dataframe object called candidates.\n\ncandidates &lt;- get_playlist_audio_features(username=\"Politico\",\n                            playlist_uris = c(\"26rVnB3MN03kRyXXWwAne0\", \"6gk4Omuze4zSr1G2nK1nQ4\",\"2kGJsgdiexWTVlWnTdGSIi\"))\n\nNote: How do you find the URI if you wanted to on your own? This can be a little tricky. When you are on the web version of Spotify, if you click on the “…” next to the playlist name, artist name, or track name, it provides a menu which includes the “Share” button. By default, the share feature allows you to copy the link to the playlist, a URL. However, this is different from the URI. To get the URI, you can “right-click” on that share button or hold down “control” on a Mac. This will shift it from being the “copy link” to the URI option. See images below to see how holding down “control” after having the menu open shifts the share feature:\n \nLet’s compare the candidates on a few metrics, including danceability (how suitable a track is to dancing from 0 to 1) and valence (musical positiveness from 0 to 1- whether a song is likely to make someone feel happy/cheerful, higher valence, or sad/depressed/angry, lower valence) using a boxplot.\n\nboxplot(danceability~playlist_name, data=candidates, horizontal=TRUE, las=1, \n        names = c(\"Christie\", \"Haley\",\n                                                                                \"Ramaswamy\"), \n        xlab=\"Danceability\", ylab=\"\",\n        cex.axis=.6)\n\n\n\n\nWe can also compare the “valence” of songs by candidate.\n\nboxplot(valence~playlist_name, data=candidates, horizontal=TRUE, las=1, \n        names = c(\"Christie\", \"Haley\",\n                                                                                \"Ramaswamy\"), \n        xlab=\"Valence\", ylab=\"\",\n        cex.axis=.6)\n\n\n\n\nWow, there is one song from Ramaswamy that has particularly low valence. Which song was this, and was it something the candidate emphasized? Yes! His Eminem moment.\n\nvivek &lt;- subset(candidates, playlist_name = \"Vivek Ramaswamy's Top 8 Songs\")\n\nvivek$track.name[vivek$valence == min(vivek$valence)]\n\n[1] \"Lose Yourself\"\n\n\n\n\n\n11.7.3 Additional Tools\nIn addition to analyzing whole playlists, you can also retrieve and analyze specific artists or tracks. Here are a couple examples:\n\n## supply a track URI\nhowdoibreathe_features &lt;- get_track_audio_features(id=\"174rZBKJAqD10VBnOjlQQ3\")\n\n## supply an artist name\nariana &lt;- get_artist_audio_features('ariana grande')\n\n\n\n11.7.4 Saving R Objects\nAfter you extract data from online, you may want to save them as a hard data file on your computer. This way if you close RStudio, you can reproduce the data.\nR allows you to save any R object as an .RData file that can be opened with the load() command. This is discussed on pg. 24 of QSS Chapter 1.\nWe can demonstrate this now by saving candidates as an RData object. It will automatically save to your working directory, but you can also add a subfolder or alternative file path.\n\nsave(candidates, file = \"candidates.RData\")\n\nThen, you can load the file (if you happen to close R/RStudio, restart your computer, etc.) with the load command.\n\nload(\"candidates.RData\")"
  },
  {
    "objectID": "12-Maps.html#why-maps",
    "href": "12-Maps.html#why-maps",
    "title": "12  Mapping",
    "section": "12.1 Why maps",
    "text": "12.1 Why maps\nHow might maps be useful for political scientists? What are examples of questions maps can help answer?\n“The visualization of spatial data through maps enables researchers to discover previously unknown patterns and present their findings in a convincing manner.” - Kosuke Imai, Chap 5 QSS\n\nNational Geographic. Mapping fatal cholera cases helped John Snow uncover the source of a cholera outbreak in London to an infected water pump in 1854.\nWhy are maps used in political science\n\nShow diffusion of a disease, policies, political power\nShow demographic patterns\nExamine clustering, regional patterns of different policies, events, etc.\nShift analysis away from an individual person or political unit to instead think about the broader social and political context\nConvey a lot of information efficiently, and in an engaging way, using intuitive heuristics of commonly known geographic locations\n\nMost maps are also inherently political! (e.g., historical trends in political boundaries, which geographic entities are recognized, etc.)\nMaps themselves may be the subject of interest. For example, we are currently in redistricting to determine the boundaries used for different elections in the U.S.\n\n\n\nWashington Post\n\n\n\n12.1.1 To map or not to map?\nIf I wanted to track COVID vaccinations by state, what are the pros and cons of using a table vs. a map? See this example from the Mayo Clinic.\n \nCan maps be misleading? See this discussion from Kieran Healy in Chap. 7: Each of these maps shows data for the same event, but the impressions they convey are very different … Often, a map is like a weird grid that you are forced to conform to even though you know it systematically misrepresents what you want to show.”\n\n\n\nHealy Chapter 7"
  },
  {
    "objectID": "12-Maps.html#mapping-in-r",
    "href": "12-Maps.html#mapping-in-r",
    "title": "12  Mapping",
    "section": "12.2 Mapping in R",
    "text": "12.2 Mapping in R\nFor a video explainer of the code for the applications with maps and color, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nInstall maps package. You only need to do this one time.\n\ninstall.packages(\"maps\")\n\nAll subsequent times, you just need to use library()\n\nlibrary(maps)\n\nThe map command is like a plot. It maps a particular entry from a database. Below are a few examples of types of maps that come readymade in the package.\n\nmap(database = \"world\")\n\n\n\n\n\nmap(database = \"usa\")\n\n\n\n\n\nmap(database = \"state\")\n\n\n\n\n\nmap(database=\"county\")\n\n\n\n\nYou can also map particular regions within a database.\n\nmap(database = \"state\", regions= c(\"New Jersey\", \"New York\" ))\n\n\n\n\n\nmap(database=\"county\", regions = \"Nebraska\")\n\n\n\n\n\nmap(database = \"world\", regions= \"Italy\")\n\n\n\n\n\n12.2.1 Using ggplot2 with maps\nWe may want to embed map in another plotting device that more easily adds informative labels, colors, and other information.\n\nWhile the plotting tools we have worked with before can do this, the function ggplot has a better interface that will more easily help us avoid mistakes, such as putting a label in the wrong place.\n\n\ninstall.packages(\"ggplot2\")\n\nOpen the packages each time you want to use them.\n\nlibrary(ggplot2)\n\nThe “gg” stands for “grammar of graphics.” The ggplot2 package has a very general function ggplot() that provides another system of visualizing data in R.\n\nIt can be used to plot all kinds of visuals, including scatterplots, barplots, histograms, etc.\nWe are going to focus on its utility for plotting maps, as many new developments in mapping and GIS (geographic information systems) in R use this interface.\nIn ggplot() you add layers to a plot by using + between lines\nWhile ggplot() can be applied very widely, we will focus on a more narrow set of applications for mapping.\n\nWe will create a map of New Jersey. Similar to before, we will first use a function to pull up map data about U.S. states. The map_data function pulls up just the data instead of making the map itself.\n\nnj_map &lt;- map_data(\"state\",regions= c(\"New Jersey\"))\n\nWe also directly integrate the data into the plotting function\n\n## Begin plot\nggplot() + #Note the use of the + sign between each line\n\n  geom_polygon(nj_map, mapping=aes(x=long, y=lat, group=group), \n               colour=\"black\")+\n  \n  ## add title\n  ggtitle(\"Map of New Jersey\")+\n  \n  ## adjust projection\n  coord_quickmap() +\n  \n  ## remove background\n  theme_void() # note: last line does not end with a +"
  },
  {
    "objectID": "12-Maps.html#choropleth-maps",
    "href": "12-Maps.html#choropleth-maps",
    "title": "12  Mapping",
    "section": "12.3 Choropleth Maps",
    "text": "12.3 Choropleth Maps\nSometimes maps use shading of polygons to display quantitative information about a geographic unit or qualitative information about what geographical units belong to different categories of a variable. We provide an example of this here.\n\nWe are going to add a variable to our mapping data that we want to visualize\nWe fill the plot using geom_polygon and can (optionally) indicate specific colors\n\n\nusmap &lt;- map_data(\"state\")\nusmap$nj &lt;- ifelse(usmap$region == \"new jersey\", \"Cannot turn left\", \n                   \"Can turn left\")\nusmap$nj &lt;- as.factor(usmap$nj)\n\nggplot()+\n  ## Note the fill= nj\n  geom_polygon(data=usmap, aes(x=long, y=lat, group=group, fill=nj))+\n  ## we can indicate colors for each category of the nj variable\n  scale_fill_manual(values = c(\"gray\", \"red3\"), name=\"Left Turns\")+\n  theme_void() +\n  ggtitle(\"Geography of Left Turns\")+\n  coord_quickmap()"
  },
  {
    "objectID": "12-Maps.html#application-2021-nj-election-results",
    "href": "12-Maps.html#application-2021-nj-election-results",
    "title": "12  Mapping",
    "section": "12.4 Application: 2021 NJ Election Results",
    "text": "12.4 Application: 2021 NJ Election Results\nWe sometimes get data from an outside source. We then need to figure out how to connect it to the mapping data.\n\nnjcounties &lt;- map_data(\"county\", region=\"New Jersey\")\n\n## 2021 county election results\nmurphyvote &lt;- data.frame(county = unique(njcounties$subregion),\n           murphy = c(43.92, 52.52, 53.28, 61.69, 36.69, 43.64, \n                      73.96, 44.63, 73.56, 40.19, 65.09,\n                      55.74, 40.31, 44.06, 31.79, 51.47, \n                      35.01, 51.54, 31.93, 61.56, 34.56))\n\nWe can use merge() to do so by indicating a shared unique identifier that the dataframes have in common. Note that subregion is the name of the county variable in the first dataframe (the x) and county is the name of the county variable in the second dataframe (the y). For this to work, we had to first make sure the county names are formatted exactly the same way in both dataframes. For example, R won’t known “camden” and “Camden” are the same. They have to exactly match for R to be able to properly join the data together. With messier data, you might have to rename some variable values prior to joining data in a merge, such as by changing the case of letters or adjusting punctuation, etc.\n\nFor more information on merging, see QSS chapter 4.2.5 or this explainer.\n\n\nnjcounties &lt;- merge(njcounties, murphyvote, \n                    by.x=\"subregion\", by.y = \"county\", \n                    all.x=TRUE, all.y=F)\n\nNow that the data are merged, we can add Murphy’s vote share as a color.\n\nggplot()+\n  \n  ## create an nj county-level plot\n  geom_polygon(data=njcounties, aes(x=long, y=lat, \n                                    group=group, \n                                    fill=murphy),\n               colour=\"white\")+\n  ## Shade the map according to the vote share\n  scale_fill_gradient(name=\"Murphy's Vote Share %\", low=\"red\", high=\"blue\")+\n  \n  ## remove background\n  theme_void()+\n  \n  ggtitle(\"2021 NJ Governor Results by County\")+\n  coord_quickmap()"
  },
  {
    "objectID": "12-Maps.html#application-voter-identification-laws",
    "href": "12-Maps.html#application-voter-identification-laws",
    "title": "12  Mapping",
    "section": "12.5 Application: Voter Identification Laws",
    "text": "12.5 Application: Voter Identification Laws\nAccording to the NCSL, 36 states have laws requesting or requiring voters to show some form of identification at the polls.\n\nThe presence of voter ID laws and the strictness of these laws has accelerated over the past decade.\nWe will look at the geography of these laws to see if there are regional or other political patterns to these\n\n\n\n\nNCSL\n\n\n\n12.5.1 Using the %in% function\nDetecting if something is contained within a vector: The function %in% asks: is this contained in the vector? Yes/No\n\n\"new jersey\" %in% c(\"new jersey\", \"california\", \"nebraska\")\n\n[1] TRUE\n\n\"florida\" %in% c(\"new jersey\", \"california\", \"nebraska\")\n\n[1] FALSE\n\n(! \"florida\" %in% c(\"new jersey\", \"california\", \"nebraska\")) # not in\n\n[1] TRUE\n\n\nWe will augment our map data with a new variable that classifies states according to their voter ID laws.\n\nusmap &lt;- map_data(\"state\")\n\nhead(usmap)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n## initialize variable\nusmap$photoidlaws &lt;- NA\n\nusmap$photoidlaws[usmap$region %in% c(\"arkansas\",\"georgia\", \"indiana\", \"kansas\",\n                                      \"mississippi\",\"missouri\", \"north carolina\", \"ohio\", \n                                      \"tennessee\", \"wisconsin\")] &lt;- \"Strict Photo ID\"\n\nusmap$photoidlaws[usmap$region %in% c(\"alabama\", \"florida\", \"idaho\", \"louisiana\", \n                                      \"michigan\", \"montana\", \"rhode island\", \"south carolina\", \n                                      \"south dakota\", \"texas\")] &lt;- \"Non-Strict Photo ID\"\n\nusmap$photoidlaws[usmap$region %in% c(\"arizona\", \"north dakota\", \"wyoming\")]&lt;- \"Strict Non-Photo ID\"\n\nusmap$photoidlaws[usmap$region %in% c(\"alaska\", \"colorado\", \"connecticut\", \"delaware\", \"hawaii\", \n                                      \"iowa\", \"kentucky\", \"new hampshire\", \"oklahoma\", \"utah\", \n                                      \"virginia\", \"washington\", \"west virginia\")] &lt;- \"Non-Strict Non-Photo ID\"\n\nusmap$photoidlaws[usmap$region %in% c(\"oregon\", \"nevada\",\"california\", \"new mexico\", \n                                      \"nebraska\", \"minnesota\", \"illinois\",  \"pennsylvania\", \n                                      \"new york\", \"new jersey\", \"massachusetts\", \"vermont\",\n                                      \"maryland\",  \"district of columbia\", \"maine\")] &lt;- \"No Document Required\"\n\n\n## Make it a factor categorical variable\nusmap$photoidlaws &lt;- as.factor(usmap$photoidlaws)\n\nLet’s create a map of the U.S. We will then annotate the map with information about voter identification laws.\n\nggplot()+\n  geom_polygon(data=usmap, aes(x=long, y=lat, group=group, \n                               fill=photoidlaws),\n               colour=\"black\")+\n  ## palette lets you pick a color scheme without specifics\n  scale_fill_brewer(palette=\"Greens\", name=\"Photo ID Laws\")+\n\n  theme_void() +\n  ggtitle(\"Map of U.S. Voter ID Laws\")+\n  coord_quickmap()"
  },
  {
    "objectID": "12-Maps.html#your-turn-to-map",
    "href": "12-Maps.html#your-turn-to-map",
    "title": "12  Mapping",
    "section": "12.6 Your turn to map",
    "text": "12.6 Your turn to map\nMake a choropleth plot of the United States or some other geographic unit.\n\nCreate a map of a geographic area of interest (e.g., a map of U.S. states)\nShade the states according to a numeric or categorical variable you add to the data\n\nHas a particular state/set of states adopted a policy?\nDoes a particular state/set of states embody a certain characteristic?\n\nShare the map on Piazza"
  },
  {
    "objectID": "12-Maps.html#application-terrorist-attacks-in-france",
    "href": "12-Maps.html#application-terrorist-attacks-in-france",
    "title": "12  Mapping",
    "section": "12.7 Application: Terrorist Attacks in France",
    "text": "12.7 Application: Terrorist Attacks in France\nPolitical scientists study a wide range of questions related to terrorism, including how targets end up being selected, how to predict and defend against attacks, what types of incidents the public considers to be terrorism, how attacks influence public attitudes, and responses to terrorism from government and other actors.\n\nWhy might mapping visualizations and spatial data be useful to political scientists for these questions?\n\n\n12.7.1 Adding points to a map\nFor a video explainer of the code for the applications with maps and points, as well as animating these points in the subsequent section, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nIn this application, we use the Global Terrorism Database to visualize where terrorist attacks (including failed attacks) have occurred in recent years in France.\n\nWe will make a map of France using map_data to get the polygon information\n\n\nlibrary(maps)\nlibrary(ggplot2)\n\n## get france data (not available for all countries)\nfrance &lt;- map_data(\"france\")\n\n## Plot France\nggplot()+\n  geom_polygon(data=france, aes(x=long, y=lat, group=group), fill=\"white\", colour=\"gray\")+\n  \n  ggtitle(\"Terrorist Attacks in France 2000-2019\")+\n  coord_quickmap()+\n  theme_void()\n\n\n\n\n\nWe load separate data that includes the latitude and longitude of attacks\n\n\nload(\"gtb.RData\")\n\n\n## Let's look at only recent attacks \ngtbfrance &lt;- subset(gtb, iyear &gt; 2000 & country_txt==\"France\")\n\n\nWe use geom_point to add a layer of points from this dataset\n\nWe can colour or size the points by additional variables in the data\n\n\n\nggplot()+\n  geom_polygon(data=france, aes(x=long, y=lat, group=group), fill=\"white\", colour=\"gray\")+\n  \n  \n  ## add points with size in proportion to fatalities\n  geom_point(data=gtbfrance, aes(x=longitude, y=latitude, size=nkill), \n             alpha=.4, colour=\"red\")+ # alpha makes points transparent\n  \n  ## range specifies how big or small you want points to be\n  scale_size(name=\"Number of Fatalities\", range=c(2, 10))+\n\n\n  ggtitle(\"Terrorist Attacks in France 2000-2019\")+\n  coord_quickmap()+\n  theme_void()\n\n\n\n\n\nWe can also add labels to the plot with geom_text_repel from the ggrepel package\n\nNote that we can use labels from yet another object so long as we have the right lat and long\n\n\n\ninstall.packages(\"ggrepel\")\n\n\nlibrary(ggrepel)\n## Let's add labels for the biggest attacks only\ngtbmajorfrance &lt;- subset(gtbfrance, nkill &gt; 75)\n\n\nggplot()+\n  geom_polygon(data=france, aes(x=long, y=lat, group=group), fill=\"white\", colour=\"gray\")+\n  \n  \n  ## add points with size in proportion to fatalities\n  geom_point(data=gtbfrance, aes(x=longitude, y=latitude, size=nkill), \n             alpha=.4, colour=\"red\")+\n  scale_size(name=\"Number of Fatalities\", range=c(2, 10))+\n\n  ## add labels from gtbmajorfrance\n  geom_text_repel(data=gtbmajorfrance, aes(x=longitude, y=latitude, \n                                           label=city), size=4,\n                  max.overlaps = 30)+\n  \n  \n  ggtitle(\"Terrorist Attacks in France 2000-2019\")+\n  coord_quickmap()+\n  theme_void()"
  },
  {
    "objectID": "12-Maps.html#animating-data",
    "href": "12-Maps.html#animating-data",
    "title": "12  Mapping",
    "section": "12.8 Animating Data",
    "text": "12.8 Animating Data\nWith R, we can go a step further to make our maps more interactive. RStudio and R allow for the ability to turn graphics into interactive applications, as well as animate visualizations to reveal or change the visual over the course of different frames. We will take a brief look at these applications.\nWhy would we want to do this? An engaging way to reveal changes over time or other states. Adding a bit of drama to presentations.\n\nWe will use the package gganimate for this. Install and load the package.\n\ninstall.packages(\"gganimate\")\n\n\nlibrary(gganimate)\n\nTo animate a ggplot, we just add an argument to indicate the variable that dictates the transition between different states, in this case, the date variable, iyear. The other parts of the plot stay very similar to before.\n\nggplot()+\n  geom_polygon(data=france, aes(x=long, y=lat, group=group), fill=\"white\", colour=\"gray\")+\n  \n  ## add points with size in proportion to fatalities\n  geom_point(data=gtbfrance, aes(x=longitude, y=latitude, size=nkill), alpha=.4, colour=\"red\")+\n  scale_size(name=\"Number of Fatalities\", range=c(2, 10))+\n\n  ggtitle(\"Terrorist Attacks in France 2000-2019\")+\n  coord_quickmap()+\n  theme_void()+\n  \n  ## add the transition and a label for the plot\n  transition_states(iyear, state_length = 8)+\n  labs(subtitle = \"Current map: {closest_state}\")\n\nanim_save(\"gtbplot.gif\")"
  },
  {
    "objectID": "12-Maps.html#application-spread-of-coronavirus",
    "href": "12-Maps.html#application-spread-of-coronavirus",
    "title": "12  Mapping",
    "section": "12.9 Application: Spread of Coronavirus",
    "text": "12.9 Application: Spread of Coronavirus\nMapping isn’t the only application where we might want to show an animation. Here is a brief example using ggplot in a different way to reveal trends over time.\nWe are going to map the spread of confirmed COVID cases based on data from John Hopkins University.\n\nload(\"covidlongfmt.RData\")\n\nWe have variables case_count and Country.Region along with date_fmt. Let’s compare trends in confirmed cases between Italy and France.\n\nitalyspain &lt;- subset(covidlongfmt, Country.Region %in% c(\"Italy\", \"Spain\"))\n\nLet’s divide cases by 1000 to make it easier to visualize.\n\nitalyspain$case_count_thousands &lt;- italyspain$case_count /1000\n\nWith ggplot, this time instead of using geom_polygon, we will use geom_line. Our x-axis will be time, and our y-axis will be the case count.\nAgain, to animate a ggplot, we just add an argument to indicate the variable that dictates the transition between different states, in this case, the date variable, date_fmt. The other parts of the plot stay very similar to before.\n\nggplot()+\n  geom_line(data=italyspain, aes(x=date_fmt, y=case_count_thousands, \n                                 colour=Country.Region))+\n  \n  ## Add labels with exact case count\n  geom_text_repel(data=italyspain, aes(x=date_fmt,  \n                                       y=case_count_thousands,\n                                       colour=Country.Region,\n                                 label=case_count_thousands))+\n \n  ## axis and title labels\n  ylab(\"Cases (thousands)\")+\n  ggtitle(\"Confirmed Cases in Italy and Spain\")+\n  theme_minimal()+\n  \n  ## We use transition_reveal to slowly reveal the trend\n  transition_reveal(date_fmt)+\n  labs(x = \"Date: {frame_along}\")\nanim_save(\"italyspain.gif\")\n\n\n\n12.9.1 Mapping Animation with World Map\nJust like with the terrorism data, we can make a map with the data, too.\nTo illustrate the process, let’s create a plot for just one day: 2020-03-01.\n\ncovidmarch &lt;- subset(covidlongfmt, date_fmt == \"2020-03-01\")\n\nWe create a world map and add points to indicate the case count, with the size proportionate to the count. We include alpha to make the points transparent.\n\nworld &lt;- map_data(\"world\")\n\nggplot()+\n  \n  ## create the world map\n  geom_polygon(data=world, aes(x=long, y=lat, group=group), colour=\"black\", fill=\"white\")+\n  \n  ## add points\n  geom_point(data=covidmarch, aes(x=Long, y=Lat, size=case_count), alpha=.4, colour=\"red\")+\n  scale_size(range = c(-1,10)) +\n  \n  ## aesthetics\n  ggtitle(\"COVID-19 Confirmed Cases on March 1, 2020\")+\n  coord_quickmap()+\n  theme_void()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow, we use the full data and transition through the date variable. We then “render” and save the animation.\n\nworld &lt;- map_data(\"world\")\n\nggplot()+\n  \n  ## create the world map\n  geom_polygon(data=world, aes(x=long, y=lat, group=group), colour=\"black\", fill=\"white\")+\n  \n  ## add points\n  geom_point(data=covidlongfmt, aes(x=Long, y=Lat, size=case_count), alpha=.4, colour=\"red\")+\n  scale_size(range = c(-1,15)) +\n  \n  ## aesthetics\n  ggtitle(\"COVID-19 Confirmed Cases\")+\n  coord_quickmap()+\n  theme_void() +\n  \n  ## add the transition and a label for the plot\n  transition_time(date_fmt)+\n  labs(title=\"Date: {frame_time}\")\n## save animation as a gif\nanim_save(\"covidplot.gif\")\n\n\nHow could we improve this visualization?"
  },
  {
    "objectID": "12-Maps.html#application-territorial-control-of-syria",
    "href": "12-Maps.html#application-territorial-control-of-syria",
    "title": "12  Mapping",
    "section": "12.10 Application: Territorial Control of Syria",
    "text": "12.10 Application: Territorial Control of Syria\nIn this article, Anita Gohdes uses data from the Syria Conflict Mapping Project (SCMP), which tracks local communities and determines which conflict party is in control. The author maps which group is in control of different communities in 2014 and 2015, including: opposition forces, Islamic State forces, government forces, and Kurdish forces.\nAmong other findings, the author shows that group presence influences the degree of targeted repressive violence.\nWe are going to map the territorial control of Syria at these two time points using the author’s data. Note: the mapping data we use does not come directly from the maps package. Instead the author used “shape files” from an external source, which then can be converted into a dataframe like we get when using map_data.\nLet’s load the data. The data have already been converted into the dataframe.\n\nload(\"SYmapd.RData\")\n\nA second dataset includes information about territorial control. Let’s also load that data.\n\nload(\"mapdata.RData\")\n\nWe can create a map.\n\nggplot()+\n  \n  ## use shape data\n   geom_polygon(data=SYmapd, aes(x=long, y=lat, group=group),\n                fill=\"white\", color=\"gray\") +  \n  \n  ## add points with map data\n   geom_point(data=mapdata, aes(x=longitude, y=latitude, color=group), alpha=.3)+\n   scale_color_manual(\"\",values=c(\"red\", \"black\", \"purple\", \"blue\"))+\n  \n  \n   theme_void()+ \n  \n  ## add animation\n   transition_states(date, transition_length = .25, state_length = 3)+\n   labs(title = \"Current map: {closest_state}\")\n\nanim_save(\"syriamap.gif\")"
  },
  {
    "objectID": "13-ChooseYourOwnAdventure.html#how-to-investigate-new-data",
    "href": "13-ChooseYourOwnAdventure.html#how-to-investigate-new-data",
    "title": "13  Choose Your Own Adventure",
    "section": "13.1 How to investigate new data",
    "text": "13.1 How to investigate new data\nReal-world data may often require some cleaning or “wrangling”\nSo you have some data…. AND it’s a mess!!!\nA lot of the data we may encounter in courses has been simplified to allow students to focus on other concepts. We may have data that look like the following:\nnicedata &lt;- data.frame(gender = c(\"Male\", \"Female\", \"Female\", \"Male\"),\n           age = c(18, 20, 66, 44),\n           voterturnout = c(1, 0, 1, 0))\n\n\n\n\n\ngender\nage\nvoterturnout\n\n\n\n\nMale\n18\n1\n\n\nFemale\n20\n0\n\n\nFemale\n66\n1\n\n\nMale\n44\n0\n\n\n\n\n\nIn the real world, our data may hit us like a ton of bricks, like the below:\nuglydata &lt;- data.frame(VV160002 = c(2, NA, 1, 2),\n           VV1400068 = c(18, 20, 66, 44),\n           VV20000 = c(1, NA, 1, NA))\n\n\n\n\n\nVV160002\nVV1400068\nVV20000\n\n\n\n\n2\n18\n1\n\n\nNA\n20\nNA\n\n\n1\n66\n1\n\n\n2\n44\nNA\n\n\n\n\n\nA lot of common datasets we use in the social sciences are messy, uninformative, sprawling, misshaped, and/or incomplete. What do I mean by this?\n\nThe data might have a lot of missing values. For example, we may have NA values in R, or perhaps a research firm has used some other notation for missing data, such as a 99.\nThe variable names may be uninformative.\n\nFor example, there may be no way to know by looking at the data, which variable represents gender. We have to look at a codebook.\n\nEven if we can tell what a variable is, its categories may not be coded in a way that aligns with how we want to use the data for our research question.\n\nFor example, perhaps you are interested in the effect of a policy on people below vs. 65 and over in age. Well, your age variables might just be a numeric variable. You will have to create a new variable that aligns with your theoretical interest.\n\nDatasets are often sprawling. Some datasets may have more than 1000 variables. It is hard to sort through all of them. Likewise, datasets may have millions of observations. We cannot practically look through all the values of a column to know what is there.\n\n\n13.1.1 Dealing with Uninformative Variable Names\nHopefully, there is an easy fix for dealing with uninformative variable names. I say “hopefully” because hopefully when you encounter a dataset with uninformative variable names, the place where you downloaded the data will also include a codebook telling you what each variable name means, and how the corresponding values are coded.\nUnfortunately, this may not always be the case. One thing you can do as a researcher is when you create a dataset for your own work, keep a record (in written form, on a word document or in a pdf or code file) of what each variable means (e.g., the survey question it corresponds to or the exact economic measure), as well as how the values of the variables are coded. This good practice will help you in the short-term, as you pause and come back to working on a project over the course of a year, as well as benefit other researchers in the long run after you finish your research.\nFor examples of large codebooks, you can view the 2016 American National Election Study Survey and click on a codebook.\nI recommend that once you locate the definition of a variable of interest, rename the variable in your dataset to be informative. You can do this by creating a new variable or overwriting the name of the existing variable. You might also comment a note for yourself of what the values mean.\n\n## Option 1: create new variable\n## gender 2=Male, 1=Female\nuglydata$gender &lt;- uglydata$VV160002\n\n## Option 2: Overwrite\nnames(uglydata)[1] &lt;- \"gender2\"\n\n\n\n13.1.2 Dealing with Missing Data\nWhen we have a column with missing data, it is best to do a few things:\n\nTry to quantify how much missing data there is and poke at the reason why data are missing.\n\nIs it minor non-response data?\nOr is it indicative of a more systematic issue? For example, maybe data from a whole group of people or countries is missing for certain variables.\n\nIf the data are missing at a very minor rate and/or there is a logical explanation for the missing data that should not affect your research question, you may choose to “ignore” the missing data when performing common analyses, such as taking the mean or running a regression.\n\nIf we want to figure out how much missing data we have in a variable, we have a couple of approaches:\n\n## Summarize this variable\nsummary(uglydata$gender)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.500   2.000   1.667   2.000   2.000       1 \n\n## What is the length of the subset of the variable where the data are missing\nlength(uglydata$gender[is.na(uglydata$gender) == T])\n\n[1] 1\n\n\nIf we choose to ignore missing data, this can often be easily accomplished in common operations. For example, when taking the mean we just add an argument na.rm = T:\n\nmean(uglydata$VV1400068, na.rm=T)\n\n[1] 37\n\n\nWe should always be careful with missing data to understand how R is treating it in a particular scenario.\n\n\n13.1.3 Dealing with Variable Codings that Aren’t Quite Right\nOftentimes the ways that variables are coded in datasets we get off-the-shelf are not coded exactly as how we were dreaming up operationalizing our concepts. Instead, we are going to have to wrangle the data to get them into shape.\nThis may involve creating new variables that recode certain values, creating new variables that collapse some values into a smaller number of categories, combining multiple variables into a single variable (e.g., representing the average), or setting some of the variable values to be missing (NA). All of these scenarios may come up when you are dealing with real data.\n\n## create variable indicating over 65 vs. under 65\n\n## approach 1\nuglydata$over65 &lt;- NA\nuglydata$over65[uglydata$VV1400068 &gt;=65] &lt;- 1\nuglydata$over65[uglydata$VV1400068 &lt; 65] &lt;- 0\n\n\n## approach 2\nuglydata$over65 &lt;- ifelse(uglydata$VV1400068 &gt;=65, 1, 0)\n\nWe can also use the %in% function to help with this recoding process, like we did in the mapping section.\nSuppose we want to code people as “milestone age” if they are 18, 21, 40, or 65. We could create a vector of these ages.\n\nmilestones &lt;- c(18, 21, 40, 65)\n\nThen, we could indicate “milestone” or “not milestone” depending on if someone’s age is in or not in (! %in%) that list.\n\nuglydata$milestoneage &lt;- ifelse(uglydata$VV1400068 %in% milestones, \"milestone age\", \"not milestone age\")\n\ntable(uglydata$milestoneage)\n\n\n    milestone age not milestone age \n                1                 3 \n\n## approach 2\nuglydata$milestoneage &lt;- NA\nuglydata$milestoneage[uglydata$VV1400068 %in% milestones] &lt;- \"milestone age\"\nuglydata$milestoneage[!(uglydata$VV1400068 %in% milestones)] &lt;-  \"not milestone age\"\ntable(uglydata$milestoneage)\n\n\n    milestone age not milestone age \n                1                 3 \n\n\n\n\n13.1.4 Dealing with Parts of Datasets\nWe may also want to limit our analysis to just small parts of datasets instead of the entire dataset. Recall the function subset to limit the data to only rows that meet certain criteria.\n\n## limit data to those over 65\nover65 &lt;- subset(uglydata, over65 == 1)\n\nWe can also trim the dataset to include only certatin columns. There are many ways to do this. Here is one, using our subset function with the select argument:\n\n## include all rows but only these 2 columns\nonlythesecolumns &lt;- subset(uglydata, \n                           select = c(gender,\n                                      over65))\nhead(onlythesecolumns)\n\n  gender over65\n1      2      0\n2     NA      0\n3      1      1\n4      2      0"
  },
  {
    "objectID": "13-ChooseYourOwnAdventure.html#where-to-go-next",
    "href": "13-ChooseYourOwnAdventure.html#where-to-go-next",
    "title": "13  Choose Your Own Adventure",
    "section": "13.2 Where to Go Next",
    "text": "13.2 Where to Go Next\nRecall that we said, four primary goals of social science include:\n\nDescribe and measure\n\nHas the U.S. population increased?\n\nExplain, evaluate, and recommend (study of causation)\n\nDoes expanding Medicaid improve health outcomes?\n\nPredict\n\nWho will win the next election?\n\nDiscover\n\nHow do policies diffuse across states?\n\n\nIn this course, we have explored a taste of each of these goals, while learning a bit of R code and statistics along the way.\n\nWe described trends in voter turnout, tolerance for domestic violence in different parts of the world\nWe estimated the causal effect of experiments to test for discrimination in the labor market, the effect of social status messages on attitudes toward economic policy, and the implementation of wind turbines on electoral support\nWe predicted the outcomes of future elections, whether someone is a campaign donor, and questioned the fairness of using these types of “machine learning” models in real-world settings, such as on social media platforms or in the criminal justice system\nWe discovered patterns in political speeches, such as the State of the Union and country constitutions, and we visualized geographic diffusion of policies and international terrorist incidents.\n\nThis section will provide a few ideas of ways you can continue your learning about data science and its use within social science.\n\n13.2.1 Network Analysis\nOne topic we do not cover in great depth is the use of network analysis in social science.\nNetworks can be a really interesting topic to study in political science and other social sciences because they are inherently “social.”\n\nNetworks describe the relationships between people, countries, institutions, etc. instead of evaluating each in isolation.\nSimilar to our section on mapping, with networks, we often visualize the interconnectedness using graphics\n\nNetwork analysis may be particularly useful in an area like international relations where scholars try to identify relationships between states.\n\nFor example, some scholars map international trade networks.\n\nThis paper by Dotan Haim (2016) explores how the nature of shared political alliances influences the amount of trade.\n\n\nWe are going to look at an example of a network of Twitter following among U.S. Senators, following the application of network analysis in QSS 5.2.\nIn R, we can use the igraph package to visualize networks.\n\ninstall.packages(\"igraph\")\n\nTo use the package, we open it with library().\n\nlibrary(igraph)\n\nWe can look at a network of which U.S. senators follow each other on Twitter, based on data from QSS chapter 5. Let’s open the network data.\n\ntwitter &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/twitter-following.csv\")\nhead(twitter)\n\n     following        followed\n1 SenAlexander        RoyBlunt\n2 SenAlexander     SenatorBurr\n3 SenAlexander     JohnBoozman\n4 SenAlexander SenJohnBarrasso\n5 SenAlexander     SenBennetCO\n6 SenAlexander     SenDanCoats\n\n\nNote that the data include two columns:\n\nfollowing: Twitter screen name of the senator that is following the senator in the second column\nfollowed: Twitter screen name of the senator that is being followed by the senator in the first column\n\nTwitter represents a “directed” network. This means that a “tie” between two individuals has a direction. Take, for, instance, the difference between Facebook friendship and Twitter following.\n\nOn Facebook, when you become friends with another user, you both are considered friends with each other. There is no directionality to the relationship in that space. This is “undirected.”\nIn contrast, on Twitter, you can follow someone with/without that person following you. This is “directionality” meaning that we have to indicate specifically in which direction/both directions the connection exists between users.\n\nWe are going to create a second dataframe which is called an “adjacency matrix” which indicates the presence or absence and direction of “ties” between each user. QSS pg. 212 describes how you can convert the simple list of follows we had into this type of matrix, and we will follow a slightly different process using the function graph_from_data_frame. We load a dataframe that contains information about each senator where the first column includes the screen names, just like the other dataframe.\n\nsenator &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/twitter-senator.csv\")\n\n\ntwitteradj &lt;- graph_from_data_frame(d=twitter, vertices=senator, directed=T) \ntwitteradj[1:6, 1:6]\n\n6 x 6 sparse Matrix of class \"dgCMatrix\"\n                Lamar Alexander Roy Blunt Barbara Boxer Sherrod Brown\nLamar Alexander               .         1             .             .\nRoy Blunt                     1         .             .             .\nBarbara Boxer                 .         .             .             1\nSherrod Brown                 .         .             1             .\nRichard Burr                  .         1             .             .\nTammy Baldwin                 1         1             1             1\n                Richard Burr Tammy Baldwin\nLamar Alexander            1             .\nRoy Blunt                  1             .\nBarbara Boxer              .             1\nSherrod Brown              .             1\nRichard Burr               .             .\nTammy Baldwin              1             .\n\n\nWe are going to visualize the network. To make it more informative, we will color the plot according to the partisanship of senators. Because our senate dataframe is in the same order as the adjacency matrix, we can do the following:\n\ncol &lt;- NA\ncol[senator$party == \"R\"] &lt;- rgb(1,0,0, alpha=.5)\ncol[senator$party == \"D\"] &lt;- rgb(0,0,1, alpha=.5)\ncol[senator$party == \"I\"] &lt;- \"black\"\n\nWe can now use the plot function with our matrix.\n\nplot(twitteradj, \n     vertex.color = col, \n     vertex.label = NA, \n     edge.arrow.size = 0.1, \n     edge.width = 0.5)\n\n\n\n\nWhat do you notice about this network visualization?\n\nWhat is easy to see from this? What is hard to see from this? What other information would you like to know?\n\nPolarization is pretty rampant in American Politics. Recently, a set of political scientists used network analysis to explore just how far polarization pervades into our lives, even when we are engaging in tasks and hobbies unrelated to politics. Stiene Praet and co-authors assess whether the Facebook likes of individuals in political vs. non-political lifestyle domains appear polarized. Fortunately, they do not find severe polarization in less political arenas. You may think you know everything about someone based on their food, exercise, and tv habits, but we are not fully polarized in those domains … yet.\n\nWhile these visuals gives us a little information, there is far more you can do with networks to better quantify the shape of the network and information about which units are most connected and most central to a network.\n\nExplore QSS Chapter 5.2 to learn more about betweenness, centrality, closeness and degree\n\nThe data described in the chapter can be downloaded here\n\nFor more on network visualization and analysis, you can also explore Professor Katherine Ognyanova’s website (School of Communication and Information), which has multiple tutorials about network analysis in R that you can download here\n\n\n\n13.2.2 Additional Topics to Explore\nRecall that in the beginning of the semester we looked at the venn diagram of data science from Drew Conway. Throughout the semester, we have answered substantive research questions in political science using data science approaches. We have developed our R skills, and used a little bit of math and statistics. As you go forward, you may want to add additional skills and training.\n\nSelf-Study Options\nTo gain additional math and statistics skills to support what you have done so far, you can review Chapter 6 of QSS for self-study. It focuses on probability. We already covered some aspects of Chapter 7 of QSS on uncertainty, but you could also review these sections as well to gain more experience with the underlying mathematical calculations that go into calculating standard errors and constructing confidence intervals.\nIf you want to continue to practice R outside of QSS, here are two online books to consider:\n\nR for Data Science by Hadley Wickham\nData Visualization: A Practical Introduction by Kieran Healy\n\nFor more examples of research at the intersection of data science and social science, you can continue also to read from Bit by Bit by Matt Salganik. We read excerpts of this in the course.\nCourse Options\nIf you would like to continue studying quantitative social science, you might also consider taking courses that count toward the following programs.\n\nCertificate in Quantitative Political Science Methods\n\nThese include undergraduate courses in statistics, political science research methods, and other methods classes in the social sciences\nThese also could include doctoral level classes in political science focused on quantitative methods, experiments, game theory, and measurement\n\nData Science Certificate/Minor\n\nIn these classes, you may expand your data wrangling, collection, and programming skills and apply them to a wide range of domains.\n\n\nIn addition, one way to practice your skills is to complete an undergraduate thesis during your senior year.\n\n\n13.2.3 That’s all"
  }
]